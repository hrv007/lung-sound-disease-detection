{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport cv2","metadata":{"id":"EXy5Ggoqczap","execution":{"iopub.status.busy":"2023-08-15T17:52:31.468767Z","iopub.execute_input":"2023-08-15T17:52:31.469173Z","iopub.status.idle":"2023-08-15T17:52:40.175862Z","shell.execute_reply.started":"2023-08-15T17:52:31.469138Z","shell.execute_reply":"2023-08-15T17:52:40.174644Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/transferlearning-ls-data/patient_diagnosis.csv\")\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"ArtW5CVwuApp","outputId":"fe2935a3-842f-4305-996a-bb1edcb03165","execution":{"iopub.status.busy":"2023-08-15T17:52:40.179813Z","iopub.execute_input":"2023-08-15T17:52:40.180395Z","iopub.status.idle":"2023-08-15T17:52:40.207630Z","shell.execute_reply.started":"2023-08-15T17:52:40.180366Z","shell.execute_reply":"2023-08-15T17:52:40.206729Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   101     URTI\n0  102  Healthy\n1  103   Asthma\n2  104     COPD\n3  105     URTI\n4  106     COPD","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>101</th>\n      <th>URTI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>102</td>\n      <td>Healthy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>103</td>\n      <td>Asthma</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>104</td>\n      <td>COPD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>105</td>\n      <td>URTI</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>106</td>\n      <td>COPD</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['URTI'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:52:40.209776Z","iopub.execute_input":"2023-08-15T17:52:40.210534Z","iopub.status.idle":"2023-08-15T17:52:40.220730Z","shell.execute_reply.started":"2023-08-15T17:52:40.210491Z","shell.execute_reply":"2023-08-15T17:52:40.219868Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"array(['Healthy', 'Asthma', 'COPD', 'URTI', 'LRTI', 'Bronchiectasis',\n       'Pneumonia', 'Bronchiolitis'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"sr_no = {'101':'URTI'}\nfor i, j in zip(df['101'].unique(), df['URTI']):\n    sr_no[str(i)] = j","metadata":{"id":"eAdDHbX6u_iz","execution":{"iopub.status.busy":"2023-08-15T17:52:40.224951Z","iopub.execute_input":"2023-08-15T17:52:40.225303Z","iopub.status.idle":"2023-08-15T17:52:40.232044Z","shell.execute_reply.started":"2023-08-15T17:52:40.225279Z","shell.execute_reply":"2023-08-15T17:52:40.231260Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"sr_no.keys()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1j95yfuivIOL","outputId":"c4ce184c-bb32-43e4-acbb-ef143254077e","execution":{"iopub.status.busy":"2023-08-15T17:52:40.233537Z","iopub.execute_input":"2023-08-15T17:52:40.234519Z","iopub.status.idle":"2023-08-15T17:52:40.243440Z","shell.execute_reply.started":"2023-08-15T17:52:40.234488Z","shell.execute_reply":"2023-08-15T17:52:40.242335Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"dict_keys(['101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226'])"},"metadata":{}}]},{"cell_type":"code","source":"import os\nsound_files = os.listdir('/kaggle/input/transferlearning-ls-data/Mel Spectrogram/Mel Spectrogram/Original')","metadata":{"id":"k_apddzYvK_5","execution":{"iopub.status.busy":"2023-08-15T17:52:40.244988Z","iopub.execute_input":"2023-08-15T17:52:40.245743Z","iopub.status.idle":"2023-08-15T17:52:40.579005Z","shell.execute_reply.started":"2023-08-15T17:52:40.245709Z","shell.execute_reply":"2023-08-15T17:52:40.578005Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"req_file_names = []\n\nfor i in sound_files:\n      req_file_names.append([i])\n\nreq_file_names","metadata":{"id":"jNTE66gVvVEg","execution":{"iopub.status.busy":"2023-08-15T17:52:40.581211Z","iopub.execute_input":"2023-08-15T17:52:40.581826Z","iopub.status.idle":"2023-08-15T17:52:40.616095Z","shell.execute_reply.started":"2023-08-15T17:52:40.581791Z","shell.execute_reply":"2023-08-15T17:52:40.615047Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[['176_2b3_Lr_mc_AKGC417L.png'],\n ['112_1p1_Pr_sc_Litt3200.png'],\n ['130_2b2_Pr_mc_AKGC417L.png'],\n ['193_7b3_Ll_mc_AKGC417L.png'],\n ['156_5b3_Pl_mc_AKGC417L.png'],\n ['162_2b2_Tc_mc_AKGC417L.png'],\n ['134_2b3_Ar_mc_LittC2SE.png'],\n ['160_1b4_Tc_mc_AKGC417L.png'],\n ['200_2p4_Pl_mc_AKGC417L.png'],\n ['169_1b2_Ll_sc_Meditron.png'],\n ['144_1b1_Tc_sc_Meditron.png'],\n ['160_2b4_Pl_mc_AKGC417L.png'],\n ['133_2p3_Pr_mc_AKGC417L.png'],\n ['205_1b3_Al_mc_AKGC417L.png'],\n ['130_3p4_Tc_mc_AKGC417L.png'],\n ['130_1p4_Lr_mc_AKGC417L.png'],\n ['200_3p4_Pl_mc_AKGC417L.png'],\n ['107_2b4_Ar_mc_AKGC417L.png'],\n ['203_2p3_Al_mc_AKGC417L.png'],\n ['113_1b1_Ar_sc_Litt3200.png'],\n ['174_2p3_Al_mc_AKGC417L.png'],\n ['178_1b2_Lr_mc_AKGC417L.png'],\n ['172_1b5_Ll_mc_AKGC417L.png'],\n ['216_1b1_Al_sc_Meditron.png'],\n ['205_4b2_Pl_mc_AKGC417L.png'],\n ['177_1b2_Lr_mc_AKGC417L.png'],\n ['151_2p4_Ll_mc_AKGC417L.png'],\n ['141_1b1_Pr_mc_LittC2SE.png'],\n ['162_1b2_Lr_mc_AKGC417L.png'],\n ['133_3p4_Tc_mc_AKGC417L.png'],\n ['147_2b3_Ll_mc_AKGC417L.png'],\n ['143_1b1_Al_sc_Meditron.png'],\n ['218_1p1_Pr_sc_Litt3200.png'],\n ['138_2p2_Al_mc_AKGC417L.png'],\n ['113_1b1_Al_sc_Litt3200.png'],\n ['144_1b1_Al_sc_Meditron.png'],\n ['130_1p4_Pl_mc_AKGC417L.png'],\n ['185_1b1_Ar_sc_Litt3200.png'],\n ['138_1p3_Pl_mc_AKGC417L.png'],\n ['122_2b2_Ar_mc_LittC2SE.png'],\n ['126_1b1_Al_sc_Meditron.png'],\n ['140_2b3_Ll_mc_LittC2SE.png'],\n ['160_1b3_Al_mc_AKGC417L.png'],\n ['186_2b4_Pr_mc_AKGC417L.png'],\n ['176_2b3_Pr_mc_AKGC417L.png'],\n ['160_1b2_Al_mc_AKGC417L.png'],\n ['136_1b1_Ar_sc_Meditron.png'],\n ['172_2b5_Tc_mc_AKGC417L.png'],\n ['176_2b3_Pl_mc_AKGC417L.png'],\n ['178_2b2_Lr_mc_AKGC417L.png'],\n ['162_1b2_Al_mc_AKGC417L.png'],\n ['133_2p2_Tc_mc_AKGC417L.png'],\n ['205_4b2_Al_mc_AKGC417L.png'],\n ['113_1b1_Pl_sc_Litt3200.png'],\n ['178_2b2_Ar_mc_AKGC417L.png'],\n ['156_5b3_Pr_mc_AKGC417L.png'],\n ['198_6p1_Ar_mc_AKGC417L.png'],\n ['213_1p3_Pr_mc_AKGC417L.png'],\n ['170_1b2_Ar_mc_AKGC417L.png'],\n ['204_7p5_Lr_mc_AKGC417L.png'],\n ['107_2b3_Pl_mc_AKGC417L.png'],\n ['176_2b3_Tc_mc_AKGC417L.png'],\n ['197_1b1_Tc_sc_Meditron.png'],\n ['196_1b1_Pr_sc_Meditron.png'],\n ['130_1p2_Pr_mc_AKGC417L.png'],\n ['146_8p3_Ar_mc_AKGC417L.png'],\n ['151_3p2_Ar_mc_AKGC417L.png'],\n ['141_1b2_Tc_mc_LittC2SE.png'],\n ['103_2b2_Ar_mc_LittC2SE.png'],\n ['211_2p4_Tc_mc_AKGC417L.png'],\n ['160_1b4_Pr_mc_AKGC417L.png'],\n ['176_1b4_Lr_mc_AKGC417L.png'],\n ['156_8b3_Ll_mc_AKGC417L.png'],\n ['205_4b2_Pr_mc_AKGC417L.png'],\n ['172_1b3_Tc_mc_AKGC417L.png'],\n ['133_2p3_Pl_mc_AKGC417L.png'],\n ['154_4b4_Pl_mc_AKGC417L.png'],\n ['176_1b4_Tc_mc_AKGC417L.png'],\n ['107_3p2_Tc_mc_AKGC417L.png'],\n ['114_1b4_Al_mc_AKGC417L.png'],\n ['207_2b4_Tc_mc_AKGC417L.png'],\n ['130_2p5_Lr_mc_AKGC417L.png'],\n ['198_6p1_Pl_mc_AKGC417L.png'],\n ['138_1p4_Ar_mc_AKGC417L.png'],\n ['151_2p3_Ar_mc_AKGC417L.png'],\n ['147_2b4_Pl_mc_AKGC417L.png'],\n ['198_1b5_Lr_mc_AKGC417L.png'],\n ['135_2b1_Tc_mc_LittC2SE.png'],\n ['200_2p2_Tc_mc_AKGC417L.png'],\n ['159_1b1_Pr_sc_Meditron.png'],\n ['186_2b4_Ar_mc_AKGC417L.png'],\n ['156_2b3_Ar_mc_AKGC417L.png'],\n ['116_1b2_Tc_sc_Meditron.png'],\n ['156_2b3_Pl_mc_AKGC417L.png'],\n ['213_2p2_Ar_mc_AKGC417L.png'],\n ['213_1p3_Ar_mc_AKGC417L.png'],\n ['120_1b1_Al_sc_Meditron.png'],\n ['160_1b3_Lr_mc_AKGC417L.png'],\n ['207_2b2_Pr_mc_AKGC417L.png'],\n ['177_1b2_Al_mc_AKGC417L.png'],\n ['221_2b2_Al_mc_LittC2SE.png'],\n ['133_2p4_Al_mc_AKGC417L.png'],\n ['154_2b4_Tc_mc_AKGC417L.png'],\n ['205_2b3_Ll_mc_AKGC417L.png'],\n ['178_1b3_Tc_mc_AKGC417L.png'],\n ['158_1p4_Al_mc_AKGC417L.png'],\n ['205_3b4_Ar_mc_AKGC417L.png'],\n ['159_1b1_Ll_sc_Meditron.png'],\n ['138_2p2_Ll_mc_AKGC417L.png'],\n ['205_1b3_Ll_mc_AKGC417L.png'],\n ['176_1b4_Pl_mc_AKGC417L.png'],\n ['162_2b4_Ar_mc_AKGC417L.png'],\n ['151_2p4_Lr_mc_AKGC417L.png'],\n ['207_2b3_Ar_mc_AKGC417L.png'],\n ['135_2b3_Al_mc_LittC2SE.png'],\n ['151_2p3_Ll_mc_AKGC417L.png'],\n ['101_1b1_Pr_sc_Meditron.png'],\n ['104_1b1_Ar_sc_Litt3200.png'],\n ['135_2b3_Pr_mc_LittC2SE.png'],\n ['151_3p2_Tc_mc_AKGC417L.png'],\n ['170_2b2_Lr_mc_AKGC417L.png'],\n ['140_2b2_Tc_mc_LittC2SE.png'],\n ['218_1p1_Pl_sc_Litt3200.png'],\n ['170_1b4_Tc_mc_AKGC417L.png'],\n ['192_2b3_Ar_mc_LittC2SE.png'],\n ['200_2p4_Al_mc_AKGC417L.png'],\n ['204_7p5_Ar_mc_AKGC417L.png'],\n ['178_2b2_Tc_mc_AKGC417L.png'],\n ['135_2b3_Tc_mc_LittC2SE.png'],\n ['221_2b2_Lr_mc_LittC2SE.png'],\n ['130_3b4_Pr_mc_AKGC417L.png'],\n ['162_2b4_Pl_mc_AKGC417L.png'],\n ['138_1p2_Ll_mc_AKGC417L.png'],\n ['109_1b1_Lr_sc_Litt3200.png'],\n ['213_1p2_Tc_mc_AKGC417L.png'],\n ['122_2b2_Tc_mc_LittC2SE.png'],\n ['174_1p4_Tc_mc_AKGC417L.png'],\n ['122_2b1_Tc_mc_LittC2SE.png'],\n ['141_1b2_Ar_mc_LittC2SE.png'],\n ['198_6p1_Lr_mc_AKGC417L.png'],\n ['149_1b1_Al_sc_Meditron.png'],\n ['176_2b3_Al_mc_AKGC417L.png'],\n ['107_2b5_Tc_mc_AKGC417L.png'],\n ['109_1b1_Al_sc_Litt3200.png'],\n ['213_1p5_Pl_mc_AKGC417L.png'],\n ['158_1p4_Tc_mc_AKGC417L.png'],\n ['200_2p3_Pl_mc_AKGC417L.png'],\n ['177_1b2_Pl_mc_AKGC417L.png'],\n ['151_2p3_Al_mc_AKGC417L.png'],\n ['147_2b2_Pl_mc_AKGC417L.png'],\n ['175_1b1_Pr_sc_Litt3200.png'],\n ['157_1b1_Ar_sc_Meditron.png'],\n ['122_2b3_Tc_mc_LittC2SE.png'],\n ['203_1p3_Tc_mc_AKGC417L.png'],\n ['166_1p1_Pl_sc_Meditron.png'],\n ['213_1p2_Al_mc_AKGC417L.png'],\n ['215_1b3_Tc_sc_Meditron.png'],\n ['110_1p1_Ll_sc_Meditron.png'],\n ['200_2p2_Ar_mc_AKGC417L.png'],\n ['202_1b1_Ar_sc_Meditron.png'],\n ['158_1p2_Lr_mc_AKGC417L.png'],\n ['147_1b2_Tc_mc_AKGC417L.png'],\n ['147_2b3_Ar_mc_AKGC417L.png'],\n ['174_2p3_Tc_mc_AKGC417L.png'],\n ['200_3p4_Al_mc_AKGC417L.png'],\n ['213_2p2_Pl_mc_AKGC417L.png'],\n ['177_1b2_Pr_mc_AKGC417L.png'],\n ['204_7p5_Ll_mc_AKGC417L.png'],\n ['162_2b3_Pr_mc_AKGC417L.png'],\n ['117_1b2_Tc_mc_LittC2SE.png'],\n ['140_2b2_Ll_mc_LittC2SE.png'],\n ['222_1b1_Lr_sc_Meditron.png'],\n ['130_2b2_Ll_mc_AKGC417L.png'],\n ['130_2p5_Pl_mc_AKGC417L.png'],\n ['172_2b5_Al_mc_AKGC417L.png'],\n ['147_2b4_Lr_mc_AKGC417L.png'],\n ['138_1p3_Ll_mc_AKGC417L.png'],\n ['138_1p4_Ll_mc_AKGC417L.png'],\n ['160_2b4_Ar_mc_AKGC417L.png'],\n ['147_2b2_Ar_mc_AKGC417L.png'],\n ['141_1b3_Ar_mc_LittC2SE.png'],\n ['124_1b1_Ll_sc_Litt3200.png'],\n ['178_1b3_Pl_mc_AKGC417L.png'],\n ['107_3p2_Ll_mc_AKGC417L.png'],\n ['130_2p5_Tc_mc_AKGC417L.png'],\n ['106_2b1_Pl_mc_LittC2SE.png'],\n ['198_1b5_Ar_mc_AKGC417L.png'],\n ['193_7b3_Pr_mc_AKGC417L.png'],\n ['198_1b5_Pr_mc_AKGC417L.png'],\n ['203_1p4_Al_mc_AKGC417L.png'],\n ['201_1b3_Ar_sc_Meditron.png'],\n ['146_2b4_Ll_mc_AKGC417L.png'],\n ['208_1b1_Ll_sc_Meditron.png'],\n ['138_2p2_Tc_mc_AKGC417L.png'],\n ['158_1p3_Ll_mc_AKGC417L.png'],\n ['178_1b6_Ll_mc_AKGC417L.png'],\n ['130_2p5_Pr_mc_AKGC417L.png'],\n ['133_2p4_Pr_mc_AKGC417L.png'],\n ['195_1b1_Pr_sc_Litt3200.png'],\n ['170_2b2_Pl_mc_AKGC417L.png'],\n ['107_2b5_Lr_mc_AKGC417L.png'],\n ['170_1b3_Lr_mc_AKGC417L.png'],\n ['174_1p3_Pl_mc_AKGC417L.png'],\n ['200_3p4_Ar_mc_AKGC417L.png'],\n ['178_1b6_Pr_mc_AKGC417L.png'],\n ['205_1b3_Ar_mc_AKGC417L.png'],\n ['198_6p1_Tc_mc_AKGC417L.png'],\n ['107_3p2_Pr_mc_AKGC417L.png'],\n ['174_1p2_Pr_mc_AKGC417L.png'],\n ['205_2b3_Ar_mc_AKGC417L.png'],\n ['188_1b1_Ar_sc_Meditron.png'],\n ['151_2p4_Pr_mc_AKGC417L.png'],\n ['176_1b3_Tc_mc_AKGC417L.png'],\n ['138_1p2_Pr_mc_AKGC417L.png'],\n ['174_1p4_Ar_mc_AKGC417L.png'],\n ['158_1p4_Pl_mc_AKGC417L.png'],\n ['158_1p3_Pl_mc_AKGC417L.png'],\n ['205_2b2_Pr_mc_AKGC417L.png'],\n ['198_6p1_Al_mc_AKGC417L.png'],\n ['170_1b2_Pl_mc_AKGC417L.png'],\n ['203_2p3_Tc_mc_AKGC417L.png'],\n ['130_3p2_Ar_mc_AKGC417L.png'],\n ['178_1b3_Al_mc_AKGC417L.png'],\n ['188_1b1_Tc_sc_Meditron.png'],\n ['147_2b2_Al_mc_AKGC417L.png'],\n ['218_1b1_Ar_sc_Meditron.png'],\n ['130_2b2_Pl_mc_AKGC417L.png'],\n ['205_1b3_Pr_mc_AKGC417L.png'],\n ['130_2p3_Pl_mc_AKGC417L.png'],\n ['172_1b3_Pr_mc_AKGC417L.png'],\n ['200_2p2_Pr_mc_AKGC417L.png'],\n ['205_1b3_Lr_mc_AKGC417L.png'],\n ['156_8b3_Lr_mc_AKGC417L.png'],\n ['200_2p2_Al_mc_AKGC417L.png'],\n ['133_3p2_Pr_mc_AKGC417L.png'],\n ['176_1b3_Lr_mc_AKGC417L.png'],\n ['135_2b2_Tc_mc_LittC2SE.png'],\n ['207_2b2_Tc_mc_AKGC417L.png'],\n ['180_1b4_Al_mc_AKGC417L.png'],\n ['120_1b1_Lr_sc_Meditron.png'],\n ['158_1p3_Pr_mc_AKGC417L.png'],\n ['154_2b4_Ar_mc_AKGC417L.png'],\n ['130_3b4_Pl_mc_AKGC417L.png'],\n ['107_2b3_Lr_mc_AKGC417L.png'],\n ['107_2b5_Ll_mc_AKGC417L.png'],\n ['211_1p2_Pr_mc_AKGC417L.png'],\n ['225_1b1_Pl_sc_Meditron.png'],\n ['207_2b4_Al_mc_AKGC417L.png'],\n ['154_3b3_Ar_mc_AKGC417L.png'],\n ['113_1b1_Pr_sc_Litt3200.png'],\n ['135_2b2_Al_mc_LittC2SE.png'],\n ['213_1p2_Pr_mc_AKGC417L.png'],\n ['207_2b3_Pr_mc_AKGC417L.png'],\n ['172_2b5_Lr_mc_AKGC417L.png'],\n ['191_2b2_Tc_mc_LittC2SE.png'],\n ['130_1p3_Ar_mc_AKGC417L.png'],\n ['162_1b2_Ll_mc_AKGC417L.png'],\n ['201_1b3_Al_sc_Meditron.png'],\n ['201_1b1_Ar_sc_Meditron.png'],\n ['213_2p2_Pr_mc_AKGC417L.png'],\n ['206_1b1_Ar_sc_Meditron.png'],\n ['178_1b3_Lr_mc_AKGC417L.png'],\n ['221_2b3_Lr_mc_LittC2SE.png'],\n ['200_2p3_Lr_mc_AKGC417L.png'],\n ['129_1b1_Ar_sc_Meditron.png'],\n ['192_2b3_Al_mc_LittC2SE.png'],\n ['221_2b3_Pr_mc_LittC2SE.png'],\n ['160_1b3_Pr_mc_AKGC417L.png'],\n ['130_2b3_Pl_mc_AKGC417L.png'],\n ['170_1b3_Al_mc_AKGC417L.png'],\n ['181_1b3_Tc_mc_LittC2SE.png'],\n ['141_1b3_Al_mc_LittC2SE.png'],\n ['186_3b3_Ar_mc_AKGC417L.png'],\n ['133_3p2_Ar_mc_AKGC417L.png'],\n ['162_1b2_Ar_mc_AKGC417L.png'],\n ['198_6p1_Ll_mc_AKGC417L.png'],\n ['130_3p2_Al_mc_AKGC417L.png'],\n ['203_2p3_Pl_mc_AKGC417L.png'],\n ['118_1b1_Al_sc_Litt3200.png'],\n ['162_2b2_Pl_mc_AKGC417L.png'],\n ['130_1p2_Tc_mc_AKGC417L.png'],\n ['104_1b1_Al_sc_Litt3200.png'],\n ['151_3p2_Lr_mc_AKGC417L.png'],\n ['205_3b4_Pr_mc_AKGC417L.png'],\n ['213_1p2_Pl_mc_AKGC417L.png'],\n ['158_1p4_Lr_mc_AKGC417L.png'],\n ['124_1b1_Lr_sc_Litt3200.png'],\n ['193_7b3_Lr_mc_AKGC417L.png'],\n ['192_2b1_Al_mc_LittC2SE.png'],\n ['220_1b1_Tc_mc_LittC2SE.png'],\n ['164_1b1_Ll_sc_Meditron.png'],\n ['154_4b4_Al_mc_AKGC417L.png'],\n ['198_1b5_Ll_mc_AKGC417L.png'],\n ['147_1b4_Tc_mc_AKGC417L.png'],\n ['130_3p3_Al_mc_AKGC417L.png'],\n ['170_1b2_Lr_mc_AKGC417L.png'],\n ['200_2p3_Tc_mc_AKGC417L.png'],\n ['181_1b2_Ar_mc_LittC2SE.png'],\n ['124_1b1_Ar_sc_Litt3200.png'],\n ['130_3p3_Pr_mc_AKGC417L.png'],\n ['166_1p1_Ar_sc_Meditron.png'],\n ['224_1b1_Tc_sc_Meditron.png'],\n ['130_3p4_Pl_mc_AKGC417L.png'],\n ['203_2p3_Ar_mc_AKGC417L.png'],\n ['215_1b2_Ar_sc_Meditron.png'],\n ['211_1p2_Pl_mc_AKGC417L.png'],\n ['172_2b5_Ar_mc_AKGC417L.png'],\n ['147_1b3_Tc_mc_AKGC417L.png'],\n ['176_1b3_Ar_mc_AKGC417L.png'],\n ['154_3b3_Ll_mc_AKGC417L.png'],\n ['122_2b1_Ar_mc_LittC2SE.png'],\n ['192_2b2_Al_mc_LittC2SE.png'],\n ['172_1b3_Lr_mc_AKGC417L.png'],\n ['130_2b3_Al_mc_AKGC417L.png'],\n ['207_2b2_Al_mc_AKGC417L.png'],\n ['112_1p1_Ll_sc_Litt3200.png'],\n ['117_1b3_Tc_mc_LittC2SE.png'],\n ['203_1p4_Tc_mc_AKGC417L.png'],\n ['135_2b2_Pl_mc_LittC2SE.png'],\n ['156_8b3_Pl_mc_AKGC417L.png'],\n ['133_3p2_Al_mc_AKGC417L.png'],\n ['130_2p5_Al_mc_AKGC417L.png'],\n ['162_2b2_Al_mc_AKGC417L.png'],\n ['141_1b3_Pr_mc_LittC2SE.png'],\n ['195_1b1_Ll_sc_Litt3200.png'],\n ['199_2b3_Ll_mc_LittC2SE.png'],\n ['108_1b1_Al_sc_Meditron.png'],\n ['138_1p3_Lr_mc_AKGC417L.png'],\n ['122_2b1_Al_mc_LittC2SE.png'],\n ['130_1p3_Tc_mc_AKGC417L.png'],\n ['163_2b2_Pl_mc_AKGC417L.png'],\n ['158_1p2_Ll_mc_AKGC417L.png'],\n ['163_2b2_Ar_mc_AKGC417L.png'],\n ['133_2p4_Pl_mc_AKGC417L.png'],\n ['174_1p3_Ll_mc_AKGC417L.png'],\n ['162_1b2_Pl_mc_AKGC417L.png'],\n ['162_1b2_Tc_mc_AKGC417L.png'],\n ['172_1b4_Ll_mc_AKGC417L.png'],\n ['191_2b1_Pr_mc_LittC2SE.png'],\n ['204_7p5_Pr_mc_AKGC417L.png'],\n ['193_1b2_Al_mc_AKGC417L.png'],\n ['135_2b1_Al_mc_LittC2SE.png'],\n ['107_2b4_Lr_mc_AKGC417L.png'],\n ['154_1b3_Lr_mc_AKGC417L.png'],\n ['107_3p2_Pl_mc_AKGC417L.png'],\n ['219_2b2_Tc_mc_LittC2SE.png'],\n ['170_1b4_Lr_mc_AKGC417L.png'],\n ['158_1p4_Ar_mc_AKGC417L.png'],\n ['135_2b2_Ar_mc_LittC2SE.png'],\n ['207_2b3_Tc_mc_AKGC417L.png'],\n ['163_2b2_Ll_mc_AKGC417L.png'],\n ['130_2b3_Tc_mc_AKGC417L.png'],\n ['123_1b1_Al_sc_Meditron.png'],\n ['145_2b2_Pr_mc_AKGC417L.png'],\n ['145_3b4_Pl_mc_AKGC417L.png'],\n ['160_1b3_Pl_mc_AKGC417L.png'],\n ['150_1b2_Al_sc_Meditron.png'],\n ['151_2p3_Tc_mc_AKGC417L.png'],\n ['187_1b1_Ll_sc_Meditron.png'],\n ['204_2b5_Ll_mc_AKGC417L.png'],\n ['138_1p3_Tc_mc_AKGC417L.png'],\n ['133_2p4_Tc_mc_AKGC417L.png'],\n ['160_2b4_Tc_mc_AKGC417L.png'],\n ['156_2b3_Pr_mc_AKGC417L.png'],\n ['160_1b4_Al_mc_AKGC417L.png'],\n ['176_1b4_Pr_mc_AKGC417L.png'],\n ['207_3b2_Ar_mc_AKGC417L.png'],\n ['176_1b3_Pr_mc_AKGC417L.png'],\n ['176_2b3_Ar_mc_AKGC417L.png'],\n ['186_3b3_Pl_mc_AKGC417L.png'],\n ['148_1b1_Al_sc_Meditron.png'],\n ['167_1b1_Al_sc_Meditron.png'],\n ['176_1b4_Al_mc_AKGC417L.png'],\n ['141_1b2_Pr_mc_LittC2SE.png'],\n ['170_2b2_Pr_mc_AKGC417L.png'],\n ['139_1b1_Al_sc_Litt3200.png'],\n ['178_1b2_Ar_mc_AKGC417L.png'],\n ['151_2p2_Ll_mc_AKGC417L.png'],\n ['200_2p3_Pr_mc_AKGC417L.png'],\n ['221_2b2_Pl_mc_LittC2SE.png'],\n ['154_4b4_Ar_mc_AKGC417L.png'],\n ['138_1p2_Lr_mc_AKGC417L.png'],\n ['133_2p4_Ar_mc_AKGC417L.png'],\n ['172_1b4_Pl_mc_AKGC417L.png'],\n ['217_1b1_Tc_sc_Meditron.png'],\n ['222_1b1_Ar_sc_Meditron.png'],\n ['162_2b4_Al_mc_AKGC417L.png'],\n ['199_2b1_Ll_mc_LittC2SE.png'],\n ['130_1p4_Al_mc_AKGC417L.png'],\n ['193_1b2_Pr_mc_AKGC417L.png'],\n ['166_1p1_Ll_sc_Meditron.png'],\n ['174_1p2_Ar_mc_AKGC417L.png'],\n ['153_1b1_Al_sc_Meditron.png'],\n ['101_1b1_Al_sc_Meditron.png'],\n ['130_3b4_Ar_mc_AKGC417L.png'],\n ['146_2b4_Al_mc_AKGC417L.png'],\n ['130_2b3_Lr_mc_AKGC417L.png'],\n ['120_1b1_Ar_sc_Meditron.png'],\n ['177_1b2_Tc_mc_AKGC417L.png'],\n ['178_1b6_Tc_mc_AKGC417L.png'],\n ['207_3b2_Tc_mc_AKGC417L.png'],\n ['197_1b1_Al_sc_Meditron.png'],\n ['207_2b4_Pl_mc_AKGC417L.png'],\n ['200_2p4_Pr_mc_AKGC417L.png'],\n ['174_1p2_Lr_mc_AKGC417L.png'],\n ['130_2b4_Al_mc_AKGC417L.png'],\n ['107_2b3_Ar_mc_AKGC417L.png'],\n ['122_2b3_Al_mc_LittC2SE.png'],\n ['213_1p5_Ar_mc_AKGC417L.png'],\n ['138_1p4_Tc_mc_AKGC417L.png'],\n ['203_1p4_Pl_mc_AKGC417L.png'],\n ['177_1b4_Ar_mc_AKGC417L.png'],\n ['163_2b2_Lr_mc_AKGC417L.png'],\n ['130_1p4_Ll_mc_AKGC417L.png'],\n ['146_8p3_Lr_mc_AKGC417L.png'],\n ['122_2b2_Al_mc_LittC2SE.png'],\n ['151_2p4_Tc_mc_AKGC417L.png'],\n ['146_2b4_Pr_mc_AKGC417L.png'],\n ['107_3p2_Ar_mc_AKGC417L.png'],\n ['206_1b1_Lr_sc_Meditron.png'],\n ['176_1b3_Ll_mc_AKGC417L.png'],\n ['158_1p2_Al_mc_AKGC417L.png'],\n ['213_1p5_Tc_mc_AKGC417L.png'],\n ['157_1b1_Lr_sc_Meditron.png'],\n ['170_1b2_Tc_mc_AKGC417L.png'],\n ['198_1b5_Tc_mc_AKGC417L.png'],\n ['172_1b4_Pr_mc_AKGC417L.png'],\n ['130_3p2_Pl_mc_AKGC417L.png'],\n ['161_1b1_Al_sc_Meditron.png'],\n ['172_1b4_Lr_mc_AKGC417L.png'],\n ['213_1p2_Ar_mc_AKGC417L.png'],\n ['151_3p2_Pl_mc_AKGC417L.png'],\n ['204_7p5_Tc_mc_AKGC417L.png'],\n ['165_1b1_Pl_sc_Meditron.png'],\n ['170_1b4_Al_mc_AKGC417L.png'],\n ['211_1p2_Ar_mc_AKGC417L.png'],\n ['178_1b2_Tc_mc_AKGC417L.png'],\n ['139_1b1_Ll_sc_Litt3200.png'],\n ['151_2p2_Pl_mc_AKGC417L.png'],\n ['177_1b2_Ar_mc_AKGC417L.png'],\n ['130_2b2_Ar_mc_AKGC417L.png'],\n ['160_2b4_Pr_mc_AKGC417L.png'],\n ['174_1p3_Pr_mc_AKGC417L.png'],\n ['160_1b2_Ar_mc_AKGC417L.png'],\n ['201_1b2_Ar_sc_Meditron.png'],\n ['172_1b5_Al_mc_AKGC417L.png'],\n ['114_1b4_Pr_mc_AKGC417L.png'],\n ['161_1b1_Pl_sc_Meditron.png'],\n ['107_2b5_Pl_mc_AKGC417L.png'],\n ['200_2p4_Lr_mc_AKGC417L.png'],\n ['205_3b4_Pl_mc_AKGC417L.png'],\n ['195_1b1_Ar_sc_Litt3200.png'],\n ['130_1p2_Ll_mc_AKGC417L.png'],\n ['104_1b1_Pl_sc_Litt3200.png'],\n ['114_1b4_Lr_mc_AKGC417L.png'],\n ['203_1p2_Al_mc_AKGC417L.png'],\n ['211_2p2_Tc_mc_AKGC417L.png'],\n ['223_1b1_Pl_sc_Meditron.png'],\n ['218_1b1_Pr_sc_Meditron.png'],\n ['158_1p2_Pr_mc_AKGC417L.png'],\n ['158_1b3_Ar_mc_LittC2SE.png'],\n ['172_1b5_Lr_mc_AKGC417L.png'],\n ['151_2p2_Lr_mc_AKGC417L.png'],\n ['163_2b2_Tc_mc_AKGC417L.png'],\n ['114_1b4_Ar_mc_AKGC417L.png'],\n ['176_1b3_Pl_mc_AKGC417L.png'],\n ['130_1p4_Tc_mc_AKGC417L.png'],\n ['193_7b3_Ar_mc_AKGC417L.png'],\n ['194_1b1_Pr_sc_Meditron.png'],\n ['186_2b2_Tc_mc_AKGC417L.png'],\n ['172_1b5_Tc_mc_AKGC417L.png'],\n ['211_2p3_Tc_mc_AKGC417L.png'],\n ['167_1b1_Pr_sc_Meditron.png'],\n ['174_2p3_Ar_mc_AKGC417L.png'],\n ['156_5b3_Al_mc_AKGC417L.png'],\n ['130_1p3_Ll_mc_AKGC417L.png'],\n ['205_2b4_Pl_mc_AKGC417L.png'],\n ['146_8p3_Pr_mc_AKGC417L.png'],\n ['124_1b1_Al_sc_Litt3200.png'],\n ['162_2b3_Lr_mc_AKGC417L.png'],\n ['119_1b1_Ar_sc_Meditron.png'],\n ['121_1p1_Tc_sc_Meditron.png'],\n ['178_1b6_Lr_mc_AKGC417L.png'],\n ['174_1p4_Ll_mc_AKGC417L.png'],\n ['138_1p3_Pr_mc_AKGC417L.png'],\n ['170_1b3_Ll_mc_AKGC417L.png'],\n ['179_1b1_Al_sc_Meditron.png'],\n ['139_1b1_Pl_sc_Litt3200.png'],\n ['130_2p5_Ar_mc_AKGC417L.png'],\n ['186_2b3_Lr_mc_AKGC417L.png'],\n ['154_1b3_Tc_mc_AKGC417L.png'],\n ['130_1p3_Lr_mc_AKGC417L.png'],\n ['180_1b4_Pl_mc_AKGC417L.png'],\n ['222_1b1_Pr_sc_Meditron.png'],\n ['212_2b2_Tc_mc_LittC2SE.png'],\n ['133_2p3_Al_mc_AKGC417L.png'],\n ['130_3b4_Lr_mc_AKGC417L.png'],\n ['195_1b1_Al_sc_Litt3200.png'],\n ['111_1b2_Tc_sc_Meditron.png'],\n ['200_2p4_Ar_mc_AKGC417L.png'],\n ['204_2b5_Ar_mc_AKGC417L.png'],\n ['186_2b2_Pl_mc_AKGC417L.png'],\n ['220_1b2_Al_mc_LittC2SE.png'],\n ['151_3p2_Pr_mc_AKGC417L.png'],\n ['162_2b3_Tc_mc_AKGC417L.png'],\n ['118_1b1_Lr_sc_Litt3200.png'],\n ['219_2b1_Ar_mc_LittC2SE.png'],\n ['200_2p3_Ar_mc_AKGC417L.png'],\n ['221_2b1_Al_mc_LittC2SE.png'],\n ['177_1b4_Pl_mc_AKGC417L.png'],\n ['207_2b4_Pr_mc_AKGC417L.png'],\n ['213_1p5_Pr_mc_AKGC417L.png'],\n ['137_1b1_Ar_sc_Meditron.png'],\n ['193_1b2_Ar_mc_AKGC417L.png'],\n ['130_1p3_Pl_mc_AKGC417L.png'],\n ['207_3b2_Pr_mc_AKGC417L.png'],\n ['107_2b4_Pr_mc_AKGC417L.png'],\n ['203_1p3_Ar_mc_AKGC417L.png'],\n ['178_1b2_Pl_mc_AKGC417L.png'],\n ['107_3p2_Lr_mc_AKGC417L.png'],\n ['154_1b3_Al_mc_AKGC417L.png'],\n ['162_2b3_Pl_mc_AKGC417L.png'],\n ['207_2b3_Al_mc_AKGC417L.png'],\n ['174_2p3_Pr_mc_AKGC417L.png'],\n ['172_1b3_Al_mc_AKGC417L.png'],\n ['166_1p1_Pr_sc_Meditron.png'],\n ['186_3b3_Al_mc_AKGC417L.png'],\n ['154_1b3_Pl_mc_AKGC417L.png'],\n ['107_3p2_Al_mc_AKGC417L.png'],\n ['191_2b1_Pl_mc_LittC2SE.png'],\n ['180_1b4_Ar_mc_AKGC417L.png'],\n ['213_1p5_Al_mc_AKGC417L.png'],\n ['176_2b3_Ll_mc_AKGC417L.png'],\n ['130_2b2_Al_mc_AKGC417L.png'],\n ['138_1p4_Lr_mc_AKGC417L.png'],\n ['107_2b5_Al_mc_AKGC417L.png'],\n ['151_2p2_Ar_mc_AKGC417L.png'],\n ['163_8b3_Al_mc_AKGC417L.png'],\n ['163_8b3_Pl_mc_AKGC417L.png'],\n ['145_2b2_Al_mc_AKGC417L.png'],\n ['121_1b1_Tc_sc_Meditron.png'],\n ['118_1b1_Ar_sc_Litt3200.png'],\n ['130_1p3_Al_mc_AKGC417L.png'],\n ['112_1p1_Pl_sc_Litt3200.png'],\n ['138_1p2_Pl_mc_AKGC417L.png'],\n ['152_1b1_Al_sc_Meditron.png'],\n ['201_1b1_Al_sc_Meditron.png'],\n ['219_2b1_Tc_mc_LittC2SE.png'],\n ['147_2b3_Al_mc_AKGC417L.png'],\n ['130_2b3_Pr_mc_AKGC417L.png'],\n ['146_8p3_Pl_mc_AKGC417L.png'],\n ['221_2b1_Lr_mc_LittC2SE.png'],\n ['170_2b2_Tc_mc_AKGC417L.png'],\n ['162_2b3_Al_mc_AKGC417L.png'],\n ['156_5b3_Ar_mc_AKGC417L.png'],\n ['154_1b3_Ll_mc_AKGC417L.png'],\n ['158_1p2_Ar_mc_AKGC417L.png'],\n ['205_4b2_Lr_mc_AKGC417L.png'],\n ['201_1b2_Al_sc_Meditron.png'],\n ['203_1p2_Lr_mc_AKGC417L.png'],\n ['162_2b2_Pr_mc_AKGC417L.png'],\n ['174_1p2_Pl_mc_AKGC417L.png'],\n ['158_1p4_Pr_mc_AKGC417L.png'],\n ['171_1b1_Al_sc_Meditron.png'],\n ['186_2b4_Tc_mc_AKGC417L.png'],\n ['118_1b1_Ll_sc_Litt3200.png'],\n ['159_1b1_Al_sc_Meditron.png'],\n ['188_1b1_Pl_sc_Meditron.png'],\n ['163_8b3_Pr_mc_AKGC417L.png'],\n ['151_2p3_Lr_mc_AKGC417L.png'],\n ['133_2p3_Ar_mc_AKGC417L.png'],\n ['226_1b1_Pl_sc_LittC2SE.png'],\n ['133_2p2_Ar_mc_AKGC417L.png'],\n ['188_1b1_Al_sc_Meditron.png'],\n ['213_1p3_Al_mc_AKGC417L.png'],\n ['110_1b1_Pr_sc_Meditron.png'],\n ['175_1b1_Pl_sc_Litt3200.png'],\n ['213_1p2_Lr_mc_AKGC417L.png'],\n ['145_3b2_Lr_mc_AKGC417L.png'],\n ['193_1b2_Pl_mc_AKGC417L.png'],\n ['213_1p3_Pl_mc_AKGC417L.png'],\n ['109_1b1_Pl_sc_Litt3200.png'],\n ['186_2b4_Lr_mc_AKGC417L.png'],\n ['186_3b3_Lr_mc_AKGC417L.png'],\n ['130_3b3_Ll_mc_AKGC417L.png'],\n ['207_2b4_Ar_mc_AKGC417L.png'],\n ['154_2b4_Pl_mc_AKGC417L.png'],\n ['190_1b1_Tc_sc_Meditron.png'],\n ['168_1b1_Al_sc_Meditron.png'],\n ['158_1p2_Pl_mc_AKGC417L.png'],\n ['138_1p2_Ar_mc_AKGC417L.png'],\n ['145_2b2_Ar_mc_AKGC417L.png'],\n ['203_1p3_Pl_mc_AKGC417L.png'],\n ['107_2b4_Tc_mc_AKGC417L.png'],\n ['130_2b3_Ar_mc_AKGC417L.png'],\n ['193_7b3_Tc_mc_AKGC417L.png'],\n ['207_3b2_Al_mc_AKGC417L.png'],\n ['151_2p4_Ar_mc_AKGC417L.png'],\n ['109_1b1_Pr_sc_Litt3200.png'],\n ['130_2b3_Ll_mc_AKGC417L.png'],\n ['154_4b4_Pr_mc_AKGC417L.png'],\n ['139_1b1_Lr_sc_Litt3200.png'],\n ['160_2b3_Lr_mc_AKGC417L.png'],\n ['151_3p3_Ll_mc_AKGC417L.png'],\n ['145_3b2_Ar_mc_AKGC417L.png'],\n ['170_1b4_Pr_mc_AKGC417L.png'],\n ['104_1b1_Pr_sc_Litt3200.png'],\n ['203_1p2_Tc_mc_AKGC417L.png'],\n ['162_2b4_Pr_mc_AKGC417L.png'],\n ['134_2b2_Ar_mc_LittC2SE.png'],\n ['172_1b4_Ar_mc_AKGC417L.png'],\n ['186_2b3_Pr_mc_AKGC417L.png'],\n ['106_2b1_Pr_mc_LittC2SE.png'],\n ['206_1b1_Pl_sc_Meditron.png'],\n ['154_2b4_Pr_mc_AKGC417L.png'],\n ['193_1b2_Ll_mc_AKGC417L.png'],\n ['102_1b1_Ar_sc_Meditron.png'],\n ['113_1b1_Ll_sc_Litt3200.png'],\n ['156_8b3_Al_mc_AKGC417L.png'],\n ['125_1b1_Tc_sc_Meditron.png'],\n ['200_2p4_Tc_mc_AKGC417L.png'],\n ['109_1b1_Ar_sc_Litt3200.png'],\n ['138_1p3_Al_mc_AKGC417L.png'],\n ['178_1b6_Ar_mc_AKGC417L.png'],\n ['147_2b4_Al_mc_AKGC417L.png'],\n ['156_2b3_Al_mc_AKGC417L.png'],\n ['163_8b3_Ll_mc_AKGC417L.png'],\n ['158_2p2_Ar_mc_AKGC417L.png'],\n ['151_2p4_Al_mc_AKGC417L.png'],\n ['210_1b1_Al_sc_Meditron.png'],\n ['107_2b3_Tc_mc_AKGC417L.png'],\n ['186_2b4_Al_mc_AKGC417L.png'],\n ['177_2b4_Al_mc_AKGC417L.png'],\n ['142_1b1_Pl_mc_LittC2SE.png'],\n ['111_1b3_Tc_sc_Meditron.png'],\n ['154_3b3_Al_mc_AKGC417L.png'],\n ['133_2p2_Al_mc_AKGC417L.png'],\n ['192_2b1_Ar_mc_LittC2SE.png'],\n ['177_1b4_Pr_mc_AKGC417L.png'],\n ['154_1b3_Pr_mc_AKGC417L.png'],\n ['170_2b2_Ar_mc_AKGC417L.png'],\n ['157_1b1_Pr_sc_Meditron.png'],\n ['216_1b1_Pl_sc_Meditron.png'],\n ['193_7b3_Pl_mc_AKGC417L.png'],\n ['172_1b5_Pl_mc_AKGC417L.png'],\n ['174_2p3_Pl_mc_AKGC417L.png'],\n ['183_1b1_Tc_sc_Meditron.png'],\n ['179_1b1_Tc_sc_Meditron.png'],\n ['170_2b2_Al_mc_AKGC417L.png'],\n ['130_2b4_Ar_mc_AKGC417L.png'],\n ['203_1p2_Ar_mc_AKGC417L.png'],\n ['128_1b3_Tc_mc_LittC2SE.png'],\n ['145_2b2_Lr_mc_AKGC417L.png'],\n ['165_1b1_Pr_sc_Meditron.png'],\n ['118_1b1_Pl_sc_Litt3200.png'],\n ['170_1b3_Tc_mc_AKGC417L.png'],\n ['130_1p2_Al_mc_AKGC417L.png'],\n ['147_2b3_Pl_mc_AKGC417L.png'],\n ['205_1b3_Pl_mc_AKGC417L.png'],\n ['219_2b2_Ar_mc_LittC2SE.png'],\n ['154_4b4_Ll_mc_AKGC417L.png'],\n ['154_2b4_Ll_mc_AKGC417L.png'],\n ['172_1b5_Ar_mc_AKGC417L.png'],\n ['185_1b1_Ll_sc_Litt3200.png'],\n ['176_1b4_Ll_mc_AKGC417L.png'],\n ['182_1b1_Tc_sc_Meditron.png'],\n ['156_2b3_Ll_mc_AKGC417L.png'],\n ['122_2b3_Ar_mc_LittC2SE.png'],\n ['175_1b1_Ll_sc_Litt3200.png'],\n ['170_1b4_Pl_mc_AKGC417L.png'],\n ['160_1b2_Pr_mc_AKGC417L.png'],\n ['176_1b3_Al_mc_AKGC417L.png'],\n ['147_2b4_Ll_mc_AKGC417L.png'],\n ['149_1b1_Pl_sc_Meditron.png'],\n ['178_1b2_Pr_mc_AKGC417L.png'],\n ['170_1b3_Pl_mc_AKGC417L.png'],\n ['174_1p3_Tc_mc_AKGC417L.png'],\n ['158_1p3_Al_mc_AKGC417L.png'],\n ['186_2b3_Pl_mc_AKGC417L.png'],\n ['130_3p4_Al_mc_AKGC417L.png'],\n ['130_3p2_Pr_mc_AKGC417L.png'],\n ['133_2p3_Tc_mc_AKGC417L.png'],\n ['133_3p2_Pl_mc_AKGC417L.png'],\n ['207_2b2_Ar_mc_AKGC417L.png'],\n ['151_2p3_Pr_mc_AKGC417L.png'],\n ['154_2b4_Al_mc_AKGC417L.png'],\n ['116_1b2_Pl_sc_Meditron.png'],\n ['210_1b1_Ar_sc_Meditron.png'],\n ['137_1b1_Ll_sc_Meditron.png'],\n ['177_2b4_Lr_mc_AKGC417L.png'],\n ['151_3p2_Al_mc_AKGC417L.png'],\n ['131_1b1_Al_sc_Meditron.png'],\n ['151_2p2_Al_mc_AKGC417L.png'],\n ['226_1b1_Ll_sc_Meditron.png'],\n ['178_1b3_Ar_mc_AKGC417L.png'],\n ['219_2b3_Tc_mc_LittC2SE.png'],\n ['186_2b2_Ar_mc_AKGC417L.png'],\n ['157_1b1_Al_sc_Meditron.png'],\n ['203_2p3_Pr_mc_AKGC417L.png'],\n ['213_2p2_Tc_mc_AKGC417L.png'],\n ['218_1b1_Pl_sc_Meditron.png'],\n ['151_2p3_Pl_mc_AKGC417L.png'],\n ['172_1b3_Ll_mc_AKGC417L.png'],\n ['132_2b2_Lr_mc_LittC2SE.png'],\n ['218_1b1_Al_sc_Meditron.png'],\n ['180_1b4_Lr_mc_AKGC417L.png'],\n ['185_1b1_Pl_sc_Litt3200.png'],\n ['138_1p4_Pr_mc_AKGC417L.png'],\n ['110_1p1_Al_sc_Meditron.png'],\n ['170_1b3_Pr_mc_AKGC417L.png'],\n ['221_2b3_Al_mc_LittC2SE.png'],\n ['130_3p3_Pl_mc_AKGC417L.png'],\n ['214_1b1_Ar_sc_Meditron.png'],\n ['130_1p2_Ar_mc_AKGC417L.png'],\n ['107_2b3_Pr_mc_AKGC417L.png'],\n ['186_3b3_Tc_mc_AKGC417L.png'],\n ['127_1b1_Ar_sc_Meditron.png'],\n ['120_1b1_Pr_sc_Meditron.png'],\n ['107_2b4_Al_mc_AKGC417L.png'],\n ['162_2b4_Tc_mc_AKGC417L.png'],\n ['107_2b4_Ll_mc_AKGC417L.png'],\n ['107_2b3_Ll_mc_AKGC417L.png'],\n ['218_1p1_Ar_sc_Litt3200.png'],\n ['138_2p2_Lr_mc_AKGC417L.png'],\n ['178_1b3_Pr_mc_AKGC417L.png'],\n ['130_3b4_Al_mc_AKGC417L.png'],\n ['112_1b1_Ar_sc_Meditron.png'],\n ['160_1b2_Lr_mc_AKGC417L.png'],\n ['195_1b1_Pl_sc_Litt3200.png'],\n ['177_1b4_Al_mc_AKGC417L.png'],\n ['115_1b1_Ar_sc_Meditron.png'],\n ['204_2b5_Al_mc_AKGC417L.png'],\n ['186_2b3_Tc_mc_AKGC417L.png'],\n ['183_1b1_Pl_sc_Meditron.png'],\n ['147_2b3_Lr_mc_AKGC417L.png'],\n ['200_3p4_Tc_mc_AKGC417L.png'],\n ['203_1p3_Al_mc_AKGC417L.png'],\n ['104_1b1_Ll_sc_Litt3200.png'],\n ['110_1p1_Lr_sc_Meditron.png'],\n ['172_1b4_Tc_mc_AKGC417L.png'],\n ['170_1b2_Al_mc_AKGC417L.png'],\n ['157_1b1_Pl_sc_Meditron.png'],\n ['172_2b5_Pl_mc_AKGC417L.png'],\n ['163_8b3_Lr_mc_AKGC417L.png'],\n ['174_1p3_Ar_mc_AKGC417L.png'],\n ['177_1b4_Lr_mc_AKGC417L.png'],\n ['120_1b1_Pl_sc_Meditron.png'],\n ['193_1b4_Lr_mc_AKGC417L.png'],\n ['186_2b3_Ar_mc_AKGC417L.png'],\n ['107_2b5_Pr_mc_AKGC417L.png'],\n ['175_1b1_Al_sc_Litt3200.png'],\n ['140_2b3_Tc_mc_LittC2SE.png'],\n ['200_2p2_Pl_mc_AKGC417L.png'],\n ['162_2b3_Ar_mc_AKGC417L.png'],\n ['158_1p3_Ar_mc_AKGC417L.png'],\n ['205_3b4_Al_mc_AKGC417L.png'],\n ['130_2b4_Ll_mc_AKGC417L.png'],\n ['130_3p3_Tc_mc_AKGC417L.png'],\n ['130_3p2_Tc_mc_AKGC417L.png'],\n ['203_1p3_Pr_mc_AKGC417L.png'],\n ['172_1b5_Pr_mc_AKGC417L.png'],\n ['130_1p2_Pl_mc_AKGC417L.png'],\n ['151_2p2_Tc_mc_AKGC417L.png'],\n ['193_7b3_Al_mc_AKGC417L.png'],\n ['207_2b2_Pl_mc_AKGC417L.png'],\n ['118_1b1_Pr_sc_Litt3200.png'],\n ['107_2b3_Al_mc_AKGC417L.png'],\n ['170_1b3_Ar_mc_AKGC417L.png'],\n ['174_1p4_Pl_mc_AKGC417L.png'],\n ['178_1b6_Pl_mc_AKGC417L.png'],\n ['223_1b1_Lr_sc_Meditron.png'],\n ['138_2p2_Ar_mc_AKGC417L.png'],\n ['130_1p4_Pr_mc_AKGC417L.png'],\n ['186_2b3_Al_mc_AKGC417L.png'],\n ['178_2b2_Pr_mc_AKGC417L.png'],\n ['172_1b4_Al_mc_AKGC417L.png'],\n ['223_1b1_Ll_sc_Meditron.png'],\n ['186_3b3_Pr_mc_AKGC417L.png'],\n ['170_1b2_Pr_mc_AKGC417L.png'],\n ['221_2b2_Ar_mc_LittC2SE.png'],\n ['135_2b1_Pl_mc_LittC2SE.png'],\n ['211_1p5_Ar_mc_AKGC417L.png'],\n ['221_2b1_Ar_mc_LittC2SE.png'],\n ['162_2b2_Ar_mc_AKGC417L.png'],\n ['204_7p5_Al_mc_AKGC417L.png'],\n ['107_2b4_Pl_mc_AKGC417L.png'],\n ['135_2b3_Pl_mc_LittC2SE.png'],\n ['147_2b4_Ar_mc_AKGC417L.png'],\n ['223_1b1_Pr_sc_Meditron.png'],\n ['209_1b1_Tc_sc_Meditron.png'],\n ['224_1b2_Al_sc_Meditron.png'],\n ['203_1p4_Pr_mc_AKGC417L.png'],\n ['163_2b2_Al_mc_AKGC417L.png'],\n ['169_1b1_Lr_sc_Meditron.png'],\n ['160_1b3_Tc_mc_AKGC417L.png'],\n ['178_2b2_Al_mc_AKGC417L.png'],\n ['207_2b3_Pl_mc_AKGC417L.png'],\n ['218_1b1_Lr_sc_Meditron.png'],\n ['154_2b4_Lr_mc_AKGC417L.png'],\n ['207_3b2_Pl_mc_AKGC417L.png'],\n ['135_2b3_Ar_mc_LittC2SE.png'],\n ['195_1b1_Lr_sc_Litt3200.png'],\n ['138_1p3_Ar_mc_AKGC417L.png'],\n ['200_2p3_Al_mc_AKGC417L.png'],\n ['149_1b1_Lr_sc_Meditron.png'],\n ['172_1b3_Pl_mc_AKGC417L.png'],\n ['159_1b1_Ar_sc_Meditron.png'],\n ['130_2b4_Lr_mc_AKGC417L.png'],\n ['105_1b1_Tc_sc_Meditron.png'],\n ['189_1b2_Lr_mc_LittC2SE.png'],\n ['193_1b2_Tc_mc_AKGC417L.png'],\n ['160_1b2_Tc_mc_AKGC417L.png'],\n ['156_8b3_Ar_mc_AKGC417L.png'],\n ['130_1p2_Lr_mc_AKGC417L.png'],\n ['174_1p4_Pr_mc_AKGC417L.png'],\n ['180_1b4_Pr_mc_AKGC417L.png'],\n ['160_1b4_Pl_mc_AKGC417L.png'],\n ['110_1p1_Pr_sc_Meditron.png'],\n ['185_1b1_Al_sc_Litt3200.png'],\n ['194_1b1_Lr_sc_Meditron.png'],\n ['163_2b2_Pr_mc_AKGC417L.png'],\n ['198_1b5_Al_mc_AKGC417L.png'],\n ['200_2p2_Lr_mc_AKGC417L.png'],\n ['203_1p2_Pl_mc_AKGC417L.png'],\n ['186_2b2_Pr_mc_AKGC417L.png'],\n ['151_2p4_Pl_mc_AKGC417L.png'],\n ['154_4b4_Lr_mc_AKGC417L.png'],\n ['177_1b4_Tc_mc_AKGC417L.png'],\n ['205_4b2_Ar_mc_AKGC417L.png'],\n ['172_2b5_Pr_mc_AKGC417L.png'],\n ['176_1b4_Ar_mc_AKGC417L.png'],\n ['163_8b3_Ar_mc_AKGC417L.png'],\n ['223_1b1_Al_sc_Meditron.png'],\n ['205_2b3_Al_mc_AKGC417L.png'],\n ['221_2b3_Ar_mc_LittC2SE.png'],\n ['198_6p1_Pr_mc_AKGC417L.png'],\n ['221_2b1_Pl_mc_LittC2SE.png'],\n ['134_2b1_Ar_mc_LittC2SE.png'],\n ['186_2b4_Pl_mc_AKGC417L.png'],\n ['160_1b4_Lr_mc_AKGC417L.png'],\n ['146_8p3_Al_mc_AKGC417L.png'],\n ['139_1b1_Pr_sc_Litt3200.png'],\n ['155_2b1_Al_mc_LittC2SE.png'],\n ['154_1b3_Ar_mc_AKGC417L.png'],\n ['156_5b3_Ll_mc_AKGC417L.png'],\n ['113_1b1_Lr_sc_Litt3200.png'],\n ['124_1b1_Pl_sc_Litt3200.png'],\n ['177_2b4_Pl_mc_AKGC417L.png'],\n ['104_1b1_Lr_sc_Litt3200.png'],\n ['162_2b4_Lr_mc_AKGC417L.png'],\n ['160_1b3_Ar_mc_AKGC417L.png'],\n ['226_1b1_Al_sc_Meditron.png'],\n ['207_3b2_Lr_mc_AKGC417L.png'],\n ['130_2b2_Tc_mc_AKGC417L.png'],\n ['192_2b2_Ar_mc_LittC2SE.png'],\n ['200_3p4_Pr_mc_AKGC417L.png'],\n ['156_2b3_Lr_mc_AKGC417L.png'],\n ['133_2p2_Pl_mc_AKGC417L.png'],\n ['178_1b2_Al_mc_AKGC417L.png'],\n ['177_2b4_Tc_mc_AKGC417L.png'],\n ['223_1b1_Ar_sc_Meditron.png'],\n ['135_2b1_Ar_mc_LittC2SE.png'],\n ['138_1p4_Pl_mc_AKGC417L.png'],\n ['178_1b6_Al_mc_AKGC417L.png'],\n ['130_2b2_Lr_mc_AKGC417L.png'],\n ['158_1p3_Lr_mc_AKGC417L.png'],\n ['138_1p2_Tc_mc_AKGC417L.png'],\n ['130_1p4_Ar_mc_AKGC417L.png'],\n ['141_1b2_Lr_mc_LittC2SE.png'],\n ['146_2b4_Ar_mc_AKGC417L.png'],\n ['177_2b4_Pr_mc_AKGC417L.png'],\n ['198_1b5_Pl_mc_AKGC417L.png'],\n ['175_1b1_Ar_sc_Litt3200.png'],\n ['134_2b1_Al_mc_LittC2SE.png'],\n ['114_1b4_Pl_mc_AKGC417L.png'],\n ['170_1b4_Ar_mc_AKGC417L.png'],\n ['139_1b1_Ar_sc_Litt3200.png'],\n ['158_2p3_Tc_mc_AKGC417L.png'],\n ['185_1b1_Lr_sc_Litt3200.png'],\n ['165_1b1_Ar_sc_Meditron.png'],\n ['174_1p2_Ll_mc_AKGC417L.png'],\n ['203_1p4_Ar_mc_AKGC417L.png'],\n ['112_1b1_Lr_sc_Meditron.png'],\n ['158_2p3_Lr_mc_AKGC417L.png'],\n ['130_3p4_Pr_mc_AKGC417L.png'],\n ['151_2p2_Pr_mc_AKGC417L.png'],\n ['146_2b2_Pl_mc_AKGC417L.png'],\n ['158_1p2_Tc_mc_AKGC417L.png'],\n ['181_1b1_Ar_mc_LittC2SE.png'],\n ['132_2b1_Lr_mc_LittC2SE.png'],\n ['130_1p3_Pr_mc_AKGC417L.png'],\n ['130_2b4_Pl_mc_AKGC417L.png'],\n ['160_1b4_Ar_mc_AKGC417L.png'],\n ['138_2p2_Pl_mc_AKGC417L.png'],\n ['162_1b2_Pr_mc_AKGC417L.png'],\n ['138_2p2_Pr_mc_AKGC417L.png'],\n ['138_1p2_Al_mc_AKGC417L.png'],\n ['146_2b4_Lr_mc_AKGC417L.png'],\n ['109_1b1_Ll_sc_Litt3200.png'],\n ['158_1p3_Tc_mc_AKGC417L.png'],\n ['186_2b2_Al_mc_AKGC417L.png'],\n ['173_1b1_Al_sc_Meditron.png'],\n ['181_1b1_Tc_mc_LittC2SE.png'],\n ['124_1b1_Pr_sc_Litt3200.png'],\n ['174_1p3_Lr_mc_AKGC417L.png'],\n ['156_5b3_Lr_mc_AKGC417L.png'],\n ['185_1b1_Pr_sc_Litt3200.png'],\n ['134_2b2_Al_mc_LittC2SE.png'],\n ['172_1b3_Ar_mc_AKGC417L.png'],\n ['174_1p4_Lr_mc_AKGC417L.png'],\n ['175_1b1_Lr_sc_Litt3200.png'],\n ['107_2b5_Ar_mc_AKGC417L.png'],\n ['174_1p2_Tc_mc_AKGC417L.png'],\n ['211_1p3_Ar_mc_AKGC417L.png'],\n ['166_1p1_Al_sc_Meditron.png'],\n ['186_2b2_Lr_mc_AKGC417L.png'],\n ['184_1b1_Ar_sc_Meditron.png'],\n ['203_1p2_Pr_mc_AKGC417L.png'],\n ['160_1b2_Pl_mc_AKGC417L.png'],\n ['213_2p2_Al_mc_AKGC417L.png']]"},"metadata":{}}]},{"cell_type":"code","source":"labels = []\nfor i in range(len(req_file_names)):\n    req_file_names[i].append(sr_no[req_file_names[i][0][:3]])\n    labels.append(sr_no[req_file_names[i][0][:3]])","metadata":{"id":"LCKsXMpUvggI","execution":{"iopub.status.busy":"2023-08-15T17:52:40.617695Z","iopub.execute_input":"2023-08-15T17:52:40.618302Z","iopub.status.idle":"2023-08-15T17:52:40.625807Z","shell.execute_reply.started":"2023-08-15T17:52:40.618269Z","shell.execute_reply":"2023-08-15T17:52:40.624549Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"labels *= 3","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:52:40.627344Z","iopub.execute_input":"2023-08-15T17:52:40.627739Z","iopub.status.idle":"2023-08-15T17:52:40.636901Z","shell.execute_reply.started":"2023-08-15T17:52:40.627708Z","shell.execute_reply":"2023-08-15T17:52:40.635757Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"x = []\n\nfor i in req_file_names:\n    img = cv2.imread('/kaggle/input/transferlearning-ls-data/Mel Spectrogram/Mel Spectrogram/Time Stretch/'+i[0])\n    img = cv2.resize(img, (350, 350))\n    x.append(img)\n\nfor i in req_file_names:\n    img = cv2.imread('/kaggle/input/transferlearning-ls-data/Mel Spectrogram/Mel Spectrogram/Pitch Shift/'+i[0])\n    img = cv2.resize(img, (350, 350))\n    x.append(img)\n    \nfor i in req_file_names:\n    img = cv2.imread('/kaggle/input/transferlearning-ls-data/Mel Spectrogram/Mel Spectrogram/Audio Shift/'+i[0])\n    img = cv2.resize(img, (350, 350))\n    x.append(img)\n\n# x = np.array(x)\n# print(x.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PoMb8etvpAk","outputId":"65cd04d4-ba35-460e-b497-d9b5ac091122","execution":{"iopub.status.busy":"2023-08-15T17:52:40.642049Z","iopub.execute_input":"2023-08-15T17:52:40.642357Z","iopub.status.idle":"2023-08-15T17:53:42.536255Z","shell.execute_reply.started":"2023-08-15T17:52:40.642324Z","shell.execute_reply":"2023-08-15T17:53:42.535203Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"x_new = []\ny = []\n\nfor i in range(len(labels)):\n    if labels[i]=='Asthma' or labels[i]=='Bronchiolitis':\n        continue\n    x_new.append(x[i])\n    y.append(labels[i])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:42.537650Z","iopub.execute_input":"2023-08-15T17:53:42.538006Z","iopub.status.idle":"2023-08-15T17:53:42.545702Z","shell.execute_reply.started":"2023-08-15T17:53:42.537973Z","shell.execute_reply":"2023-08-15T17:53:42.544767Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"x = np.array(x_new)\nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:42.546925Z","iopub.execute_input":"2023-08-15T17:53:42.547857Z","iopub.status.idle":"2023-08-15T17:53:42.853446Z","shell.execute_reply.started":"2023-08-15T17:53:42.547822Z","shell.execute_reply":"2023-08-15T17:53:42.852463Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(2718, 350, 350, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"req_file_names *= 3","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:42.854859Z","iopub.execute_input":"2023-08-15T17:53:42.855315Z","iopub.status.idle":"2023-08-15T17:53:42.860285Z","shell.execute_reply.started":"2023-08-15T17:53:42.855281Z","shell.execute_reply":"2023-08-15T17:53:42.859316Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"y = np.array(y)\ny.shape","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbwY828LxkMZ","outputId":"54c8937f-563d-41af-c159-3bd798c5cdf0","execution":{"iopub.status.busy":"2023-08-15T17:53:42.861777Z","iopub.execute_input":"2023-08-15T17:53:42.862379Z","iopub.status.idle":"2023-08-15T17:53:42.875659Z","shell.execute_reply.started":"2023-08-15T17:53:42.862346Z","shell.execute_reply":"2023-08-15T17:53:42.874592Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(2718,)"},"metadata":{}}]},{"cell_type":"code","source":"one_hot_y = np.array(pd.get_dummies(y))","metadata":{"id":"bTiogsRk3ujL","execution":{"iopub.status.busy":"2023-08-15T17:53:42.877414Z","iopub.execute_input":"2023-08-15T17:53:42.877756Z","iopub.status.idle":"2023-08-15T17:53:42.890488Z","shell.execute_reply.started":"2023-08-15T17:53:42.877724Z","shell.execute_reply":"2023-08-15T17:53:42.889584Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, one_hot_y, test_size=0.2, random_state=39, stratify=y)\nprint(x_train.shape, y_train.shape, x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:42.893290Z","iopub.execute_input":"2023-08-15T17:53:42.893998Z","iopub.status.idle":"2023-08-15T17:53:43.192456Z","shell.execute_reply.started":"2023-08-15T17:53:42.893966Z","shell.execute_reply":"2023-08-15T17:53:43.191296Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(2174, 350, 350, 3) (2174, 6) (544, 350, 350, 3)\n","output_type":"stream"}]},{"cell_type":"raw","source":"model = keras.Sequential()\nmodel.add(keras.layers.Conv2D(64, (7,7), activation='relu', input_shape=(350, 350, 3)))\nmodel.add(keras.layers.MaxPool2D((3,3)))\nmodel.add(keras.layers.Conv2D(128, (5,5), activation='relu'))\nmodel.add(keras.layers.MaxPool2D((3,3)))\nmodel.add(keras.layers.Conv2D(256, (3,3), activation='relu'))\n\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(128, activation='relu'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.Dense(6, activation='softmax'))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:33:07.282871Z","iopub.execute_input":"2023-08-15T09:33:07.283432Z","iopub.status.idle":"2023-08-15T09:33:11.132386Z","shell.execute_reply.started":"2023-08-15T09:33:07.283394Z","shell.execute_reply":"2023-08-15T09:33:11.129210Z"}}},{"cell_type":"code","source":"checkpoint_cnn = keras.callbacks.ModelCheckpoint(\"/kaggle/working/CNN_model.h5\", save_best_only = True)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:43.194246Z","iopub.execute_input":"2023-08-15T17:53:43.194880Z","iopub.status.idle":"2023-08-15T17:53:43.199846Z","shell.execute_reply.started":"2023-08-15T17:53:43.194844Z","shell.execute_reply":"2023-08-15T17:53:43.198619Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"input1 = keras.layers.Input(shape=(350, 350, 3))\nconv1 = keras.layers.Conv2D(64, (7,7), activation='relu')(input1)\nmp1 = keras.layers.MaxPool2D((2,2))(conv1)\nconv2 = keras.layers.Conv2D(128, (5,5), activation='relu')(mp1)\nmp2 = keras.layers.MaxPool2D((2,2))(conv2)\ndo1 = keras.layers.Dropout(0.2)(mp2)\nconv3 = keras.layers.Conv2D(256, (3,3), activation='relu')(do1)\nmp3 = keras.layers.MaxPool2D((2,2))(conv3)\n\nrandom = np.random.random((mp3.shape[1:]))\n\nattention = keras.layers.Attention()([mp3, random])\n\nflatten = keras.layers.Flatten()(attention)\ndense1 = keras.layers.Dense(128, activation='relu')(flatten)\nbn = keras.layers.BatchNormalization()(dense1)\ndense2 = keras.layers.Dense(64, activation='relu')(bn)\noutput = keras.layers.Dense(6, activation='softmax')(dense2)\n\nmodel = keras.Model(inputs=input1, outputs=output)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:43.201697Z","iopub.execute_input":"2023-08-15T17:53:43.202115Z","iopub.status.idle":"2023-08-15T17:53:46.238669Z","shell.execute_reply.started":"2023-08-15T17:53:43.202082Z","shell.execute_reply":"2023-08-15T17:53:46.237893Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 350, 350, 3)]     0         \n                                                                 \n conv2d (Conv2D)             (None, 344, 344, 64)      9472      \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 172, 172, 64)     0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 168, 168, 128)     204928    \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 84, 84, 128)      0         \n 2D)                                                             \n                                                                 \n dropout (Dropout)           (None, 84, 84, 128)       0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 82, 82, 256)       295168    \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 41, 41, 256)      0         \n 2D)                                                             \n                                                                 \n attention (Attention)       (None, 41, 41, 256)       0         \n                                                                 \n flatten (Flatten)           (None, 430336)            0         \n                                                                 \n dense (Dense)               (None, 128)               55083136  \n                                                                 \n batch_normalization (BatchN  (None, 128)              512       \n ormalization)                                                   \n                                                                 \n dense_1 (Dense)             (None, 64)                8256      \n                                                                 \n dense_2 (Dense)             (None, 6)                 390       \n                                                                 \n=================================================================\nTotal params: 55,601,862\nTrainable params: 55,601,606\nNon-trainable params: 256\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:46.239744Z","iopub.execute_input":"2023-08-15T17:53:46.240195Z","iopub.status.idle":"2023-08-15T17:53:46.268953Z","shell.execute_reply.started":"2023-08-15T17:53:46.240163Z","shell.execute_reply":"2023-08-15T17:53:46.268073Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"history_cnn = model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test), callbacks=[checkpoint_cnn])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T17:53:46.270648Z","iopub.execute_input":"2023-08-15T17:53:46.271314Z","iopub.status.idle":"2023-08-15T18:31:13.447559Z","shell.execute_reply.started":"2023-08-15T17:53:46.271280Z","shell.execute_reply":"2023-08-15T18:31:13.446557Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/120\n","output_type":"stream"},{"name":"stderr","text":"2023-08-15 17:53:49.045718: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"68/68 [==============================] - 37s 351ms/step - loss: 1.2440 - accuracy: 0.5630 - val_loss: 6.1148 - val_accuracy: 0.0404\nEpoch 2/120\n68/68 [==============================] - 21s 309ms/step - loss: 0.6802 - accuracy: 0.8450 - val_loss: 3.0475 - val_accuracy: 0.0735\nEpoch 3/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.5689 - accuracy: 0.8707 - val_loss: 3.0590 - val_accuracy: 0.3254\nEpoch 4/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.5132 - accuracy: 0.8790 - val_loss: 0.7496 - val_accuracy: 0.7132\nEpoch 5/120\n68/68 [==============================] - 21s 303ms/step - loss: 0.4734 - accuracy: 0.8924 - val_loss: 0.5432 - val_accuracy: 0.8346\nEpoch 6/120\n68/68 [==============================] - 20s 300ms/step - loss: 0.4391 - accuracy: 0.8997 - val_loss: 0.3716 - val_accuracy: 0.8934\nEpoch 7/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.4101 - accuracy: 0.9020 - val_loss: 0.4362 - val_accuracy: 0.8805\nEpoch 8/120\n68/68 [==============================] - 21s 308ms/step - loss: 0.3896 - accuracy: 0.9131 - val_loss: 0.3287 - val_accuracy: 0.9044\nEpoch 9/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.3644 - accuracy: 0.9167 - val_loss: 0.3499 - val_accuracy: 0.9173\nEpoch 10/120\n68/68 [==============================] - 21s 306ms/step - loss: 0.3612 - accuracy: 0.9149 - val_loss: 0.2853 - val_accuracy: 0.9081\nEpoch 11/120\n68/68 [==============================] - 20s 296ms/step - loss: 0.3320 - accuracy: 0.9255 - val_loss: 0.2838 - val_accuracy: 0.9118\nEpoch 12/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.3191 - accuracy: 0.9324 - val_loss: 0.2900 - val_accuracy: 0.9044\nEpoch 13/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.2985 - accuracy: 0.9416 - val_loss: 0.2665 - val_accuracy: 0.9246\nEpoch 14/120\n68/68 [==============================] - 21s 303ms/step - loss: 0.2965 - accuracy: 0.9333 - val_loss: 0.2601 - val_accuracy: 0.9191\nEpoch 15/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.2802 - accuracy: 0.9430 - val_loss: 0.2426 - val_accuracy: 0.9265\nEpoch 16/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.2738 - accuracy: 0.9411 - val_loss: 30.5680 - val_accuracy: 0.0404\nEpoch 17/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.2498 - accuracy: 0.9494 - val_loss: 2.2014 - val_accuracy: 0.0662\nEpoch 18/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.2489 - accuracy: 0.9443 - val_loss: 0.6249 - val_accuracy: 0.7812\nEpoch 19/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.2383 - accuracy: 0.9503 - val_loss: 0.5892 - val_accuracy: 0.7794\nEpoch 20/120\n68/68 [==============================] - 17s 257ms/step - loss: 0.2193 - accuracy: 0.9609 - val_loss: 0.2610 - val_accuracy: 0.9265\nEpoch 21/120\n68/68 [==============================] - 20s 296ms/step - loss: 0.2266 - accuracy: 0.9499 - val_loss: 0.2246 - val_accuracy: 0.9449\nEpoch 22/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.2054 - accuracy: 0.9586 - val_loss: 0.2240 - val_accuracy: 0.9154\nEpoch 23/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.1967 - accuracy: 0.9623 - val_loss: 0.1916 - val_accuracy: 0.9338\nEpoch 24/120\n68/68 [==============================] - 21s 307ms/step - loss: 0.1917 - accuracy: 0.9623 - val_loss: 0.1741 - val_accuracy: 0.9375\nEpoch 25/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.1801 - accuracy: 0.9678 - val_loss: 0.1582 - val_accuracy: 0.9449\nEpoch 26/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.1666 - accuracy: 0.9724 - val_loss: 0.1764 - val_accuracy: 0.9632\nEpoch 27/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.1593 - accuracy: 0.9770 - val_loss: 0.1657 - val_accuracy: 0.9632\nEpoch 28/120\n68/68 [==============================] - 17s 257ms/step - loss: 0.1494 - accuracy: 0.9756 - val_loss: 0.1730 - val_accuracy: 0.9504\nEpoch 29/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.1480 - accuracy: 0.9756 - val_loss: 0.1675 - val_accuracy: 0.9504\nEpoch 30/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.1411 - accuracy: 0.9830 - val_loss: 0.1401 - val_accuracy: 0.9596\nEpoch 31/120\n68/68 [==============================] - 17s 257ms/step - loss: 0.1272 - accuracy: 0.9839 - val_loss: 0.1705 - val_accuracy: 0.9706\nEpoch 32/120\n68/68 [==============================] - 20s 294ms/step - loss: 0.1295 - accuracy: 0.9793 - val_loss: 0.1341 - val_accuracy: 0.9761\nEpoch 33/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.1229 - accuracy: 0.9839 - val_loss: 0.1548 - val_accuracy: 0.9798\nEpoch 34/120\n68/68 [==============================] - 20s 294ms/step - loss: 0.1108 - accuracy: 0.9890 - val_loss: 0.1020 - val_accuracy: 0.9779\nEpoch 35/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.1118 - accuracy: 0.9857 - val_loss: 0.1040 - val_accuracy: 0.9761\nEpoch 36/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.1094 - accuracy: 0.9839 - val_loss: 0.1118 - val_accuracy: 0.9669\nEpoch 37/120\n68/68 [==============================] - 21s 304ms/step - loss: 0.0929 - accuracy: 0.9949 - val_loss: 0.0843 - val_accuracy: 0.9853\nEpoch 38/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.0903 - accuracy: 0.9917 - val_loss: 1.7415 - val_accuracy: 0.8750\nEpoch 39/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0840 - accuracy: 0.9931 - val_loss: 0.2270 - val_accuracy: 0.9375\nEpoch 40/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.0900 - accuracy: 0.9880 - val_loss: 0.1131 - val_accuracy: 0.9798\nEpoch 41/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.0832 - accuracy: 0.9922 - val_loss: 0.0906 - val_accuracy: 0.9688\nEpoch 42/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0766 - accuracy: 0.9931 - val_loss: 0.1026 - val_accuracy: 0.9706\nEpoch 43/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0742 - accuracy: 0.9926 - val_loss: 0.0899 - val_accuracy: 0.9779\nEpoch 44/120\n68/68 [==============================] - 20s 296ms/step - loss: 0.0713 - accuracy: 0.9954 - val_loss: 0.0744 - val_accuracy: 0.9798\nEpoch 45/120\n68/68 [==============================] - 20s 294ms/step - loss: 0.0587 - accuracy: 0.9968 - val_loss: 0.0656 - val_accuracy: 0.9926\nEpoch 46/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0635 - accuracy: 0.9931 - val_loss: 0.0853 - val_accuracy: 0.9890\nEpoch 47/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.0564 - accuracy: 0.9972 - val_loss: 0.0520 - val_accuracy: 0.9963\nEpoch 48/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0555 - accuracy: 0.9959 - val_loss: 0.0612 - val_accuracy: 0.9890\nEpoch 49/120\n68/68 [==============================] - 20s 293ms/step - loss: 0.0542 - accuracy: 0.9963 - val_loss: 0.0505 - val_accuracy: 0.9926\nEpoch 50/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.0508 - accuracy: 0.9982 - val_loss: 0.0492 - val_accuracy: 0.9926\nEpoch 51/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0519 - accuracy: 0.9972 - val_loss: 0.0550 - val_accuracy: 0.9908\nEpoch 52/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0474 - accuracy: 0.9977 - val_loss: 0.0517 - val_accuracy: 0.9908\nEpoch 53/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0434 - accuracy: 0.9977 - val_loss: 0.0524 - val_accuracy: 0.9853\nEpoch 54/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.0385 - accuracy: 0.9986 - val_loss: 0.0490 - val_accuracy: 0.9871\nEpoch 55/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.0386 - accuracy: 0.9986 - val_loss: 0.0461 - val_accuracy: 0.9908\nEpoch 56/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0356 - accuracy: 0.9995 - val_loss: 0.0521 - val_accuracy: 0.9798\nEpoch 57/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0364 - accuracy: 0.9982 - val_loss: 0.0481 - val_accuracy: 0.9871\nEpoch 58/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0383 - accuracy: 0.9991 - val_loss: 0.0502 - val_accuracy: 0.9871\nEpoch 59/120\n68/68 [==============================] - 20s 294ms/step - loss: 0.0325 - accuracy: 0.9986 - val_loss: 0.0379 - val_accuracy: 0.9890\nEpoch 60/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.0305 - accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 1.0000\nEpoch 61/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0313 - accuracy: 0.9986 - val_loss: 0.0472 - val_accuracy: 0.9871\nEpoch 62/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.0342 - val_accuracy: 0.9908\nEpoch 63/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0288 - accuracy: 0.9991 - val_loss: 0.0391 - val_accuracy: 0.9908\nEpoch 64/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0295 - accuracy: 0.9991 - val_loss: 0.0474 - val_accuracy: 0.9890\nEpoch 65/120\n68/68 [==============================] - 21s 305ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 0.9945\nEpoch 66/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.0365 - val_accuracy: 0.9908\nEpoch 67/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0255 - accuracy: 0.9995 - val_loss: 0.0326 - val_accuracy: 0.9926\nEpoch 68/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9890\nEpoch 69/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.0242 - val_accuracy: 0.9982\nEpoch 70/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 0.9908\nEpoch 71/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 0.9982\nEpoch 72/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 0.9890\nEpoch 73/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0201 - accuracy: 0.9995 - val_loss: 0.0311 - val_accuracy: 0.9871\nEpoch 74/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0297 - val_accuracy: 0.9963\nEpoch 75/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 0.9926\nEpoch 76/120\n68/68 [==============================] - 21s 303ms/step - loss: 0.0199 - accuracy: 0.9991 - val_loss: 0.0226 - val_accuracy: 0.9945\nEpoch 77/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 0.9982\nEpoch 78/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9982\nEpoch 79/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\nEpoch 80/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0191 - accuracy: 0.9995 - val_loss: 0.0229 - val_accuracy: 0.9982\nEpoch 81/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0172 - accuracy: 0.9995 - val_loss: 0.0227 - val_accuracy: 1.0000\nEpoch 82/120\n68/68 [==============================] - 20s 294ms/step - loss: 0.0176 - accuracy: 0.9995 - val_loss: 0.0194 - val_accuracy: 1.0000\nEpoch 83/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0220 - val_accuracy: 0.9926\nEpoch 84/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9926\nEpoch 85/120\n68/68 [==============================] - 20s 301ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 0.9963\nEpoch 86/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000\nEpoch 87/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9945\nEpoch 88/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 0.9963\nEpoch 89/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 0.9945\nEpoch 90/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0106 - accuracy: 0.9995 - val_loss: 0.0138 - val_accuracy: 0.9982\nEpoch 91/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 0.9982\nEpoch 92/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 0.9982\nEpoch 93/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9963\nEpoch 94/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0094 - accuracy: 0.9995 - val_loss: 0.0151 - val_accuracy: 0.9982\nEpoch 95/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 0.9982\nEpoch 96/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.0144 - val_accuracy: 0.9945\nEpoch 97/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0086 - accuracy: 0.9995 - val_loss: 0.0174 - val_accuracy: 0.9982\nEpoch 98/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 0.9945\nEpoch 99/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0115 - accuracy: 0.9991 - val_loss: 0.0274 - val_accuracy: 0.9926\nEpoch 100/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 0.9982\nEpoch 101/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 0.9963\nEpoch 102/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 0.9963\nEpoch 103/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 0.9963\nEpoch 104/120\n68/68 [==============================] - 20s 302ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 0.9982\nEpoch 105/120\n68/68 [==============================] - 20s 300ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 0.9982\nEpoch 106/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9926\nEpoch 107/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 0.9963\nEpoch 108/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 0.9945\nEpoch 109/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 0.9963\nEpoch 110/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 0.9963\nEpoch 111/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 0.9963\nEpoch 112/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 0.9963\nEpoch 113/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0200 - val_accuracy: 0.9926\nEpoch 114/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 0.9945\nEpoch 115/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 0.9945\nEpoch 116/120\n68/68 [==============================] - 20s 295ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 0.9982\nEpoch 117/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 0.9945\nEpoch 118/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 0.9963\nEpoch 119/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.0280 - val_accuracy: 0.9982\nEpoch 120/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.0077 - accuracy: 0.9995 - val_loss: 0.0176 - val_accuracy: 0.9945\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(history_cnn.history)\ndf.to_csv('/kaggle/working/CNN_history.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:31:13.450972Z","iopub.execute_input":"2023-08-15T18:31:13.451312Z","iopub.status.idle":"2023-08-15T18:31:13.462817Z","shell.execute_reply.started":"2023-08-15T18:31:13.451285Z","shell.execute_reply":"2023-08-15T18:31:13.461778Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Transfer Learning","metadata":{}},{"cell_type":"markdown","source":"### Xception model","metadata":{}},{"cell_type":"code","source":"xception_wo_top = keras.applications.xception.Xception(include_top=False, weights='imagenet', input_shape=(350, 350, 3))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:31:13.464455Z","iopub.execute_input":"2023-08-15T18:31:13.464803Z","iopub.status.idle":"2023-08-15T18:31:15.389856Z","shell.execute_reply.started":"2023-08-15T18:31:13.464772Z","shell.execute_reply":"2023-08-15T18:31:15.388784Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83683744/83683744 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"xception_wo_top.trainable = False","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:31:15.391413Z","iopub.execute_input":"2023-08-15T18:31:15.391764Z","iopub.status.idle":"2023-08-15T18:31:15.402198Z","shell.execute_reply.started":"2023-08-15T18:31:15.391731Z","shell.execute_reply":"2023-08-15T18:31:15.401128Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"raw","source":"xception_model = keras.models.Sequential()\nxception_model.add(xception_wo_top)\nxception_model.add(keras.layers.Flatten())\nxception_model.add(keras.layers.Dense(128, activation='relu'))\nxception_model.add(keras.layers.Dense(64, activation='relu'))\nxception_model.add(keras.layers.Dense(8, activation='softmax'))\n\nxception_model.summary()","metadata":{"execution":{"iopub.execute_input":"2023-08-08T22:22:35.863120Z","iopub.status.busy":"2023-08-08T22:22:35.862042Z","iopub.status.idle":"2023-08-08T22:22:36.477488Z","shell.execute_reply":"2023-08-08T22:22:36.476629Z","shell.execute_reply.started":"2023-08-08T22:22:35.863076Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"input_xception = keras.layers.Input(shape=(350, 350, 3))\nxception_layer = xception_wo_top(input_xception)\n\nrandom = np.random.random(xception_layer.shape[1:])\n\nattention = keras.layers.Attention()([xception_layer, random])\n\nflatten = keras.layers.Flatten()(attention)\n\ndense1 = keras.layers.Dense(128, activation='relu')(flatten)\nbn1 = keras.layers.BatchNormalization()(dense1)\ndense2 = keras.layers.Dense(64, activation='relu')(bn1)\noutput = keras.layers.Dense(6, activation='softmax')(dense2)\n\nxception_model = keras.Model(inputs=input_xception, outputs=output)\nxception_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:31:15.403545Z","iopub.execute_input":"2023-08-15T18:31:15.404007Z","iopub.status.idle":"2023-08-15T18:31:15.941118Z","shell.execute_reply.started":"2023-08-15T18:31:15.403975Z","shell.execute_reply":"2023-08-15T18:31:15.940111Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 350, 350, 3)]     0         \n                                                                 \n xception (Functional)       (None, 11, 11, 2048)      20861480  \n                                                                 \n attention_1 (Attention)     (None, 11, 11, 2048)      0         \n                                                                 \n flatten_1 (Flatten)         (None, 247808)            0         \n                                                                 \n dense_3 (Dense)             (None, 128)               31719552  \n                                                                 \n batch_normalization_5 (Batc  (None, 128)              512       \n hNormalization)                                                 \n                                                                 \n dense_4 (Dense)             (None, 64)                8256      \n                                                                 \n dense_5 (Dense)             (None, 6)                 390       \n                                                                 \n=================================================================\nTotal params: 52,590,190\nTrainable params: 31,728,454\nNon-trainable params: 20,861,736\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint_xception = keras.callbacks.ModelCheckpoint('/kaggle/working/Xception_Model.h5', save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:31:15.942429Z","iopub.execute_input":"2023-08-15T18:31:15.942717Z","iopub.status.idle":"2023-08-15T18:31:15.948304Z","shell.execute_reply.started":"2023-08-15T18:31:15.942692Z","shell.execute_reply":"2023-08-15T18:31:15.947384Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"xception_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:31:15.949456Z","iopub.execute_input":"2023-08-15T18:31:15.949859Z","iopub.status.idle":"2023-08-15T18:31:15.974318Z","shell.execute_reply.started":"2023-08-15T18:31:15.949827Z","shell.execute_reply":"2023-08-15T18:31:15.973326Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"history_xception = xception_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test), callbacks=[checkpoint_xception])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T18:31:15.975701Z","iopub.execute_input":"2023-08-15T18:31:15.976036Z","iopub.status.idle":"2023-08-15T19:02:42.809951Z","shell.execute_reply.started":"2023-08-15T18:31:15.976004Z","shell.execute_reply":"2023-08-15T19:02:42.808860Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Epoch 1/120\n68/68 [==============================] - 25s 281ms/step - loss: 0.9554 - accuracy: 0.7562 - val_loss: 6.1404 - val_accuracy: 0.0018\nEpoch 2/120\n68/68 [==============================] - 19s 277ms/step - loss: 0.6093 - accuracy: 0.8859 - val_loss: 2.6626 - val_accuracy: 0.0221\nEpoch 3/120\n68/68 [==============================] - 16s 243ms/step - loss: 0.5296 - accuracy: 0.9117 - val_loss: 1.1192 - val_accuracy: 0.5533\nEpoch 4/120\n68/68 [==============================] - 16s 243ms/step - loss: 0.4750 - accuracy: 0.9255 - val_loss: 0.6902 - val_accuracy: 0.8327\nEpoch 5/120\n68/68 [==============================] - 19s 277ms/step - loss: 0.4358 - accuracy: 0.9315 - val_loss: 0.5916 - val_accuracy: 0.8695\nEpoch 6/120\n68/68 [==============================] - 16s 242ms/step - loss: 0.4083 - accuracy: 0.9448 - val_loss: 0.5628 - val_accuracy: 0.8768\nEpoch 7/120\n68/68 [==============================] - 16s 241ms/step - loss: 0.3833 - accuracy: 0.9545 - val_loss: 0.4892 - val_accuracy: 0.8934\nEpoch 8/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.3571 - accuracy: 0.9600 - val_loss: 0.4898 - val_accuracy: 0.8952\nEpoch 9/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.3333 - accuracy: 0.9577 - val_loss: 0.5127 - val_accuracy: 0.8732\nEpoch 10/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.3131 - accuracy: 0.9692 - val_loss: 0.5007 - val_accuracy: 0.8842\nEpoch 11/120\n68/68 [==============================] - 16s 242ms/step - loss: 0.2946 - accuracy: 0.9747 - val_loss: 0.4318 - val_accuracy: 0.8989\nEpoch 12/120\n68/68 [==============================] - 15s 215ms/step - loss: 0.2760 - accuracy: 0.9738 - val_loss: 0.5308 - val_accuracy: 0.8842\nEpoch 13/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.2699 - accuracy: 0.9729 - val_loss: 0.5715 - val_accuracy: 0.8603\nEpoch 14/120\n68/68 [==============================] - 16s 243ms/step - loss: 0.2553 - accuracy: 0.9775 - val_loss: 0.4296 - val_accuracy: 0.8952\nEpoch 15/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.2427 - accuracy: 0.9802 - val_loss: 0.4790 - val_accuracy: 0.8879\nEpoch 16/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.2390 - accuracy: 0.9807 - val_loss: 0.4712 - val_accuracy: 0.8750\nEpoch 17/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.2136 - accuracy: 0.9890 - val_loss: 0.4376 - val_accuracy: 0.8860\nEpoch 18/120\n68/68 [==============================] - 19s 276ms/step - loss: 0.2140 - accuracy: 0.9844 - val_loss: 0.3986 - val_accuracy: 0.8915\nEpoch 19/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.1994 - accuracy: 0.9871 - val_loss: 0.4357 - val_accuracy: 0.8842\nEpoch 20/120\n68/68 [==============================] - 19s 276ms/step - loss: 0.1848 - accuracy: 0.9890 - val_loss: 0.3967 - val_accuracy: 0.9044\nEpoch 21/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.1850 - accuracy: 0.9894 - val_loss: 0.4050 - val_accuracy: 0.9007\nEpoch 22/120\n68/68 [==============================] - 16s 243ms/step - loss: 0.1751 - accuracy: 0.9913 - val_loss: 0.3935 - val_accuracy: 0.8879\nEpoch 23/120\n68/68 [==============================] - 16s 241ms/step - loss: 0.1741 - accuracy: 0.9880 - val_loss: 0.3930 - val_accuracy: 0.8989\nEpoch 24/120\n68/68 [==============================] - 15s 215ms/step - loss: 0.1592 - accuracy: 0.9913 - val_loss: 0.3954 - val_accuracy: 0.8989\nEpoch 25/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.1559 - accuracy: 0.9926 - val_loss: 0.4205 - val_accuracy: 0.8860\nEpoch 26/120\n68/68 [==============================] - 15s 215ms/step - loss: 0.1530 - accuracy: 0.9913 - val_loss: 0.4228 - val_accuracy: 0.8842\nEpoch 27/120\n68/68 [==============================] - 16s 241ms/step - loss: 0.1448 - accuracy: 0.9931 - val_loss: 0.3868 - val_accuracy: 0.8989\nEpoch 28/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.1403 - accuracy: 0.9945 - val_loss: 0.4549 - val_accuracy: 0.8768\nEpoch 29/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.1375 - accuracy: 0.9913 - val_loss: 0.4322 - val_accuracy: 0.8732\nEpoch 30/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.1349 - accuracy: 0.9936 - val_loss: 0.4673 - val_accuracy: 0.8695\nEpoch 31/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.1270 - accuracy: 0.9949 - val_loss: 0.4145 - val_accuracy: 0.8787\nEpoch 32/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.1176 - accuracy: 0.9963 - val_loss: 0.3954 - val_accuracy: 0.8897\nEpoch 33/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.1182 - accuracy: 0.9940 - val_loss: 0.4083 - val_accuracy: 0.8824\nEpoch 34/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.1132 - accuracy: 0.9954 - val_loss: 0.3960 - val_accuracy: 0.8952\nEpoch 35/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.1112 - accuracy: 0.9940 - val_loss: 0.3917 - val_accuracy: 0.8787\nEpoch 36/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.1073 - accuracy: 0.9949 - val_loss: 0.4055 - val_accuracy: 0.8768\nEpoch 37/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.1021 - accuracy: 0.9959 - val_loss: 0.4294 - val_accuracy: 0.8842\nEpoch 38/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.1036 - accuracy: 0.9963 - val_loss: 0.3910 - val_accuracy: 0.8897\nEpoch 39/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0943 - accuracy: 0.9977 - val_loss: 0.4389 - val_accuracy: 0.8658\nEpoch 40/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0915 - accuracy: 0.9977 - val_loss: 0.4195 - val_accuracy: 0.8713\nEpoch 41/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0866 - accuracy: 0.9977 - val_loss: 0.4104 - val_accuracy: 0.8768\nEpoch 42/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.0861 - accuracy: 0.9982 - val_loss: 0.3959 - val_accuracy: 0.8860\nEpoch 43/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0826 - accuracy: 0.9977 - val_loss: 0.3970 - val_accuracy: 0.8842\nEpoch 44/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0808 - accuracy: 0.9972 - val_loss: 0.3964 - val_accuracy: 0.8934\nEpoch 45/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0807 - accuracy: 0.9977 - val_loss: 0.4267 - val_accuracy: 0.8658\nEpoch 46/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0744 - accuracy: 0.9982 - val_loss: 0.4290 - val_accuracy: 0.8750\nEpoch 47/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0748 - accuracy: 0.9986 - val_loss: 0.4158 - val_accuracy: 0.8805\nEpoch 48/120\n68/68 [==============================] - 17s 247ms/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 0.4103 - val_accuracy: 0.8860\nEpoch 49/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0690 - accuracy: 0.9991 - val_loss: 0.4361 - val_accuracy: 0.8695\nEpoch 50/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.0655 - accuracy: 0.9986 - val_loss: 0.4029 - val_accuracy: 0.8879\nEpoch 51/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0698 - accuracy: 0.9972 - val_loss: 0.4138 - val_accuracy: 0.8805\nEpoch 52/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0696 - accuracy: 0.9959 - val_loss: 0.4342 - val_accuracy: 0.8695\nEpoch 53/120\n68/68 [==============================] - 15s 215ms/step - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.3916 - val_accuracy: 0.8824\nEpoch 54/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0576 - accuracy: 0.9986 - val_loss: 0.4107 - val_accuracy: 0.8860\nEpoch 55/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0560 - accuracy: 0.9991 - val_loss: 0.4148 - val_accuracy: 0.8879\nEpoch 56/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0585 - accuracy: 0.9991 - val_loss: 0.4275 - val_accuracy: 0.8713\nEpoch 57/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0570 - accuracy: 0.9986 - val_loss: 0.4172 - val_accuracy: 0.8750\nEpoch 58/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0538 - accuracy: 0.9991 - val_loss: 0.4192 - val_accuracy: 0.8713\nEpoch 59/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0570 - accuracy: 0.9982 - val_loss: 0.4119 - val_accuracy: 0.8860\nEpoch 60/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0558 - accuracy: 0.9991 - val_loss: 0.4090 - val_accuracy: 0.8934\nEpoch 61/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0490 - accuracy: 0.9991 - val_loss: 0.4520 - val_accuracy: 0.8658\nEpoch 62/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.0478 - accuracy: 0.9991 - val_loss: 0.4347 - val_accuracy: 0.8750\nEpoch 63/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.4226 - val_accuracy: 0.8805\nEpoch 64/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0464 - accuracy: 0.9977 - val_loss: 0.4229 - val_accuracy: 0.8676\nEpoch 65/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.0462 - accuracy: 0.9991 - val_loss: 0.4303 - val_accuracy: 0.8676\nEpoch 66/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0454 - accuracy: 0.9991 - val_loss: 0.4306 - val_accuracy: 0.8842\nEpoch 67/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0426 - accuracy: 0.9991 - val_loss: 0.4130 - val_accuracy: 0.8787\nEpoch 68/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 0.8805\nEpoch 69/120\n68/68 [==============================] - 17s 247ms/step - loss: 0.0442 - accuracy: 0.9982 - val_loss: 0.4166 - val_accuracy: 0.8842\nEpoch 70/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0392 - accuracy: 0.9991 - val_loss: 0.4359 - val_accuracy: 0.8934\nEpoch 71/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0391 - accuracy: 0.9977 - val_loss: 0.4315 - val_accuracy: 0.8713\nEpoch 72/120\n68/68 [==============================] - 15s 215ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.8842\nEpoch 73/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0338 - accuracy: 0.9995 - val_loss: 0.4267 - val_accuracy: 0.8805\nEpoch 74/120\n68/68 [==============================] - 15s 215ms/step - loss: 0.0361 - accuracy: 0.9982 - val_loss: 0.4162 - val_accuracy: 0.8824\nEpoch 75/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0364 - accuracy: 0.9995 - val_loss: 0.5147 - val_accuracy: 0.8327\nEpoch 76/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 0.8474\nEpoch 77/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0347 - accuracy: 0.9986 - val_loss: 0.5322 - val_accuracy: 0.8382\nEpoch 78/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0354 - accuracy: 0.9995 - val_loss: 0.4567 - val_accuracy: 0.8695\nEpoch 79/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.4297 - val_accuracy: 0.8787\nEpoch 80/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0275 - accuracy: 0.9995 - val_loss: 0.4344 - val_accuracy: 0.8805\nEpoch 81/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0301 - accuracy: 0.9991 - val_loss: 0.4406 - val_accuracy: 0.8860\nEpoch 82/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.4271 - val_accuracy: 0.8750\nEpoch 83/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0300 - accuracy: 0.9991 - val_loss: 0.4540 - val_accuracy: 0.8787\nEpoch 84/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0272 - accuracy: 0.9986 - val_loss: 0.5583 - val_accuracy: 0.8750\nEpoch 85/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0272 - accuracy: 0.9991 - val_loss: 0.4969 - val_accuracy: 0.8805\nEpoch 86/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.4664 - val_accuracy: 0.8842\nEpoch 87/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.4634 - val_accuracy: 0.8805\nEpoch 88/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0254 - accuracy: 0.9995 - val_loss: 0.4637 - val_accuracy: 0.8768\nEpoch 89/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.0249 - accuracy: 0.9995 - val_loss: 0.4599 - val_accuracy: 0.8805\nEpoch 90/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0260 - accuracy: 0.9982 - val_loss: 0.5073 - val_accuracy: 0.8419\nEpoch 91/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0254 - accuracy: 0.9995 - val_loss: 0.4520 - val_accuracy: 0.8621\nEpoch 92/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0279 - accuracy: 0.9986 - val_loss: 0.4631 - val_accuracy: 0.8713\nEpoch 93/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.4437 - val_accuracy: 0.8695\nEpoch 94/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0212 - accuracy: 0.9995 - val_loss: 0.4622 - val_accuracy: 0.8640\nEpoch 95/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0199 - accuracy: 0.9995 - val_loss: 0.4545 - val_accuracy: 0.8805\nEpoch 96/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0208 - accuracy: 0.9995 - val_loss: 0.4262 - val_accuracy: 0.8713\nEpoch 97/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.4331 - val_accuracy: 0.8750\nEpoch 98/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0195 - accuracy: 0.9991 - val_loss: 0.4454 - val_accuracy: 0.8768\nEpoch 99/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0193 - accuracy: 0.9995 - val_loss: 0.4506 - val_accuracy: 0.8805\nEpoch 100/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0193 - accuracy: 0.9995 - val_loss: 0.4373 - val_accuracy: 0.8805\nEpoch 101/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0203 - accuracy: 0.9995 - val_loss: 0.4440 - val_accuracy: 0.8695\nEpoch 102/120\n68/68 [==============================] - 15s 215ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.4533 - val_accuracy: 0.8676\nEpoch 103/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.4651 - val_accuracy: 0.8860\nEpoch 104/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.4718 - val_accuracy: 0.8695\nEpoch 105/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.4687 - val_accuracy: 0.8695\nEpoch 106/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.4593 - val_accuracy: 0.8713\nEpoch 107/120\n68/68 [==============================] - 14s 213ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.4978 - val_accuracy: 0.8768\nEpoch 108/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.4882 - val_accuracy: 0.8676\nEpoch 109/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.5048 - val_accuracy: 0.8621\nEpoch 110/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0159 - accuracy: 0.9995 - val_loss: 0.4787 - val_accuracy: 0.8732\nEpoch 111/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0164 - accuracy: 0.9995 - val_loss: 0.4853 - val_accuracy: 0.8621\nEpoch 112/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.4899 - val_accuracy: 0.8713\nEpoch 113/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0167 - accuracy: 0.9986 - val_loss: 0.4910 - val_accuracy: 0.8879\nEpoch 114/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0185 - accuracy: 0.9991 - val_loss: 0.5296 - val_accuracy: 0.8456\nEpoch 115/120\n68/68 [==============================] - 17s 248ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4895 - val_accuracy: 0.8768\nEpoch 116/120\n68/68 [==============================] - 17s 249ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.8879\nEpoch 117/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0128 - accuracy: 0.9995 - val_loss: 0.4693 - val_accuracy: 0.8842\nEpoch 118/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.5201 - val_accuracy: 0.8824\nEpoch 119/120\n68/68 [==============================] - 15s 214ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8860\nEpoch 120/120\n68/68 [==============================] - 14s 214ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.4744 - val_accuracy: 0.8805\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(history_xception.history)\ndf.to_csv('/kaggle/working/Xception_history.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:02:42.816652Z","iopub.execute_input":"2023-08-15T19:02:42.816960Z","iopub.status.idle":"2023-08-15T19:02:42.826117Z","shell.execute_reply.started":"2023-08-15T19:02:42.816933Z","shell.execute_reply":"2023-08-15T19:02:42.825182Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### VGG19","metadata":{}},{"cell_type":"code","source":"vgg_wo_top = keras.applications.vgg19.VGG19(include_top=False, weights='imagenet', input_shape=(350, 350, 3))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:02:42.827827Z","iopub.execute_input":"2023-08-15T19:02:42.828235Z","iopub.status.idle":"2023-08-15T19:02:43.800097Z","shell.execute_reply.started":"2023-08-15T19:02:42.828201Z","shell.execute_reply":"2023-08-15T19:02:43.799018Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80134624/80134624 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"vgg_wo_top.trainable = False","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:02:43.801431Z","iopub.execute_input":"2023-08-15T19:02:43.801801Z","iopub.status.idle":"2023-08-15T19:02:43.807514Z","shell.execute_reply.started":"2023-08-15T19:02:43.801767Z","shell.execute_reply":"2023-08-15T19:02:43.806505Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"raw","source":"vgg_model = keras.models.Sequential()\nvgg_model.add(vgg_wo_top)\nvgg_model.add(keras.layers.Flatten())\nvgg_model.add(keras.layers.Dense(128, activation='relu'))\nvgg_model.add(keras.layers.BatchNormalization())\nvgg_model.add(keras.layers.Dense(64, activation='relu'))\nvgg_model.add(keras.layers.BatchNormalization())\nvgg_model.add(keras.layers.Dense(6, activation='softmax'))\n\nvgg_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:53:13.364155Z","iopub.execute_input":"2023-08-15T07:53:13.364975Z","iopub.status.idle":"2023-08-15T07:53:13.540067Z","shell.execute_reply.started":"2023-08-15T07:53:13.364940Z","shell.execute_reply":"2023-08-15T07:53:13.539304Z"}}},{"cell_type":"code","source":"input_vgg = keras.layers.Input(shape=(350, 350, 3))\nvgg_layer = vgg_wo_top(input_vgg)\n\nrandom = np.random.random(vgg_layer.shape[1:])\n\nattention = keras.layers.Attention()([vgg_layer, random])\nflatten = keras.layers.Flatten()(attention)\n\ndense1 = keras.layers.Dense(128, activation='relu')(flatten)\nbn1 = keras.layers.BatchNormalization()(dense1)\ndense2 = keras.layers.Dense(64, activation='relu')(bn1)\noutput = keras.layers.Dense(6, activation='softmax')(dense2)\n\nvgg_model = keras.Model(inputs=input_vgg, outputs=output)\nvgg_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:02:43.808761Z","iopub.execute_input":"2023-08-15T19:02:43.809526Z","iopub.status.idle":"2023-08-15T19:02:43.971492Z","shell.execute_reply.started":"2023-08-15T19:02:43.809493Z","shell.execute_reply":"2023-08-15T19:02:43.970708Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, 350, 350, 3)]     0         \n                                                                 \n vgg19 (Functional)          (None, 10, 10, 512)       20024384  \n                                                                 \n attention_2 (Attention)     (None, 10, 10, 512)       0         \n                                                                 \n flatten_2 (Flatten)         (None, 51200)             0         \n                                                                 \n dense_6 (Dense)             (None, 128)               6553728   \n                                                                 \n batch_normalization_6 (Batc  (None, 128)              512       \n hNormalization)                                                 \n                                                                 \n dense_7 (Dense)             (None, 64)                8256      \n                                                                 \n dense_8 (Dense)             (None, 6)                 390       \n                                                                 \n=================================================================\nTotal params: 26,587,270\nTrainable params: 6,562,630\nNon-trainable params: 20,024,640\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint_vgg = keras.callbacks.ModelCheckpoint('/kaggle/working/VGG_Model.h5', save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:02:43.972533Z","iopub.execute_input":"2023-08-15T19:02:43.972948Z","iopub.status.idle":"2023-08-15T19:02:43.977442Z","shell.execute_reply.started":"2023-08-15T19:02:43.972921Z","shell.execute_reply":"2023-08-15T19:02:43.976696Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"vgg_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:02:43.978400Z","iopub.execute_input":"2023-08-15T19:02:43.978732Z","iopub.status.idle":"2023-08-15T19:02:43.995869Z","shell.execute_reply.started":"2023-08-15T19:02:43.978700Z","shell.execute_reply":"2023-08-15T19:02:43.994946Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"history_vgg = vgg_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test), callbacks=[checkpoint_vgg])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:02:43.997079Z","iopub.execute_input":"2023-08-15T19:02:43.997968Z","iopub.status.idle":"2023-08-15T19:35:38.693847Z","shell.execute_reply.started":"2023-08-15T19:02:43.997935Z","shell.execute_reply":"2023-08-15T19:35:38.692776Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1/120\n68/68 [==============================] - 27s 303ms/step - loss: 1.7226 - accuracy: 0.3615 - val_loss: 1.8962 - val_accuracy: 0.0827\nEpoch 2/120\n68/68 [==============================] - 16s 232ms/step - loss: 0.9023 - accuracy: 0.8031 - val_loss: 1.2722 - val_accuracy: 0.5129\nEpoch 3/120\n68/68 [==============================] - 16s 233ms/step - loss: 0.7134 - accuracy: 0.8850 - val_loss: 0.8512 - val_accuracy: 0.7849\nEpoch 4/120\n68/68 [==============================] - 16s 233ms/step - loss: 0.6220 - accuracy: 0.9154 - val_loss: 0.6958 - val_accuracy: 0.8419\nEpoch 5/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.5630 - accuracy: 0.9250 - val_loss: 0.6203 - val_accuracy: 0.8879\nEpoch 6/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.5139 - accuracy: 0.9420 - val_loss: 0.5165 - val_accuracy: 0.8897\nEpoch 7/120\n68/68 [==============================] - 16s 232ms/step - loss: 0.4783 - accuracy: 0.9476 - val_loss: 0.4554 - val_accuracy: 0.9154\nEpoch 8/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.4491 - accuracy: 0.9591 - val_loss: 0.5065 - val_accuracy: 0.9007\nEpoch 9/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.4271 - accuracy: 0.9540 - val_loss: 0.5693 - val_accuracy: 0.8934\nEpoch 10/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.3997 - accuracy: 0.9641 - val_loss: 0.5962 - val_accuracy: 0.8768\nEpoch 11/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.3824 - accuracy: 0.9701 - val_loss: 0.4536 - val_accuracy: 0.9099\nEpoch 12/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.3648 - accuracy: 0.9683 - val_loss: 0.4424 - val_accuracy: 0.9044\nEpoch 13/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.3444 - accuracy: 0.9729 - val_loss: 0.3992 - val_accuracy: 0.9210\nEpoch 14/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.3255 - accuracy: 0.9761 - val_loss: 0.4298 - val_accuracy: 0.9228\nEpoch 15/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.3178 - accuracy: 0.9802 - val_loss: 0.4147 - val_accuracy: 0.9191\nEpoch 16/120\n68/68 [==============================] - 16s 230ms/step - loss: 0.2974 - accuracy: 0.9816 - val_loss: 0.3948 - val_accuracy: 0.9265\nEpoch 17/120\n68/68 [==============================] - 16s 232ms/step - loss: 0.2839 - accuracy: 0.9756 - val_loss: 0.3609 - val_accuracy: 0.9338\nEpoch 18/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.2745 - accuracy: 0.9816 - val_loss: 0.4048 - val_accuracy: 0.9265\nEpoch 19/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.2649 - accuracy: 0.9816 - val_loss: 0.3744 - val_accuracy: 0.9393\nEpoch 20/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.2475 - accuracy: 0.9890 - val_loss: 0.3705 - val_accuracy: 0.9265\nEpoch 21/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.2399 - accuracy: 0.9848 - val_loss: 0.3369 - val_accuracy: 0.9320\nEpoch 22/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.2281 - accuracy: 0.9880 - val_loss: 0.3426 - val_accuracy: 0.9320\nEpoch 23/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.2217 - accuracy: 0.9867 - val_loss: 0.3268 - val_accuracy: 0.9301\nEpoch 24/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.2129 - accuracy: 0.9862 - val_loss: 0.3581 - val_accuracy: 0.9173\nEpoch 25/120\n68/68 [==============================] - 16s 230ms/step - loss: 0.2061 - accuracy: 0.9917 - val_loss: 0.2908 - val_accuracy: 0.9393\nEpoch 26/120\n68/68 [==============================] - 15s 225ms/step - loss: 0.2025 - accuracy: 0.9890 - val_loss: 0.3315 - val_accuracy: 0.9283\nEpoch 27/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.1917 - accuracy: 0.9913 - val_loss: 0.3079 - val_accuracy: 0.9338\nEpoch 28/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.1830 - accuracy: 0.9903 - val_loss: 0.2891 - val_accuracy: 0.9412\nEpoch 29/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.1757 - accuracy: 0.9922 - val_loss: 0.3140 - val_accuracy: 0.9357\nEpoch 30/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.1682 - accuracy: 0.9917 - val_loss: 0.2865 - val_accuracy: 0.9357\nEpoch 31/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.1662 - accuracy: 0.9894 - val_loss: 0.2545 - val_accuracy: 0.9430\nEpoch 32/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.1597 - accuracy: 0.9922 - val_loss: 0.2757 - val_accuracy: 0.9357\nEpoch 33/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.1548 - accuracy: 0.9922 - val_loss: 0.2563 - val_accuracy: 0.9430\nEpoch 34/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.1465 - accuracy: 0.9940 - val_loss: 0.2729 - val_accuracy: 0.9393\nEpoch 35/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.1412 - accuracy: 0.9968 - val_loss: 0.3174 - val_accuracy: 0.9265\nEpoch 36/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.1382 - accuracy: 0.9959 - val_loss: 0.3274 - val_accuracy: 0.9265\nEpoch 37/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.1353 - accuracy: 0.9959 - val_loss: 0.2436 - val_accuracy: 0.9412\nEpoch 38/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.1302 - accuracy: 0.9936 - val_loss: 0.2400 - val_accuracy: 0.9393\nEpoch 39/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.1223 - accuracy: 0.9959 - val_loss: 0.2703 - val_accuracy: 0.9375\nEpoch 40/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.1229 - accuracy: 0.9936 - val_loss: 0.2938 - val_accuracy: 0.9449\nEpoch 41/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.1158 - accuracy: 0.9945 - val_loss: 0.2516 - val_accuracy: 0.9430\nEpoch 42/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.1182 - accuracy: 0.9949 - val_loss: 0.2335 - val_accuracy: 0.9357\nEpoch 43/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.1112 - accuracy: 0.9968 - val_loss: 0.2309 - val_accuracy: 0.9467\nEpoch 44/120\n68/68 [==============================] - 16s 232ms/step - loss: 0.1068 - accuracy: 0.9963 - val_loss: 0.2274 - val_accuracy: 0.9467\nEpoch 45/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.1046 - accuracy: 0.9954 - val_loss: 0.2777 - val_accuracy: 0.9320\nEpoch 46/120\n68/68 [==============================] - 16s 230ms/step - loss: 0.0981 - accuracy: 0.9968 - val_loss: 0.2202 - val_accuracy: 0.9467\nEpoch 47/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0937 - accuracy: 0.9972 - val_loss: 0.2255 - val_accuracy: 0.9393\nEpoch 48/120\n68/68 [==============================] - 16s 232ms/step - loss: 0.0919 - accuracy: 0.9977 - val_loss: 0.2136 - val_accuracy: 0.9504\nEpoch 49/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.0871 - accuracy: 0.9972 - val_loss: 0.2053 - val_accuracy: 0.9467\nEpoch 50/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0895 - accuracy: 0.9949 - val_loss: 0.2082 - val_accuracy: 0.9467\nEpoch 51/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0826 - accuracy: 0.9982 - val_loss: 0.2110 - val_accuracy: 0.9467\nEpoch 52/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0862 - accuracy: 0.9954 - val_loss: 0.2073 - val_accuracy: 0.9522\nEpoch 53/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0785 - accuracy: 0.9977 - val_loss: 0.2138 - val_accuracy: 0.9449\nEpoch 54/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0774 - accuracy: 0.9972 - val_loss: 0.2148 - val_accuracy: 0.9485\nEpoch 55/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0723 - accuracy: 0.9977 - val_loss: 0.2082 - val_accuracy: 0.9449\nEpoch 56/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.0711 - accuracy: 0.9959 - val_loss: 0.1987 - val_accuracy: 0.9540\nEpoch 57/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.0672 - accuracy: 0.9991 - val_loss: 0.1977 - val_accuracy: 0.9449\nEpoch 58/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.0725 - accuracy: 0.9963 - val_loss: 0.1928 - val_accuracy: 0.9430\nEpoch 59/120\n68/68 [==============================] - 16s 230ms/step - loss: 0.0634 - accuracy: 0.9977 - val_loss: 0.1927 - val_accuracy: 0.9485\nEpoch 60/120\n68/68 [==============================] - 17s 257ms/step - loss: 0.0648 - accuracy: 0.9991 - val_loss: 0.2060 - val_accuracy: 0.9393\nEpoch 61/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0644 - accuracy: 0.9968 - val_loss: 0.1955 - val_accuracy: 0.9375\nEpoch 62/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0619 - accuracy: 0.9982 - val_loss: 0.1993 - val_accuracy: 0.9430\nEpoch 63/120\n68/68 [==============================] - 18s 262ms/step - loss: 0.0625 - accuracy: 0.9982 - val_loss: 0.1876 - val_accuracy: 0.9504\nEpoch 64/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0591 - accuracy: 0.9972 - val_loss: 0.2203 - val_accuracy: 0.9393\nEpoch 65/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0575 - accuracy: 0.9982 - val_loss: 0.1999 - val_accuracy: 0.9412\nEpoch 66/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0563 - accuracy: 0.9977 - val_loss: 0.2088 - val_accuracy: 0.9412\nEpoch 67/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0571 - accuracy: 0.9982 - val_loss: 0.1898 - val_accuracy: 0.9393\nEpoch 68/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0549 - accuracy: 0.9968 - val_loss: 0.2009 - val_accuracy: 0.9357\nEpoch 69/120\n68/68 [==============================] - 16s 232ms/step - loss: 0.0471 - accuracy: 0.9995 - val_loss: 0.1842 - val_accuracy: 0.9449\nEpoch 70/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0552 - accuracy: 0.9963 - val_loss: 0.1808 - val_accuracy: 0.9449\nEpoch 71/120\n68/68 [==============================] - 15s 225ms/step - loss: 0.0511 - accuracy: 0.9982 - val_loss: 0.1905 - val_accuracy: 0.9449\nEpoch 72/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0475 - accuracy: 0.9982 - val_loss: 0.2053 - val_accuracy: 0.9393\nEpoch 73/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0465 - accuracy: 0.9986 - val_loss: 0.1846 - val_accuracy: 0.9449\nEpoch 74/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0471 - accuracy: 0.9972 - val_loss: 0.1884 - val_accuracy: 0.9467\nEpoch 75/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0479 - accuracy: 0.9963 - val_loss: 0.2241 - val_accuracy: 0.9375\nEpoch 76/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0417 - accuracy: 0.9995 - val_loss: 0.1995 - val_accuracy: 0.9449\nEpoch 77/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0397 - accuracy: 0.9991 - val_loss: 0.1825 - val_accuracy: 0.9485\nEpoch 78/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0435 - accuracy: 0.9986 - val_loss: 0.1961 - val_accuracy: 0.9375\nEpoch 79/120\n68/68 [==============================] - 15s 225ms/step - loss: 0.0411 - accuracy: 0.9977 - val_loss: 0.1922 - val_accuracy: 0.9393\nEpoch 80/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0377 - accuracy: 0.9991 - val_loss: 0.1877 - val_accuracy: 0.9430\nEpoch 81/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.0396 - accuracy: 0.9977 - val_loss: 0.1743 - val_accuracy: 0.9485\nEpoch 82/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0362 - accuracy: 0.9986 - val_loss: 0.1744 - val_accuracy: 0.9485\nEpoch 83/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0387 - accuracy: 0.9963 - val_loss: 0.1753 - val_accuracy: 0.9430\nEpoch 84/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0408 - accuracy: 0.9982 - val_loss: 0.1781 - val_accuracy: 0.9430\nEpoch 85/120\n68/68 [==============================] - 17s 257ms/step - loss: 0.0362 - accuracy: 0.9972 - val_loss: 0.1846 - val_accuracy: 0.9393\nEpoch 86/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9449\nEpoch 87/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0346 - accuracy: 0.9982 - val_loss: 0.1860 - val_accuracy: 0.9430\nEpoch 88/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0350 - accuracy: 0.9968 - val_loss: 0.1750 - val_accuracy: 0.9504\nEpoch 89/120\n68/68 [==============================] - 17s 257ms/step - loss: 0.0318 - accuracy: 0.9986 - val_loss: 0.2228 - val_accuracy: 0.9375\nEpoch 90/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0312 - accuracy: 0.9986 - val_loss: 0.1685 - val_accuracy: 0.9504\nEpoch 91/120\n68/68 [==============================] - 18s 264ms/step - loss: 0.0302 - accuracy: 0.9986 - val_loss: 0.1682 - val_accuracy: 0.9504\nEpoch 92/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0319 - accuracy: 0.9982 - val_loss: 0.1741 - val_accuracy: 0.9504\nEpoch 93/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.0288 - accuracy: 0.9991 - val_loss: 0.1656 - val_accuracy: 0.9504\nEpoch 94/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0277 - accuracy: 0.9986 - val_loss: 0.1659 - val_accuracy: 0.9504\nEpoch 95/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.0304 - accuracy: 0.9977 - val_loss: 0.1646 - val_accuracy: 0.9485\nEpoch 96/120\n68/68 [==============================] - 16s 231ms/step - loss: 0.0275 - accuracy: 0.9995 - val_loss: 0.1640 - val_accuracy: 0.9504\nEpoch 97/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0262 - accuracy: 0.9995 - val_loss: 0.1762 - val_accuracy: 0.9449\nEpoch 98/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0252 - accuracy: 0.9986 - val_loss: 0.1853 - val_accuracy: 0.9430\nEpoch 99/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0262 - accuracy: 0.9977 - val_loss: 0.1655 - val_accuracy: 0.9467\nEpoch 100/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0233 - accuracy: 0.9991 - val_loss: 0.1760 - val_accuracy: 0.9467\nEpoch 101/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0279 - accuracy: 0.9968 - val_loss: 0.1662 - val_accuracy: 0.9485\nEpoch 102/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0271 - accuracy: 0.9986 - val_loss: 0.1734 - val_accuracy: 0.9467\nEpoch 103/120\n68/68 [==============================] - 18s 263ms/step - loss: 0.0210 - accuracy: 0.9991 - val_loss: 0.1626 - val_accuracy: 0.9540\nEpoch 104/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0200 - accuracy: 0.9995 - val_loss: 0.1728 - val_accuracy: 0.9522\nEpoch 105/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0263 - accuracy: 0.9972 - val_loss: 0.1762 - val_accuracy: 0.9485\nEpoch 106/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0239 - accuracy: 0.9986 - val_loss: 0.1698 - val_accuracy: 0.9430\nEpoch 107/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.1837 - val_accuracy: 0.9430\nEpoch 108/120\n68/68 [==============================] - 18s 265ms/step - loss: 0.0207 - accuracy: 0.9991 - val_loss: 0.1625 - val_accuracy: 0.9504\nEpoch 109/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0215 - accuracy: 0.9991 - val_loss: 0.1630 - val_accuracy: 0.9522\nEpoch 110/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0225 - accuracy: 0.9972 - val_loss: 0.1750 - val_accuracy: 0.9504\nEpoch 111/120\n68/68 [==============================] - 15s 223ms/step - loss: 0.0206 - accuracy: 0.9995 - val_loss: 0.1714 - val_accuracy: 0.9485\nEpoch 112/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0228 - accuracy: 0.9977 - val_loss: 0.1718 - val_accuracy: 0.9522\nEpoch 113/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0199 - accuracy: 0.9977 - val_loss: 0.1948 - val_accuracy: 0.9338\nEpoch 114/120\n68/68 [==============================] - 15s 224ms/step - loss: 0.0201 - accuracy: 0.9995 - val_loss: 0.1757 - val_accuracy: 0.9449\nEpoch 115/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0166 - accuracy: 0.9995 - val_loss: 0.1647 - val_accuracy: 0.9559\nEpoch 116/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0177 - accuracy: 0.9995 - val_loss: 0.1629 - val_accuracy: 0.9467\nEpoch 117/120\n68/68 [==============================] - 17s 255ms/step - loss: 0.0169 - accuracy: 0.9986 - val_loss: 0.1655 - val_accuracy: 0.9449\nEpoch 118/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0186 - accuracy: 0.9986 - val_loss: 0.1636 - val_accuracy: 0.9430\nEpoch 119/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0226 - accuracy: 0.9982 - val_loss: 0.1779 - val_accuracy: 0.9430\nEpoch 120/120\n68/68 [==============================] - 17s 256ms/step - loss: 0.0233 - accuracy: 0.9982 - val_loss: 0.1826 - val_accuracy: 0.9449\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(history_vgg.history)\ndf.to_csv('/kaggle/working/VGG_history.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:35:38.695876Z","iopub.execute_input":"2023-08-15T19:35:38.696278Z","iopub.status.idle":"2023-08-15T19:35:38.705116Z","shell.execute_reply.started":"2023-08-15T19:35:38.696242Z","shell.execute_reply":"2023-08-15T19:35:38.704111Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### EfficientNet B0","metadata":{}},{"cell_type":"raw","source":"enb0_wo_top = keras.applications.efficientnet.EfficientNetB0(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb0_wo_top.trainable = False\n\nenb0_model = keras.models.Sequential()\nenb0_model.add(enb0_wo_top)\nenb0_model.add(keras.layers.Flatten())\nenb0_model.add(keras.layers.Dense(128, activation='relu'))\nenb0_model.add(keras.layers.Dense(64, activation='relu'))\nenb0_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb0_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb0_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T23:21:13.331078Z","iopub.status.busy":"2023-08-08T23:21:13.330336Z","iopub.status.idle":"2023-08-08T23:31:05.879965Z","shell.execute_reply":"2023-08-08T23:31:05.878846Z","shell.execute_reply.started":"2023-08-08T23:21:13.331044Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"enb0_wo_top = keras.applications.efficientnet.EfficientNetB0(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb0_wo_top.trainable = False\n\ninput_enb0 = keras.layers.Input(shape=(350, 350, 3))\nenb0_layer = enb0_wo_top(input_enb0)\n\nrandom = np.random.random(enb0_layer.shape[1:])\n\nattention = keras.layers.Attention()([enb0_layer, random])\nflatten = keras.layers.Flatten()(attention)\n\ndense1 = keras.layers.Dense(128, activation='relu')(flatten)\nbn1 = keras.layers.BatchNormalization()(dense1)\ndense2 = keras.layers.Dense(64, activation='relu')(bn1)\noutput = keras.layers.Dense(6, activation='softmax')(dense2)\n\nenb0_model = keras.Model(inputs=input_enb0, outputs=output)\nenb0_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:35:38.706770Z","iopub.execute_input":"2023-08-15T19:35:38.707134Z","iopub.status.idle":"2023-08-15T19:35:41.946600Z","shell.execute_reply.started":"2023-08-15T19:35:38.707101Z","shell.execute_reply":"2023-08-15T19:35:41.945612Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n16705208/16705208 [==============================] - 0s 0us/step\nModel: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_7 (InputLayer)        [(None, 350, 350, 3)]     0         \n                                                                 \n efficientnetb0 (Functional)  (None, 11, 11, 1280)     4049571   \n                                                                 \n attention_3 (Attention)     (None, 11, 11, 1280)      0         \n                                                                 \n flatten_3 (Flatten)         (None, 154880)            0         \n                                                                 \n dense_9 (Dense)             (None, 128)               19824768  \n                                                                 \n batch_normalization_7 (Batc  (None, 128)              512       \n hNormalization)                                                 \n                                                                 \n dense_10 (Dense)            (None, 64)                8256      \n                                                                 \n dense_11 (Dense)            (None, 6)                 390       \n                                                                 \n=================================================================\nTotal params: 23,883,497\nTrainable params: 19,833,670\nNon-trainable params: 4,049,827\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint_enb0 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB0_Model.h5', save_best_only=True)\n\nenb0_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:35:41.948342Z","iopub.execute_input":"2023-08-15T19:35:41.949049Z","iopub.status.idle":"2023-08-15T19:35:41.967195Z","shell.execute_reply.started":"2023-08-15T19:35:41.949015Z","shell.execute_reply":"2023-08-15T19:35:41.966120Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"history_enb0 = enb0_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:35:41.968504Z","iopub.execute_input":"2023-08-15T19:35:41.969041Z","iopub.status.idle":"2023-08-15T19:51:36.937158Z","shell.execute_reply.started":"2023-08-15T19:35:41.969008Z","shell.execute_reply":"2023-08-15T19:51:36.936025Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1/120\n","output_type":"stream"},{"name":"stderr","text":"2023-08-15 19:35:50.509911: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_3/efficientnetb0/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"68/68 [==============================] - 19s 159ms/step - loss: 1.1501 - accuracy: 0.6467 - val_loss: 2.4751 - val_accuracy: 0.0551\nEpoch 2/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.6784 - accuracy: 0.8804 - val_loss: 1.8332 - val_accuracy: 0.1599\nEpoch 3/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.6020 - accuracy: 0.8970 - val_loss: 1.5520 - val_accuracy: 0.2794\nEpoch 4/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.5598 - accuracy: 0.8988 - val_loss: 0.8333 - val_accuracy: 0.7776\nEpoch 5/120\n68/68 [==============================] - 9s 130ms/step - loss: 0.5237 - accuracy: 0.9121 - val_loss: 0.5875 - val_accuracy: 0.9026\nEpoch 6/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.4908 - accuracy: 0.9167 - val_loss: 0.6117 - val_accuracy: 0.8879\nEpoch 7/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.4729 - accuracy: 0.9204 - val_loss: 0.7878 - val_accuracy: 0.8217\nEpoch 8/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.4539 - accuracy: 0.9241 - val_loss: 0.7880 - val_accuracy: 0.8107\nEpoch 9/120\n68/68 [==============================] - 8s 112ms/step - loss: 0.4378 - accuracy: 0.9273 - val_loss: 0.5267 - val_accuracy: 0.9136\nEpoch 10/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.4292 - accuracy: 0.9209 - val_loss: 0.4329 - val_accuracy: 0.9265\nEpoch 11/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.4025 - accuracy: 0.9324 - val_loss: 0.4806 - val_accuracy: 0.9210\nEpoch 12/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.3935 - accuracy: 0.9301 - val_loss: 0.4002 - val_accuracy: 0.9210\nEpoch 13/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.3803 - accuracy: 0.9319 - val_loss: 0.3610 - val_accuracy: 0.9320\nEpoch 14/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.3699 - accuracy: 0.9287 - val_loss: 0.3747 - val_accuracy: 0.9228\nEpoch 15/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.3553 - accuracy: 0.9384 - val_loss: 38.8919 - val_accuracy: 0.0257\nEpoch 16/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.3525 - accuracy: 0.9351 - val_loss: 2.0581 - val_accuracy: 0.1562\nEpoch 17/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.3386 - accuracy: 0.9397 - val_loss: 0.5995 - val_accuracy: 0.8805\nEpoch 18/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.3360 - accuracy: 0.9356 - val_loss: 0.4081 - val_accuracy: 0.9228\nEpoch 19/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.3170 - accuracy: 0.9388 - val_loss: 0.3983 - val_accuracy: 0.9283\nEpoch 20/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.3140 - accuracy: 0.9388 - val_loss: 0.3958 - val_accuracy: 0.9283\nEpoch 21/120\n68/68 [==============================] - 8s 112ms/step - loss: 0.2973 - accuracy: 0.9462 - val_loss: 0.3255 - val_accuracy: 0.9320\nEpoch 22/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.2923 - accuracy: 0.9411 - val_loss: 0.3112 - val_accuracy: 0.9228\nEpoch 23/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2864 - accuracy: 0.9457 - val_loss: 0.3273 - val_accuracy: 0.9265\nEpoch 24/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.2750 - accuracy: 0.9545 - val_loss: 0.3071 - val_accuracy: 0.9375\nEpoch 25/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.2779 - accuracy: 0.9489 - val_loss: 0.2859 - val_accuracy: 0.9430\nEpoch 26/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.2661 - accuracy: 0.9494 - val_loss: 0.2598 - val_accuracy: 0.9375\nEpoch 27/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2649 - accuracy: 0.9499 - val_loss: 0.2837 - val_accuracy: 0.9485\nEpoch 28/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2497 - accuracy: 0.9476 - val_loss: 0.2960 - val_accuracy: 0.9467\nEpoch 29/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.2480 - accuracy: 0.9554 - val_loss: 0.2731 - val_accuracy: 0.9430\nEpoch 30/120\n68/68 [==============================] - 7s 109ms/step - loss: 0.2438 - accuracy: 0.9563 - val_loss: 0.2486 - val_accuracy: 0.9467\nEpoch 31/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2256 - accuracy: 0.9632 - val_loss: 0.2758 - val_accuracy: 0.9449\nEpoch 32/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.2410 - accuracy: 0.9554 - val_loss: 0.2294 - val_accuracy: 0.9430\nEpoch 33/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2384 - accuracy: 0.9517 - val_loss: 0.3045 - val_accuracy: 0.9228\nEpoch 34/120\n68/68 [==============================] - 7s 109ms/step - loss: 0.2325 - accuracy: 0.9508 - val_loss: 0.2112 - val_accuracy: 0.9485\nEpoch 35/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.2192 - accuracy: 0.9632 - val_loss: 0.2003 - val_accuracy: 0.9467\nEpoch 36/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2221 - accuracy: 0.9595 - val_loss: 1.9803 - val_accuracy: 0.8750\nEpoch 37/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.2188 - accuracy: 0.9591 - val_loss: 0.3267 - val_accuracy: 0.9210\nEpoch 38/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2065 - accuracy: 0.9614 - val_loss: 0.2579 - val_accuracy: 0.9485\nEpoch 39/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1963 - accuracy: 0.9641 - val_loss: 0.2910 - val_accuracy: 0.9449\nEpoch 40/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.2024 - accuracy: 0.9604 - val_loss: 0.3353 - val_accuracy: 0.9320\nEpoch 41/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.1900 - accuracy: 0.9600 - val_loss: 0.2168 - val_accuracy: 0.9485\nEpoch 42/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.1935 - accuracy: 0.9581 - val_loss: 0.2178 - val_accuracy: 0.9485\nEpoch 43/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1946 - accuracy: 0.9623 - val_loss: 0.2949 - val_accuracy: 0.9210\nEpoch 44/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.1791 - accuracy: 0.9655 - val_loss: 0.2102 - val_accuracy: 0.9449\nEpoch 45/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1788 - accuracy: 0.9669 - val_loss: 0.2176 - val_accuracy: 0.9504\nEpoch 46/120\n68/68 [==============================] - 8s 112ms/step - loss: 0.1786 - accuracy: 0.9623 - val_loss: 0.2047 - val_accuracy: 0.9504\nEpoch 47/120\n68/68 [==============================] - 7s 111ms/step - loss: 0.1787 - accuracy: 0.9641 - val_loss: 0.2248 - val_accuracy: 0.9449\nEpoch 48/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.1759 - accuracy: 0.9641 - val_loss: 0.1980 - val_accuracy: 0.9540\nEpoch 49/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.1768 - accuracy: 0.9641 - val_loss: 0.1851 - val_accuracy: 0.9393\nEpoch 50/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.1684 - accuracy: 0.9650 - val_loss: 0.1895 - val_accuracy: 0.9449\nEpoch 51/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1613 - accuracy: 0.9701 - val_loss: 0.2249 - val_accuracy: 0.9430\nEpoch 52/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1666 - accuracy: 0.9660 - val_loss: 0.1781 - val_accuracy: 0.9485\nEpoch 53/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1581 - accuracy: 0.9678 - val_loss: 0.1825 - val_accuracy: 0.9467\nEpoch 54/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.1550 - accuracy: 0.9678 - val_loss: 0.1773 - val_accuracy: 0.9467\nEpoch 55/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1562 - accuracy: 0.9683 - val_loss: 0.1810 - val_accuracy: 0.9504\nEpoch 56/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.1417 - accuracy: 0.9770 - val_loss: 0.1855 - val_accuracy: 0.9577\nEpoch 57/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1379 - accuracy: 0.9761 - val_loss: 0.1661 - val_accuracy: 0.9596\nEpoch 58/120\n68/68 [==============================] - 8s 112ms/step - loss: 0.1428 - accuracy: 0.9710 - val_loss: 0.1626 - val_accuracy: 0.9559\nEpoch 59/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1530 - accuracy: 0.9614 - val_loss: 0.2159 - val_accuracy: 0.9320\nEpoch 60/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1413 - accuracy: 0.9683 - val_loss: 0.1998 - val_accuracy: 0.9449\nEpoch 61/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1301 - accuracy: 0.9793 - val_loss: 0.1640 - val_accuracy: 0.9485\nEpoch 62/120\n68/68 [==============================] - 8s 112ms/step - loss: 0.1458 - accuracy: 0.9687 - val_loss: 0.1613 - val_accuracy: 0.9504\nEpoch 63/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1328 - accuracy: 0.9784 - val_loss: 0.1745 - val_accuracy: 0.9559\nEpoch 64/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1496 - accuracy: 0.9623 - val_loss: 0.1767 - val_accuracy: 0.9559\nEpoch 65/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.1348 - accuracy: 0.9756 - val_loss: 0.1548 - val_accuracy: 0.9504\nEpoch 66/120\n68/68 [==============================] - 8s 112ms/step - loss: 0.1321 - accuracy: 0.9710 - val_loss: 0.1587 - val_accuracy: 0.9596\nEpoch 67/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1307 - accuracy: 0.9706 - val_loss: 0.1785 - val_accuracy: 0.9412\nEpoch 68/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1277 - accuracy: 0.9701 - val_loss: 0.1453 - val_accuracy: 0.9504\nEpoch 69/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.1163 - accuracy: 0.9788 - val_loss: 0.2271 - val_accuracy: 0.9228\nEpoch 70/120\n68/68 [==============================] - 9s 130ms/step - loss: 0.1271 - accuracy: 0.9724 - val_loss: 0.1411 - val_accuracy: 0.9559\nEpoch 71/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1195 - accuracy: 0.9747 - val_loss: 0.1868 - val_accuracy: 0.9504\nEpoch 72/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1218 - accuracy: 0.9724 - val_loss: 0.1723 - val_accuracy: 0.9522\nEpoch 73/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1149 - accuracy: 0.9807 - val_loss: 0.2366 - val_accuracy: 0.9136\nEpoch 74/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.1135 - accuracy: 0.9798 - val_loss: 0.1627 - val_accuracy: 0.9485\nEpoch 75/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1126 - accuracy: 0.9756 - val_loss: 0.1822 - val_accuracy: 0.9430\nEpoch 76/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1318 - accuracy: 0.9701 - val_loss: 0.1460 - val_accuracy: 0.9614\nEpoch 77/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.1203 - accuracy: 0.9696 - val_loss: 0.1617 - val_accuracy: 0.9540\nEpoch 78/120\n68/68 [==============================] - 8s 112ms/step - loss: 0.1094 - accuracy: 0.9752 - val_loss: 0.1353 - val_accuracy: 0.9632\nEpoch 79/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1041 - accuracy: 0.9793 - val_loss: 0.1385 - val_accuracy: 0.9688\nEpoch 80/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1167 - accuracy: 0.9724 - val_loss: 0.1335 - val_accuracy: 0.9614\nEpoch 81/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1168 - accuracy: 0.9724 - val_loss: 0.1317 - val_accuracy: 0.9559\nEpoch 82/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0989 - accuracy: 0.9811 - val_loss: 0.1318 - val_accuracy: 0.9632\nEpoch 83/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.1086 - accuracy: 0.9738 - val_loss: 0.1377 - val_accuracy: 0.9577\nEpoch 84/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.1065 - accuracy: 0.9742 - val_loss: 0.1338 - val_accuracy: 0.9540\nEpoch 85/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1134 - accuracy: 0.9752 - val_loss: 0.1427 - val_accuracy: 0.9540\nEpoch 86/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.1001 - accuracy: 0.9811 - val_loss: 0.1389 - val_accuracy: 0.9522\nEpoch 87/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0966 - accuracy: 0.9834 - val_loss: 0.1464 - val_accuracy: 0.9522\nEpoch 88/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1048 - accuracy: 0.9747 - val_loss: 0.1453 - val_accuracy: 0.9467\nEpoch 89/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0985 - accuracy: 0.9752 - val_loss: 0.1962 - val_accuracy: 0.9393\nEpoch 90/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0959 - accuracy: 0.9816 - val_loss: 0.1307 - val_accuracy: 0.9577\nEpoch 91/120\n68/68 [==============================] - 9s 130ms/step - loss: 0.1068 - accuracy: 0.9738 - val_loss: 0.1292 - val_accuracy: 0.9596\nEpoch 92/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0972 - accuracy: 0.9788 - val_loss: 0.1328 - val_accuracy: 0.9449\nEpoch 93/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0991 - accuracy: 0.9761 - val_loss: 0.1465 - val_accuracy: 0.9540\nEpoch 94/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.1105 - accuracy: 0.9724 - val_loss: 0.1226 - val_accuracy: 0.9577\nEpoch 95/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.0977 - accuracy: 0.9775 - val_loss: 0.1205 - val_accuracy: 0.9596\nEpoch 96/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.0916 - accuracy: 0.9807 - val_loss: 0.1244 - val_accuracy: 0.9669\nEpoch 97/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0835 - accuracy: 0.9848 - val_loss: 0.2690 - val_accuracy: 0.9210\nEpoch 98/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.0894 - accuracy: 0.9779 - val_loss: 0.1561 - val_accuracy: 0.9540\nEpoch 99/120\n68/68 [==============================] - 9s 130ms/step - loss: 0.0892 - accuracy: 0.9802 - val_loss: 0.1271 - val_accuracy: 0.9651\nEpoch 100/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.0903 - accuracy: 0.9793 - val_loss: 0.1448 - val_accuracy: 0.9485\nEpoch 101/120\n68/68 [==============================] - 7s 111ms/step - loss: 0.0883 - accuracy: 0.9802 - val_loss: 0.1446 - val_accuracy: 0.9485\nEpoch 102/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.0923 - accuracy: 0.9784 - val_loss: 0.1243 - val_accuracy: 0.9614\nEpoch 103/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0927 - accuracy: 0.9825 - val_loss: 0.1302 - val_accuracy: 0.9596\nEpoch 104/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0880 - accuracy: 0.9811 - val_loss: 0.1196 - val_accuracy: 0.9669\nEpoch 105/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0928 - accuracy: 0.9761 - val_loss: 0.1405 - val_accuracy: 0.9614\nEpoch 106/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0821 - accuracy: 0.9844 - val_loss: 0.1195 - val_accuracy: 0.9596\nEpoch 107/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.0854 - accuracy: 0.9825 - val_loss: 0.1214 - val_accuracy: 0.9669\nEpoch 108/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.0834 - accuracy: 0.9816 - val_loss: 0.1266 - val_accuracy: 0.9540\nEpoch 109/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0970 - accuracy: 0.9793 - val_loss: 0.1248 - val_accuracy: 0.9596\nEpoch 110/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.0914 - accuracy: 0.9807 - val_loss: 0.1174 - val_accuracy: 0.9651\nEpoch 111/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.0802 - accuracy: 0.9844 - val_loss: 0.1391 - val_accuracy: 0.9559\nEpoch 112/120\n68/68 [==============================] - 8s 111ms/step - loss: 0.0828 - accuracy: 0.9798 - val_loss: 0.1143 - val_accuracy: 0.9688\nEpoch 113/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0953 - accuracy: 0.9733 - val_loss: 0.2455 - val_accuracy: 0.9136\nEpoch 114/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.0847 - accuracy: 0.9802 - val_loss: 0.1346 - val_accuracy: 0.9559\nEpoch 115/120\n68/68 [==============================] - 9s 128ms/step - loss: 0.0912 - accuracy: 0.9788 - val_loss: 0.1193 - val_accuracy: 0.9596\nEpoch 116/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0865 - accuracy: 0.9779 - val_loss: 0.1858 - val_accuracy: 0.9375\nEpoch 117/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0819 - accuracy: 0.9802 - val_loss: 0.1305 - val_accuracy: 0.9504\nEpoch 118/120\n68/68 [==============================] - 9s 129ms/step - loss: 0.0902 - accuracy: 0.9807 - val_loss: 0.1344 - val_accuracy: 0.9449\nEpoch 119/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0834 - accuracy: 0.9788 - val_loss: 0.1219 - val_accuracy: 0.9559\nEpoch 120/120\n68/68 [==============================] - 7s 110ms/step - loss: 0.0788 - accuracy: 0.9825 - val_loss: 0.1301 - val_accuracy: 0.9485\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame(history_enb0.history)\ndf.to_csv('/kaggle/working/ENB0_history.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:51:36.938939Z","iopub.execute_input":"2023-08-15T19:51:36.939363Z","iopub.status.idle":"2023-08-15T19:51:36.949224Z","shell.execute_reply.started":"2023-08-15T19:51:36.939326Z","shell.execute_reply":"2023-08-15T19:51:36.948214Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### EfficientNet B1","metadata":{}},{"cell_type":"raw","source":"enb1_wo_top = keras.applications.efficientnet.EfficientNetB1(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb1_wo_top.trainable = False\n\nenb1_model = keras.models.Sequential()\nenb1_model.add(enb1_wo_top)\nenb1_model.add(keras.layers.Flatten())\nenb1_model.add(keras.layers.Dense(128, activation='relu'))\nenb1_model.add(keras.layers.Dense(64, activation='relu'))\nenb1_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb1_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb1_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T23:33:52.744967Z","iopub.status.busy":"2023-08-08T23:33:52.743959Z","iopub.status.idle":"2023-08-08T23:45:29.747600Z","shell.execute_reply":"2023-08-08T23:45:29.746515Z","shell.execute_reply.started":"2023-08-08T23:33:52.744928Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"enb1_wo_top = keras.applications.efficientnet.EfficientNetB1(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb1_wo_top.trainable = False\n\ninput_enb1 = keras.layers.Input(shape=(350, 350, 3))\nenb1_layer = enb1_wo_top(input_enb1)\n\nrandom = np.random.random(enb1_layer.shape[1:])\n\nattention = keras.layers.Attention()([enb1_layer, random])\nflatten = keras.layers.Flatten()(attention)\n\ndense1 = keras.layers.Dense(128, activation='relu')(flatten)\nbn1 = keras.layers.BatchNormalization()(dense1)\ndense2 = keras.layers.Dense(64, activation='relu')(bn1)\noutput = keras.layers.Dense(6, activation='softmax')(dense2)\n\nenb1_model = keras.Model(inputs=input_enb1, outputs=output)\nenb1_model.summary()\n\ncheckpoint_enb1 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB1_Model.h5', save_best_only=True)\n\nenb1_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory_enb1 = enb1_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))\n\ndf = pd.DataFrame(history_enb1.history)\ndf.to_csv('/kaggle/working/ENB1_history.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T19:51:36.950976Z","iopub.execute_input":"2023-08-15T19:51:36.951366Z","iopub.status.idle":"2023-08-15T20:13:01.412115Z","shell.execute_reply.started":"2023-08-15T19:51:36.951333Z","shell.execute_reply":"2023-08-15T20:13:01.410952Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n27018416/27018416 [==============================] - 0s 0us/step\nModel: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_9 (InputLayer)        [(None, 350, 350, 3)]     0         \n                                                                 \n efficientnetb1 (Functional)  (None, 11, 11, 1280)     6575239   \n                                                                 \n attention_4 (Attention)     (None, 11, 11, 1280)      0         \n                                                                 \n flatten_4 (Flatten)         (None, 154880)            0         \n                                                                 \n dense_12 (Dense)            (None, 128)               19824768  \n                                                                 \n batch_normalization_8 (Batc  (None, 128)              512       \n hNormalization)                                                 \n                                                                 \n dense_13 (Dense)            (None, 64)                8256      \n                                                                 \n dense_14 (Dense)            (None, 6)                 390       \n                                                                 \n=================================================================\nTotal params: 26,409,165\nTrainable params: 19,833,670\nNon-trainable params: 6,575,495\n_________________________________________________________________\nEpoch 1/120\n","output_type":"stream"},{"name":"stderr","text":"2023-08-15 19:51:52.928538: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_4/efficientnetb1/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"68/68 [==============================] - 25s 200ms/step - loss: 0.6239 - accuracy: 0.8293 - val_loss: 0.6275 - val_accuracy: 0.8750\nEpoch 2/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.4130 - accuracy: 0.8914 - val_loss: 0.4555 - val_accuracy: 0.8750\nEpoch 3/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.3656 - accuracy: 0.9034 - val_loss: 0.3486 - val_accuracy: 0.8787\nEpoch 4/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.3522 - accuracy: 0.9034 - val_loss: 0.3020 - val_accuracy: 0.8952\nEpoch 5/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.3406 - accuracy: 0.9094 - val_loss: 0.2852 - val_accuracy: 0.8879\nEpoch 6/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.3163 - accuracy: 0.9181 - val_loss: 0.6130 - val_accuracy: 0.8199\nEpoch 7/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.3045 - accuracy: 0.9218 - val_loss: 0.2787 - val_accuracy: 0.9081\nEpoch 8/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.2929 - accuracy: 0.9218 - val_loss: 0.3072 - val_accuracy: 0.9246\nEpoch 9/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.2879 - accuracy: 0.9241 - val_loss: 0.2198 - val_accuracy: 0.9265\nEpoch 10/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.2800 - accuracy: 0.9227 - val_loss: 0.2196 - val_accuracy: 0.9393\nEpoch 11/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.2635 - accuracy: 0.9328 - val_loss: 0.2350 - val_accuracy: 0.9301\nEpoch 12/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.2697 - accuracy: 0.9292 - val_loss: 0.2196 - val_accuracy: 0.9301\nEpoch 13/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.2591 - accuracy: 0.9292 - val_loss: 0.2123 - val_accuracy: 0.9246\nEpoch 14/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.2560 - accuracy: 0.9278 - val_loss: 0.2225 - val_accuracy: 0.9081\nEpoch 15/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.2408 - accuracy: 0.9397 - val_loss: 0.1790 - val_accuracy: 0.9449\nEpoch 16/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.2385 - accuracy: 0.9397 - val_loss: 0.1908 - val_accuracy: 0.9320\nEpoch 17/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.2274 - accuracy: 0.9393 - val_loss: 0.2153 - val_accuracy: 0.9228\nEpoch 18/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.2299 - accuracy: 0.9370 - val_loss: 0.2163 - val_accuracy: 0.9173\nEpoch 19/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.2219 - accuracy: 0.9425 - val_loss: 0.2071 - val_accuracy: 0.9265\nEpoch 20/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.2258 - accuracy: 0.9356 - val_loss: 0.1960 - val_accuracy: 0.9375\nEpoch 21/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.2155 - accuracy: 0.9425 - val_loss: 0.1811 - val_accuracy: 0.9485\nEpoch 22/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.2155 - accuracy: 0.9457 - val_loss: 0.1853 - val_accuracy: 0.9338\nEpoch 23/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.2136 - accuracy: 0.9439 - val_loss: 0.1982 - val_accuracy: 0.9154\nEpoch 24/120\n68/68 [==============================] - 11s 163ms/step - loss: 0.2134 - accuracy: 0.9425 - val_loss: 0.1719 - val_accuracy: 0.9301\nEpoch 25/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.2162 - accuracy: 0.9402 - val_loss: 0.1631 - val_accuracy: 0.9393\nEpoch 26/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1985 - accuracy: 0.9503 - val_loss: 0.1704 - val_accuracy: 0.9301\nEpoch 27/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.2051 - accuracy: 0.9453 - val_loss: 0.1633 - val_accuracy: 0.9559\nEpoch 28/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1981 - accuracy: 0.9443 - val_loss: 0.1561 - val_accuracy: 0.9393\nEpoch 29/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1996 - accuracy: 0.9480 - val_loss: 0.1726 - val_accuracy: 0.9265\nEpoch 30/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1879 - accuracy: 0.9545 - val_loss: 0.1973 - val_accuracy: 0.9191\nEpoch 31/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1866 - accuracy: 0.9503 - val_loss: 0.1515 - val_accuracy: 0.9393\nEpoch 32/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1845 - accuracy: 0.9480 - val_loss: 0.1466 - val_accuracy: 0.9412\nEpoch 33/120\n68/68 [==============================] - 11s 163ms/step - loss: 0.1894 - accuracy: 0.9512 - val_loss: 0.1490 - val_accuracy: 0.9485\nEpoch 34/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1917 - accuracy: 0.9485 - val_loss: 0.1608 - val_accuracy: 0.9393\nEpoch 35/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1868 - accuracy: 0.9512 - val_loss: 0.1411 - val_accuracy: 0.9559\nEpoch 36/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1791 - accuracy: 0.9494 - val_loss: 0.1420 - val_accuracy: 0.9522\nEpoch 37/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1864 - accuracy: 0.9471 - val_loss: 0.1388 - val_accuracy: 0.9504\nEpoch 38/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1809 - accuracy: 0.9531 - val_loss: 0.1396 - val_accuracy: 0.9522\nEpoch 39/120\n68/68 [==============================] - 10s 153ms/step - loss: 0.1729 - accuracy: 0.9526 - val_loss: 0.1405 - val_accuracy: 0.9522\nEpoch 40/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1581 - accuracy: 0.9637 - val_loss: 0.1682 - val_accuracy: 0.9265\nEpoch 41/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1716 - accuracy: 0.9540 - val_loss: 0.1904 - val_accuracy: 0.9246\nEpoch 42/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1678 - accuracy: 0.9558 - val_loss: 0.1414 - val_accuracy: 0.9522\nEpoch 43/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1710 - accuracy: 0.9577 - val_loss: 0.1496 - val_accuracy: 0.9338\nEpoch 44/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1720 - accuracy: 0.9545 - val_loss: 0.1287 - val_accuracy: 0.9706\nEpoch 45/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1737 - accuracy: 0.9554 - val_loss: 0.1447 - val_accuracy: 0.9412\nEpoch 46/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1657 - accuracy: 0.9499 - val_loss: 0.1507 - val_accuracy: 0.9504\nEpoch 47/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1644 - accuracy: 0.9549 - val_loss: 0.1510 - val_accuracy: 0.9393\nEpoch 48/120\n68/68 [==============================] - 11s 163ms/step - loss: 0.1570 - accuracy: 0.9558 - val_loss: 0.1405 - val_accuracy: 0.9632\nEpoch 49/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1562 - accuracy: 0.9595 - val_loss: 0.1704 - val_accuracy: 0.9301\nEpoch 50/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1604 - accuracy: 0.9558 - val_loss: 0.1442 - val_accuracy: 0.9393\nEpoch 51/120\n68/68 [==============================] - 11s 163ms/step - loss: 0.1673 - accuracy: 0.9535 - val_loss: 0.7216 - val_accuracy: 0.8750\nEpoch 52/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1575 - accuracy: 0.9568 - val_loss: 0.1639 - val_accuracy: 0.9412\nEpoch 53/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1585 - accuracy: 0.9614 - val_loss: 0.2193 - val_accuracy: 0.9136\nEpoch 54/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1515 - accuracy: 0.9581 - val_loss: 0.1472 - val_accuracy: 0.9504\nEpoch 55/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1569 - accuracy: 0.9600 - val_loss: 0.1297 - val_accuracy: 0.9467\nEpoch 56/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1592 - accuracy: 0.9591 - val_loss: 0.1687 - val_accuracy: 0.9265\nEpoch 57/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1501 - accuracy: 0.9581 - val_loss: 0.1343 - val_accuracy: 0.9577\nEpoch 58/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1561 - accuracy: 0.9558 - val_loss: 0.1533 - val_accuracy: 0.9430\nEpoch 59/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1633 - accuracy: 0.9522 - val_loss: 0.1950 - val_accuracy: 0.9210\nEpoch 60/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1600 - accuracy: 0.9549 - val_loss: 0.1493 - val_accuracy: 0.9449\nEpoch 61/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1560 - accuracy: 0.9591 - val_loss: 0.1305 - val_accuracy: 0.9430\nEpoch 62/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1563 - accuracy: 0.9526 - val_loss: 0.1347 - val_accuracy: 0.9504\nEpoch 63/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1424 - accuracy: 0.9627 - val_loss: 0.1452 - val_accuracy: 0.9522\nEpoch 64/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1415 - accuracy: 0.9641 - val_loss: 0.1457 - val_accuracy: 0.9393\nEpoch 65/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1407 - accuracy: 0.9678 - val_loss: 0.1262 - val_accuracy: 0.9596\nEpoch 66/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1477 - accuracy: 0.9581 - val_loss: 0.1375 - val_accuracy: 0.9449\nEpoch 67/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1415 - accuracy: 0.9595 - val_loss: 0.1286 - val_accuracy: 0.9632\nEpoch 68/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1455 - accuracy: 0.9623 - val_loss: 0.1540 - val_accuracy: 0.9357\nEpoch 69/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1492 - accuracy: 0.9581 - val_loss: 0.1276 - val_accuracy: 0.9651\nEpoch 70/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1465 - accuracy: 0.9595 - val_loss: 0.1458 - val_accuracy: 0.9504\nEpoch 71/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1393 - accuracy: 0.9660 - val_loss: 0.1248 - val_accuracy: 0.9504\nEpoch 72/120\n68/68 [==============================] - 11s 163ms/step - loss: 0.1511 - accuracy: 0.9554 - val_loss: 0.1248 - val_accuracy: 0.9559\nEpoch 73/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1419 - accuracy: 0.9623 - val_loss: 0.1624 - val_accuracy: 0.9357\nEpoch 74/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1323 - accuracy: 0.9678 - val_loss: 0.1814 - val_accuracy: 0.9283\nEpoch 75/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1271 - accuracy: 0.9641 - val_loss: 0.1210 - val_accuracy: 0.9669\nEpoch 76/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1366 - accuracy: 0.9609 - val_loss: 0.1269 - val_accuracy: 0.9540\nEpoch 77/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1391 - accuracy: 0.9595 - val_loss: 0.1192 - val_accuracy: 0.9614\nEpoch 78/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1342 - accuracy: 0.9641 - val_loss: 0.1197 - val_accuracy: 0.9614\nEpoch 79/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1387 - accuracy: 0.9609 - val_loss: 0.1222 - val_accuracy: 0.9540\nEpoch 80/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1261 - accuracy: 0.9623 - val_loss: 0.1150 - val_accuracy: 0.9614\nEpoch 81/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1307 - accuracy: 0.9641 - val_loss: 0.1309 - val_accuracy: 0.9485\nEpoch 82/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1244 - accuracy: 0.9650 - val_loss: 0.1028 - val_accuracy: 0.9651\nEpoch 83/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1356 - accuracy: 0.9614 - val_loss: 0.0968 - val_accuracy: 0.9706\nEpoch 84/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1398 - accuracy: 0.9595 - val_loss: 0.1025 - val_accuracy: 0.9724\nEpoch 85/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1284 - accuracy: 0.9600 - val_loss: 0.1262 - val_accuracy: 0.9559\nEpoch 86/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1331 - accuracy: 0.9669 - val_loss: 0.1005 - val_accuracy: 0.9706\nEpoch 87/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1252 - accuracy: 0.9669 - val_loss: 0.1152 - val_accuracy: 0.9559\nEpoch 88/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1277 - accuracy: 0.9664 - val_loss: 0.1130 - val_accuracy: 0.9559\nEpoch 89/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1361 - accuracy: 0.9614 - val_loss: 0.1278 - val_accuracy: 0.9540\nEpoch 90/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1238 - accuracy: 0.9632 - val_loss: 0.1057 - val_accuracy: 0.9669\nEpoch 91/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1062 - accuracy: 0.9761 - val_loss: 0.1041 - val_accuracy: 0.9669\nEpoch 92/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1305 - accuracy: 0.9637 - val_loss: 0.1162 - val_accuracy: 0.9522\nEpoch 93/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1255 - accuracy: 0.9646 - val_loss: 0.1230 - val_accuracy: 0.9559\nEpoch 94/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1209 - accuracy: 0.9701 - val_loss: 0.1031 - val_accuracy: 0.9632\nEpoch 95/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1221 - accuracy: 0.9632 - val_loss: 0.1026 - val_accuracy: 0.9688\nEpoch 96/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1203 - accuracy: 0.9660 - val_loss: 0.1251 - val_accuracy: 0.9559\nEpoch 97/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1179 - accuracy: 0.9673 - val_loss: 0.1101 - val_accuracy: 0.9632\nEpoch 98/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1090 - accuracy: 0.9710 - val_loss: 0.1190 - val_accuracy: 0.9467\nEpoch 99/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1186 - accuracy: 0.9692 - val_loss: 0.0954 - val_accuracy: 0.9632\nEpoch 100/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1140 - accuracy: 0.9696 - val_loss: 0.1094 - val_accuracy: 0.9614\nEpoch 101/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1174 - accuracy: 0.9664 - val_loss: 0.1190 - val_accuracy: 0.9430\nEpoch 102/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1142 - accuracy: 0.9687 - val_loss: 0.1226 - val_accuracy: 0.9430\nEpoch 103/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1302 - accuracy: 0.9600 - val_loss: 0.1058 - val_accuracy: 0.9632\nEpoch 104/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1098 - accuracy: 0.9692 - val_loss: 0.1089 - val_accuracy: 0.9651\nEpoch 105/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1145 - accuracy: 0.9724 - val_loss: 0.1024 - val_accuracy: 0.9669\nEpoch 106/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1198 - accuracy: 0.9719 - val_loss: 0.0962 - val_accuracy: 0.9724\nEpoch 107/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1141 - accuracy: 0.9669 - val_loss: 0.1247 - val_accuracy: 0.9540\nEpoch 108/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1113 - accuracy: 0.9729 - val_loss: 0.1068 - val_accuracy: 0.9632\nEpoch 109/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1245 - accuracy: 0.9632 - val_loss: 0.1033 - val_accuracy: 0.9761\nEpoch 110/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1081 - accuracy: 0.9719 - val_loss: 0.1185 - val_accuracy: 0.9632\nEpoch 111/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1096 - accuracy: 0.9724 - val_loss: 0.1061 - val_accuracy: 0.9540\nEpoch 112/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1166 - accuracy: 0.9646 - val_loss: 0.1020 - val_accuracy: 0.9596\nEpoch 113/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1029 - accuracy: 0.9752 - val_loss: 0.0913 - val_accuracy: 0.9651\nEpoch 114/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1070 - accuracy: 0.9724 - val_loss: 0.0948 - val_accuracy: 0.9688\nEpoch 115/120\n68/68 [==============================] - 10s 152ms/step - loss: 0.1102 - accuracy: 0.9664 - val_loss: 0.1169 - val_accuracy: 0.9559\nEpoch 116/120\n68/68 [==============================] - 11s 162ms/step - loss: 0.1079 - accuracy: 0.9701 - val_loss: 0.1158 - val_accuracy: 0.9559\nEpoch 117/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1130 - accuracy: 0.9692 - val_loss: 0.1220 - val_accuracy: 0.9577\nEpoch 118/120\n68/68 [==============================] - 10s 151ms/step - loss: 0.1022 - accuracy: 0.9733 - val_loss: 0.1055 - val_accuracy: 0.9596\nEpoch 119/120\n68/68 [==============================] - 10s 150ms/step - loss: 0.1030 - accuracy: 0.9729 - val_loss: 0.1141 - val_accuracy: 0.9577\nEpoch 120/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1067 - accuracy: 0.9696 - val_loss: 0.0862 - val_accuracy: 0.9724\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### EfficientNet B2","metadata":{}},{"cell_type":"raw","source":"enb2_wo_top = keras.applications.efficientnet.EfficientNetB2(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb2_wo_top.trainable = False\n\nenb2_model = keras.models.Sequential()\nenb2_model.add(enb0_wo_top)\nenb2_model.add(keras.layers.Flatten())\nenb2_model.add(keras.layers.Dense(128, activation='relu'))\nenb2_model.add(keras.layers.Dense(64, activation='relu'))\nenb2_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb2_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb2_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T23:45:29.750308Z","iopub.status.busy":"2023-08-08T23:45:29.749927Z","iopub.status.idle":"2023-08-08T23:55:09.765371Z","shell.execute_reply":"2023-08-08T23:55:09.764195Z","shell.execute_reply.started":"2023-08-08T23:45:29.750272Z"}}},{"cell_type":"code","source":"enb2_wo_top = keras.applications.efficientnet.EfficientNetB2(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb2_wo_top.trainable = False\n\ninput_enb2 = keras.layers.Input(shape=(350, 350, 3))\nenb2_layer = enb2_wo_top(input_enb2)\n\nrandom = np.random.random(enb2_layer.shape[1:])\n\nattention = keras.layers.Attention()([enb2_layer, random])\nflatten = keras.layers.Flatten()(attention)\n\ndense1 = keras.layers.Dense(128, activation='relu')(flatten)\nbn1 = keras.layers.BatchNormalization()(dense1)\ndense2 = keras.layers.Dense(64, activation='relu')(bn1)\noutput = keras.layers.Dense(6, activation='softmax')(dense2)\n\nenb2_model = keras.Model(inputs=input_enb2, outputs=output)\nenb2_model.summary()\n\ncheckpoint_enb2 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB2_Model.h5', save_best_only=True)\n\nenb2_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory_enb2 = enb2_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))\n\ndf = pd.DataFrame(history_enb2.history)\ndf.to_csv('/kaggle/working/ENB2_history.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T20:13:01.419606Z","iopub.execute_input":"2023-08-15T20:13:01.419914Z","iopub.status.idle":"2023-08-15T20:35:35.883171Z","shell.execute_reply.started":"2023-08-15T20:13:01.419886Z","shell.execute_reply":"2023-08-15T20:35:35.882111Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n31790344/31790344 [==============================] - 0s 0us/step\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_11 (InputLayer)       [(None, 350, 350, 3)]     0         \n                                                                 \n efficientnetb2 (Functional)  (None, 11, 11, 1408)     7768569   \n                                                                 \n attention_5 (Attention)     (None, 11, 11, 1408)      0         \n                                                                 \n flatten_5 (Flatten)         (None, 170368)            0         \n                                                                 \n dense_15 (Dense)            (None, 128)               21807232  \n                                                                 \n batch_normalization_9 (Batc  (None, 128)              512       \n hNormalization)                                                 \n                                                                 \n dense_16 (Dense)            (None, 64)                8256      \n                                                                 \n dense_17 (Dense)            (None, 6)                 390       \n                                                                 \n=================================================================\nTotal params: 29,584,959\nTrainable params: 21,816,134\nNon-trainable params: 7,768,825\n_________________________________________________________________\nEpoch 1/120\n","output_type":"stream"},{"name":"stderr","text":"2023-08-15 20:13:17.293574: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_5/efficientnetb2/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"68/68 [==============================] - 25s 203ms/step - loss: 0.7368 - accuracy: 0.8224 - val_loss: 0.7506 - val_accuracy: 0.8860\nEpoch 2/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.4680 - accuracy: 0.8947 - val_loss: 0.7095 - val_accuracy: 0.8162\nEpoch 3/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.4239 - accuracy: 0.8947 - val_loss: 0.4271 - val_accuracy: 0.8897\nEpoch 4/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.3753 - accuracy: 0.9098 - val_loss: 0.3615 - val_accuracy: 0.8824\nEpoch 5/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.3686 - accuracy: 0.9117 - val_loss: 0.3653 - val_accuracy: 0.9081\nEpoch 6/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.3472 - accuracy: 0.9144 - val_loss: 0.2921 - val_accuracy: 0.9246\nEpoch 7/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.3188 - accuracy: 0.9269 - val_loss: 0.2842 - val_accuracy: 0.9173\nEpoch 8/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.3151 - accuracy: 0.9255 - val_loss: 0.2658 - val_accuracy: 0.9301\nEpoch 9/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.3011 - accuracy: 0.9292 - val_loss: 0.3266 - val_accuracy: 0.9393\nEpoch 10/120\n68/68 [==============================] - 12s 170ms/step - loss: 0.2931 - accuracy: 0.9282 - val_loss: 0.2737 - val_accuracy: 0.9375\nEpoch 11/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2892 - accuracy: 0.9310 - val_loss: 0.2885 - val_accuracy: 0.9320\nEpoch 12/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2925 - accuracy: 0.9269 - val_loss: 0.6569 - val_accuracy: 0.8456\nEpoch 13/120\n68/68 [==============================] - 11s 170ms/step - loss: 0.2622 - accuracy: 0.9434 - val_loss: 0.4634 - val_accuracy: 0.8952\nEpoch 14/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.2613 - accuracy: 0.9411 - val_loss: 0.2643 - val_accuracy: 0.9412\nEpoch 15/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.2647 - accuracy: 0.9338 - val_loss: 1.2596 - val_accuracy: 0.5312\nEpoch 16/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.2581 - accuracy: 0.9356 - val_loss: 0.5077 - val_accuracy: 0.8989\nEpoch 17/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2443 - accuracy: 0.9425 - val_loss: 0.3466 - val_accuracy: 0.9320\nEpoch 18/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.2549 - accuracy: 0.9333 - val_loss: 0.2863 - val_accuracy: 0.9393\nEpoch 19/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2392 - accuracy: 0.9374 - val_loss: 0.2224 - val_accuracy: 0.9485\nEpoch 20/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2301 - accuracy: 0.9402 - val_loss: 0.2951 - val_accuracy: 0.9485\nEpoch 21/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.2360 - accuracy: 0.9453 - val_loss: 0.3344 - val_accuracy: 0.9283\nEpoch 22/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2221 - accuracy: 0.9471 - val_loss: 0.2840 - val_accuracy: 0.9338\nEpoch 23/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.2111 - accuracy: 0.9531 - val_loss: 0.2074 - val_accuracy: 0.9504\nEpoch 24/120\n68/68 [==============================] - 11s 170ms/step - loss: 0.2136 - accuracy: 0.9494 - val_loss: 0.2066 - val_accuracy: 0.9485\nEpoch 25/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2138 - accuracy: 0.9489 - val_loss: 0.1988 - val_accuracy: 0.9485\nEpoch 26/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.2020 - accuracy: 0.9499 - val_loss: 0.2480 - val_accuracy: 0.9338\nEpoch 27/120\n68/68 [==============================] - 11s 170ms/step - loss: 0.2131 - accuracy: 0.9489 - val_loss: 0.2123 - val_accuracy: 0.9540\nEpoch 28/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.1982 - accuracy: 0.9563 - val_loss: 0.1832 - val_accuracy: 0.9485\nEpoch 29/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1955 - accuracy: 0.9540 - val_loss: 0.2054 - val_accuracy: 0.9688\nEpoch 30/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1942 - accuracy: 0.9499 - val_loss: 0.1858 - val_accuracy: 0.9577\nEpoch 31/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1969 - accuracy: 0.9522 - val_loss: 0.1813 - val_accuracy: 0.9559\nEpoch 32/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1905 - accuracy: 0.9558 - val_loss: 0.2007 - val_accuracy: 0.9522\nEpoch 33/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1764 - accuracy: 0.9609 - val_loss: 0.1731 - val_accuracy: 0.9559\nEpoch 34/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1857 - accuracy: 0.9540 - val_loss: 0.1794 - val_accuracy: 0.9632\nEpoch 35/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1737 - accuracy: 0.9572 - val_loss: 0.1929 - val_accuracy: 0.9559\nEpoch 36/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1836 - accuracy: 0.9485 - val_loss: 0.1786 - val_accuracy: 0.9540\nEpoch 37/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.1837 - accuracy: 0.9558 - val_loss: 0.1505 - val_accuracy: 0.9540\nEpoch 38/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1673 - accuracy: 0.9673 - val_loss: 0.2086 - val_accuracy: 0.9485\nEpoch 39/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1726 - accuracy: 0.9595 - val_loss: 0.1672 - val_accuracy: 0.9485\nEpoch 40/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1608 - accuracy: 0.9692 - val_loss: 0.1747 - val_accuracy: 0.9522\nEpoch 41/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1689 - accuracy: 0.9586 - val_loss: 0.1737 - val_accuracy: 0.9577\nEpoch 42/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1552 - accuracy: 0.9660 - val_loss: 0.1866 - val_accuracy: 0.9577\nEpoch 43/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1592 - accuracy: 0.9660 - val_loss: 0.1681 - val_accuracy: 0.9559\nEpoch 44/120\n68/68 [==============================] - 11s 170ms/step - loss: 0.1646 - accuracy: 0.9581 - val_loss: 0.1610 - val_accuracy: 0.9540\nEpoch 45/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1607 - accuracy: 0.9646 - val_loss: 0.1603 - val_accuracy: 0.9577\nEpoch 46/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1570 - accuracy: 0.9618 - val_loss: 0.1523 - val_accuracy: 0.9485\nEpoch 47/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1686 - accuracy: 0.9600 - val_loss: 0.1897 - val_accuracy: 0.9504\nEpoch 48/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1627 - accuracy: 0.9637 - val_loss: 0.1712 - val_accuracy: 0.9577\nEpoch 49/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1503 - accuracy: 0.9618 - val_loss: 0.2182 - val_accuracy: 0.9449\nEpoch 50/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1515 - accuracy: 0.9627 - val_loss: 0.1585 - val_accuracy: 0.9596\nEpoch 51/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1484 - accuracy: 0.9627 - val_loss: 0.1709 - val_accuracy: 0.9614\nEpoch 52/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1583 - accuracy: 0.9609 - val_loss: 0.1605 - val_accuracy: 0.9614\nEpoch 53/120\n68/68 [==============================] - 12s 170ms/step - loss: 0.1523 - accuracy: 0.9650 - val_loss: 0.1513 - val_accuracy: 0.9596\nEpoch 54/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1469 - accuracy: 0.9683 - val_loss: 0.1417 - val_accuracy: 0.9577\nEpoch 55/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1429 - accuracy: 0.9683 - val_loss: 0.1442 - val_accuracy: 0.9632\nEpoch 56/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1485 - accuracy: 0.9600 - val_loss: 0.1466 - val_accuracy: 0.9688\nEpoch 57/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1374 - accuracy: 0.9673 - val_loss: 0.1370 - val_accuracy: 0.9596\nEpoch 58/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1314 - accuracy: 0.9683 - val_loss: 0.1354 - val_accuracy: 0.9688\nEpoch 59/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1298 - accuracy: 0.9724 - val_loss: 0.1284 - val_accuracy: 0.9614\nEpoch 60/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1415 - accuracy: 0.9655 - val_loss: 0.1404 - val_accuracy: 0.9632\nEpoch 61/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1429 - accuracy: 0.9664 - val_loss: 0.1569 - val_accuracy: 0.9559\nEpoch 62/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1404 - accuracy: 0.9655 - val_loss: 0.1692 - val_accuracy: 0.9614\nEpoch 63/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1339 - accuracy: 0.9719 - val_loss: 0.1225 - val_accuracy: 0.9614\nEpoch 64/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1321 - accuracy: 0.9664 - val_loss: 0.1702 - val_accuracy: 0.9540\nEpoch 65/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1382 - accuracy: 0.9655 - val_loss: 0.1382 - val_accuracy: 0.9651\nEpoch 66/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1319 - accuracy: 0.9673 - val_loss: 0.1494 - val_accuracy: 0.9504\nEpoch 67/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1355 - accuracy: 0.9650 - val_loss: 0.1435 - val_accuracy: 0.9669\nEpoch 68/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1271 - accuracy: 0.9752 - val_loss: 0.1423 - val_accuracy: 0.9577\nEpoch 69/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1272 - accuracy: 0.9696 - val_loss: 0.1705 - val_accuracy: 0.9449\nEpoch 70/120\n68/68 [==============================] - 11s 170ms/step - loss: 0.1384 - accuracy: 0.9650 - val_loss: 0.1282 - val_accuracy: 0.9614\nEpoch 71/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1411 - accuracy: 0.9623 - val_loss: 0.1398 - val_accuracy: 0.9577\nEpoch 72/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.1339 - accuracy: 0.9637 - val_loss: 0.1332 - val_accuracy: 0.9614\nEpoch 73/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1282 - accuracy: 0.9683 - val_loss: 0.8415 - val_accuracy: 0.7463\nEpoch 74/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1192 - accuracy: 0.9729 - val_loss: 0.1643 - val_accuracy: 0.9577\nEpoch 75/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.1272 - accuracy: 0.9678 - val_loss: 0.1451 - val_accuracy: 0.9577\nEpoch 76/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1259 - accuracy: 0.9650 - val_loss: 0.2123 - val_accuracy: 0.9540\nEpoch 77/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1213 - accuracy: 0.9683 - val_loss: 0.1669 - val_accuracy: 0.9559\nEpoch 78/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1231 - accuracy: 0.9710 - val_loss: 0.1739 - val_accuracy: 0.9540\nEpoch 79/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1158 - accuracy: 0.9715 - val_loss: 0.1431 - val_accuracy: 0.9632\nEpoch 80/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1148 - accuracy: 0.9710 - val_loss: 0.1219 - val_accuracy: 0.9669\nEpoch 81/120\n68/68 [==============================] - 11s 170ms/step - loss: 0.1081 - accuracy: 0.9719 - val_loss: 0.1228 - val_accuracy: 0.9669\nEpoch 82/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1204 - accuracy: 0.9696 - val_loss: 0.1220 - val_accuracy: 0.9669\nEpoch 83/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1085 - accuracy: 0.9733 - val_loss: 0.1531 - val_accuracy: 0.9485\nEpoch 84/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1105 - accuracy: 0.9715 - val_loss: 0.1512 - val_accuracy: 0.9577\nEpoch 85/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1259 - accuracy: 0.9669 - val_loss: 0.1142 - val_accuracy: 0.9688\nEpoch 86/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1095 - accuracy: 0.9752 - val_loss: 0.1148 - val_accuracy: 0.9706\nEpoch 87/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1047 - accuracy: 0.9770 - val_loss: 0.1147 - val_accuracy: 0.9577\nEpoch 88/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1142 - accuracy: 0.9683 - val_loss: 0.1211 - val_accuracy: 0.9669\nEpoch 89/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1270 - accuracy: 0.9664 - val_loss: 0.1470 - val_accuracy: 0.9651\nEpoch 90/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.1148 - accuracy: 0.9673 - val_loss: 0.1368 - val_accuracy: 0.9596\nEpoch 91/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1120 - accuracy: 0.9729 - val_loss: 0.1227 - val_accuracy: 0.9651\nEpoch 92/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1132 - accuracy: 0.9696 - val_loss: 0.1201 - val_accuracy: 0.9632\nEpoch 93/120\n68/68 [==============================] - 12s 170ms/step - loss: 0.1105 - accuracy: 0.9729 - val_loss: 0.1228 - val_accuracy: 0.9724\nEpoch 94/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.1039 - accuracy: 0.9775 - val_loss: 0.1094 - val_accuracy: 0.9743\nEpoch 95/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1082 - accuracy: 0.9752 - val_loss: 0.1463 - val_accuracy: 0.9632\nEpoch 96/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.1123 - accuracy: 0.9729 - val_loss: 0.1278 - val_accuracy: 0.9706\nEpoch 97/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1158 - accuracy: 0.9701 - val_loss: 0.1125 - val_accuracy: 0.9706\nEpoch 98/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.1100 - accuracy: 0.9724 - val_loss: 0.1144 - val_accuracy: 0.9743\nEpoch 99/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1054 - accuracy: 0.9738 - val_loss: 0.1227 - val_accuracy: 0.9688\nEpoch 100/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1039 - accuracy: 0.9738 - val_loss: 0.1220 - val_accuracy: 0.9724\nEpoch 101/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1023 - accuracy: 0.9733 - val_loss: 0.1228 - val_accuracy: 0.9651\nEpoch 102/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1099 - accuracy: 0.9715 - val_loss: 0.1062 - val_accuracy: 0.9761\nEpoch 103/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.1064 - accuracy: 0.9719 - val_loss: 0.1196 - val_accuracy: 0.9761\nEpoch 104/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.0988 - accuracy: 0.9756 - val_loss: 0.1121 - val_accuracy: 0.9706\nEpoch 105/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.1015 - accuracy: 0.9742 - val_loss: 0.1337 - val_accuracy: 0.9706\nEpoch 106/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.0959 - accuracy: 0.9747 - val_loss: 0.1176 - val_accuracy: 0.9706\nEpoch 107/120\n68/68 [==============================] - 11s 161ms/step - loss: 0.0996 - accuracy: 0.9738 - val_loss: 0.1114 - val_accuracy: 0.9706\nEpoch 108/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1057 - accuracy: 0.9738 - val_loss: 0.1266 - val_accuracy: 0.9724\nEpoch 109/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.1040 - accuracy: 0.9798 - val_loss: 0.1124 - val_accuracy: 0.9669\nEpoch 110/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.0965 - accuracy: 0.9765 - val_loss: 0.1218 - val_accuracy: 0.9651\nEpoch 111/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.0992 - accuracy: 0.9747 - val_loss: 0.1080 - val_accuracy: 0.9724\nEpoch 112/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.0979 - accuracy: 0.9738 - val_loss: 0.1034 - val_accuracy: 0.9761\nEpoch 113/120\n68/68 [==============================] - 11s 160ms/step - loss: 0.0998 - accuracy: 0.9719 - val_loss: 0.1120 - val_accuracy: 0.9614\nEpoch 114/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.0939 - accuracy: 0.9775 - val_loss: 0.1538 - val_accuracy: 0.9485\nEpoch 115/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.0932 - accuracy: 0.9779 - val_loss: 0.1087 - val_accuracy: 0.9688\nEpoch 116/120\n68/68 [==============================] - 12s 170ms/step - loss: 0.0878 - accuracy: 0.9802 - val_loss: 0.1057 - val_accuracy: 0.9706\nEpoch 117/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.0894 - accuracy: 0.9798 - val_loss: 0.1011 - val_accuracy: 0.9706\nEpoch 118/120\n68/68 [==============================] - 11s 169ms/step - loss: 0.0880 - accuracy: 0.9825 - val_loss: 0.1003 - val_accuracy: 0.9724\nEpoch 119/120\n68/68 [==============================] - 11s 168ms/step - loss: 0.0955 - accuracy: 0.9738 - val_loss: 0.1281 - val_accuracy: 0.9669\nEpoch 120/120\n68/68 [==============================] - 11s 159ms/step - loss: 0.0975 - accuracy: 0.9742 - val_loss: 0.1006 - val_accuracy: 0.9632\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### EfficientNet B3","metadata":{}},{"cell_type":"raw","source":"enb3_wo_top = keras.applications.efficientnet.EfficientNetB3(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb3_wo_top.trainable = False\n\nenb3_model = keras.models.Sequential()\nenb3_model.add(enb3_wo_top)\nenb3_model.add(keras.layers.Flatten())\nenb3_model.add(keras.layers.Dense(128, activation='relu'))\nenb3_model.add(keras.layers.Dense(64, activation='relu'))\nenb3_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb3_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb3_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T23:55:09.768882Z","iopub.status.busy":"2023-08-08T23:55:09.768130Z","iopub.status.idle":"2023-08-09T00:09:21.243960Z","shell.execute_reply":"2023-08-09T00:09:21.242914Z","shell.execute_reply.started":"2023-08-08T23:55:09.768806Z"}}},{"cell_type":"code","source":"enb3_wo_top = keras.applications.efficientnet.EfficientNetB3(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb3_wo_top.trainable = False\n\ninput_enb3 = keras.layers.Input(shape=(350, 350, 3))\nenb3_layer = enb3_wo_top(input_enb3)\n\nrandom = np.random.random(enb3_layer.shape[1:])\n\nattention = keras.layers.Attention()([enb3_layer, random])\nflatten = keras.layers.Flatten()(attention)\n\ndense1 = keras.layers.Dense(128, activation='relu')(flatten)\nbn1 = keras.layers.BatchNormalization()(dense1)\ndense2 = keras.layers.Dense(64, activation='relu')(bn1)\noutput = keras.layers.Dense(6, activation='softmax')(dense2)\n\nenb3_model = keras.Model(inputs=input_enb3, outputs=output)\nenb3_model.summary()\n\ncheckpoint_enb3 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB3_Model.h5', save_best_only=True)\n\nenb3_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory_enb3 = enb3_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))\n\ndf = pd.DataFrame(history_enb3.history)\ndf.to_csv('/kaggle/working/ENB3_history.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-15T20:35:35.884976Z","iopub.execute_input":"2023-08-15T20:35:35.885372Z","iopub.status.idle":"2023-08-15T21:03:45.719610Z","shell.execute_reply.started":"2023-08-15T20:35:35.885337Z","shell.execute_reply":"2023-08-15T21:03:45.718573Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n43941136/43941136 [==============================] - 0s 0us/step\nModel: \"model_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_13 (InputLayer)       [(None, 350, 350, 3)]     0         \n                                                                 \n efficientnetb3 (Functional)  (None, 11, 11, 1536)     10783535  \n                                                                 \n attention_6 (Attention)     (None, 11, 11, 1536)      0         \n                                                                 \n flatten_6 (Flatten)         (None, 185856)            0         \n                                                                 \n dense_18 (Dense)            (None, 128)               23789696  \n                                                                 \n batch_normalization_10 (Bat  (None, 128)              512       \n chNormalization)                                                \n                                                                 \n dense_19 (Dense)            (None, 64)                8256      \n                                                                 \n dense_20 (Dense)            (None, 6)                 390       \n                                                                 \n=================================================================\nTotal params: 34,582,389\nTrainable params: 23,798,598\nNon-trainable params: 10,783,791\n_________________________________________________________________\nEpoch 1/120\n","output_type":"stream"},{"name":"stderr","text":"2023-08-15 20:35:53.201611: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_6/efficientnetb3/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"68/68 [==============================] - 32s 285ms/step - loss: 0.8667 - accuracy: 0.7769 - val_loss: 2.1853 - val_accuracy: 0.0478\nEpoch 2/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.5504 - accuracy: 0.8905 - val_loss: 0.7921 - val_accuracy: 0.7739\nEpoch 3/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.5152 - accuracy: 0.8887 - val_loss: 0.4501 - val_accuracy: 0.9044\nEpoch 4/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.4782 - accuracy: 0.9052 - val_loss: 0.4245 - val_accuracy: 0.9154\nEpoch 5/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.4274 - accuracy: 0.9135 - val_loss: 0.4721 - val_accuracy: 0.9136\nEpoch 6/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.4159 - accuracy: 0.9144 - val_loss: 0.5409 - val_accuracy: 0.8860\nEpoch 7/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.3984 - accuracy: 0.9186 - val_loss: 0.4973 - val_accuracy: 0.9062\nEpoch 8/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.3953 - accuracy: 0.9195 - val_loss: 0.4884 - val_accuracy: 0.8915\nEpoch 9/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.3765 - accuracy: 0.9250 - val_loss: 0.4665 - val_accuracy: 0.9062\nEpoch 10/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.3653 - accuracy: 0.9200 - val_loss: 0.5607 - val_accuracy: 0.8824\nEpoch 11/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.3483 - accuracy: 0.9292 - val_loss: 0.4003 - val_accuracy: 0.9154\nEpoch 12/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.3443 - accuracy: 0.9319 - val_loss: 0.3278 - val_accuracy: 0.9136\nEpoch 13/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.3391 - accuracy: 0.9255 - val_loss: 0.3866 - val_accuracy: 0.9081\nEpoch 14/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.3182 - accuracy: 0.9324 - val_loss: 0.3315 - val_accuracy: 0.9228\nEpoch 15/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.3147 - accuracy: 0.9259 - val_loss: 0.4824 - val_accuracy: 0.8787\nEpoch 16/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.3004 - accuracy: 0.9356 - val_loss: 0.3266 - val_accuracy: 0.9191\nEpoch 17/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.3004 - accuracy: 0.9347 - val_loss: 0.3837 - val_accuracy: 0.9154\nEpoch 18/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2965 - accuracy: 0.9402 - val_loss: 0.3517 - val_accuracy: 0.9099\nEpoch 19/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2852 - accuracy: 0.9370 - val_loss: 0.3824 - val_accuracy: 0.9099\nEpoch 20/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2799 - accuracy: 0.9430 - val_loss: 0.4301 - val_accuracy: 0.9173\nEpoch 21/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2798 - accuracy: 0.9411 - val_loss: 0.3985 - val_accuracy: 0.9301\nEpoch 22/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2761 - accuracy: 0.9416 - val_loss: 0.3055 - val_accuracy: 0.9228\nEpoch 23/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.2731 - accuracy: 0.9416 - val_loss: 0.2531 - val_accuracy: 0.9338\nEpoch 24/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2667 - accuracy: 0.9434 - val_loss: 0.2683 - val_accuracy: 0.9357\nEpoch 25/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.2582 - accuracy: 0.9434 - val_loss: 0.2941 - val_accuracy: 0.9301\nEpoch 26/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.2565 - accuracy: 0.9370 - val_loss: 0.2828 - val_accuracy: 0.9375\nEpoch 27/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2530 - accuracy: 0.9439 - val_loss: 0.2498 - val_accuracy: 0.9504\nEpoch 28/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2367 - accuracy: 0.9485 - val_loss: 0.3124 - val_accuracy: 0.9246\nEpoch 29/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2500 - accuracy: 0.9393 - val_loss: 0.2411 - val_accuracy: 0.9191\nEpoch 30/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2490 - accuracy: 0.9425 - val_loss: 0.2303 - val_accuracy: 0.9154\nEpoch 31/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2366 - accuracy: 0.9448 - val_loss: 0.2529 - val_accuracy: 0.9412\nEpoch 32/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2328 - accuracy: 0.9489 - val_loss: 0.2271 - val_accuracy: 0.9320\nEpoch 33/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2326 - accuracy: 0.9466 - val_loss: 0.2032 - val_accuracy: 0.9228\nEpoch 34/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2283 - accuracy: 0.9485 - val_loss: 0.2288 - val_accuracy: 0.9412\nEpoch 35/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.2173 - accuracy: 0.9508 - val_loss: 0.2077 - val_accuracy: 0.9449\nEpoch 36/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.2195 - accuracy: 0.9480 - val_loss: 0.2077 - val_accuracy: 0.9485\nEpoch 37/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2132 - accuracy: 0.9489 - val_loss: 0.2411 - val_accuracy: 0.9596\nEpoch 38/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.2034 - accuracy: 0.9572 - val_loss: 0.2044 - val_accuracy: 0.9540\nEpoch 39/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.2146 - accuracy: 0.9443 - val_loss: 0.2446 - val_accuracy: 0.9522\nEpoch 40/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.2157 - accuracy: 0.9457 - val_loss: 0.2337 - val_accuracy: 0.9467\nEpoch 41/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.2072 - accuracy: 0.9535 - val_loss: 0.2362 - val_accuracy: 0.9393\nEpoch 42/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.2028 - accuracy: 0.9522 - val_loss: 0.2059 - val_accuracy: 0.9577\nEpoch 43/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1990 - accuracy: 0.9572 - val_loss: 0.1789 - val_accuracy: 0.9485\nEpoch 44/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1840 - accuracy: 0.9609 - val_loss: 0.1765 - val_accuracy: 0.9504\nEpoch 45/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.2090 - accuracy: 0.9462 - val_loss: 0.2890 - val_accuracy: 0.9412\nEpoch 46/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.2009 - accuracy: 0.9476 - val_loss: 0.1953 - val_accuracy: 0.9412\nEpoch 47/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1994 - accuracy: 0.9531 - val_loss: 0.1872 - val_accuracy: 0.9467\nEpoch 48/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1796 - accuracy: 0.9591 - val_loss: 0.2898 - val_accuracy: 0.9430\nEpoch 49/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1818 - accuracy: 0.9535 - val_loss: 0.1911 - val_accuracy: 0.9485\nEpoch 50/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1880 - accuracy: 0.9554 - val_loss: 0.2265 - val_accuracy: 0.9412\nEpoch 51/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1696 - accuracy: 0.9558 - val_loss: 0.2259 - val_accuracy: 0.9467\nEpoch 52/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1788 - accuracy: 0.9577 - val_loss: 0.1944 - val_accuracy: 0.9393\nEpoch 53/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1689 - accuracy: 0.9646 - val_loss: 0.2042 - val_accuracy: 0.9357\nEpoch 54/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1679 - accuracy: 0.9549 - val_loss: 0.1689 - val_accuracy: 0.9504\nEpoch 55/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1653 - accuracy: 0.9577 - val_loss: 0.2177 - val_accuracy: 0.9485\nEpoch 56/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1838 - accuracy: 0.9517 - val_loss: 0.1830 - val_accuracy: 0.9430\nEpoch 57/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1766 - accuracy: 0.9568 - val_loss: 0.1788 - val_accuracy: 0.9485\nEpoch 58/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1608 - accuracy: 0.9600 - val_loss: 0.1922 - val_accuracy: 0.9485\nEpoch 59/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1649 - accuracy: 0.9568 - val_loss: 0.1675 - val_accuracy: 0.9522\nEpoch 60/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1685 - accuracy: 0.9591 - val_loss: 0.2691 - val_accuracy: 0.9228\nEpoch 61/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1616 - accuracy: 0.9600 - val_loss: 0.1936 - val_accuracy: 0.9467\nEpoch 62/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1659 - accuracy: 0.9600 - val_loss: 0.2051 - val_accuracy: 0.9449\nEpoch 63/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1614 - accuracy: 0.9623 - val_loss: 0.1992 - val_accuracy: 0.9449\nEpoch 64/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1634 - accuracy: 0.9581 - val_loss: 0.1680 - val_accuracy: 0.9449\nEpoch 65/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1568 - accuracy: 0.9604 - val_loss: 0.1935 - val_accuracy: 0.9449\nEpoch 66/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1491 - accuracy: 0.9641 - val_loss: 0.2345 - val_accuracy: 0.9449\nEpoch 67/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1561 - accuracy: 0.9623 - val_loss: 0.4092 - val_accuracy: 0.8915\nEpoch 68/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1587 - accuracy: 0.9618 - val_loss: 0.1851 - val_accuracy: 0.9522\nEpoch 69/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1478 - accuracy: 0.9650 - val_loss: 0.1690 - val_accuracy: 0.9540\nEpoch 70/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1489 - accuracy: 0.9632 - val_loss: 0.1860 - val_accuracy: 0.9430\nEpoch 71/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1512 - accuracy: 0.9618 - val_loss: 0.1935 - val_accuracy: 0.9522\nEpoch 72/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1527 - accuracy: 0.9623 - val_loss: 0.1821 - val_accuracy: 0.9485\nEpoch 73/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1604 - accuracy: 0.9627 - val_loss: 0.1631 - val_accuracy: 0.9540\nEpoch 74/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1485 - accuracy: 0.9627 - val_loss: 0.2347 - val_accuracy: 0.9338\nEpoch 75/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1542 - accuracy: 0.9549 - val_loss: 0.1658 - val_accuracy: 0.9412\nEpoch 76/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1561 - accuracy: 0.9595 - val_loss: 0.1594 - val_accuracy: 0.9540\nEpoch 77/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1428 - accuracy: 0.9618 - val_loss: 0.1749 - val_accuracy: 0.9504\nEpoch 78/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1480 - accuracy: 0.9604 - val_loss: 0.1692 - val_accuracy: 0.9577\nEpoch 79/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1483 - accuracy: 0.9650 - val_loss: 0.1519 - val_accuracy: 0.9449\nEpoch 80/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1497 - accuracy: 0.9614 - val_loss: 0.1417 - val_accuracy: 0.9540\nEpoch 81/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1376 - accuracy: 0.9664 - val_loss: 0.1640 - val_accuracy: 0.9651\nEpoch 82/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1396 - accuracy: 0.9683 - val_loss: 0.2070 - val_accuracy: 0.9449\nEpoch 83/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1389 - accuracy: 0.9678 - val_loss: 0.1841 - val_accuracy: 0.9467\nEpoch 84/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1402 - accuracy: 0.9572 - val_loss: 0.1837 - val_accuracy: 0.9522\nEpoch 85/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1438 - accuracy: 0.9673 - val_loss: 0.1823 - val_accuracy: 0.9540\nEpoch 86/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1518 - accuracy: 0.9614 - val_loss: 0.1531 - val_accuracy: 0.9577\nEpoch 87/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1316 - accuracy: 0.9692 - val_loss: 0.1419 - val_accuracy: 0.9559\nEpoch 88/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1436 - accuracy: 0.9581 - val_loss: 0.1964 - val_accuracy: 0.9449\nEpoch 89/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1352 - accuracy: 0.9623 - val_loss: 0.1437 - val_accuracy: 0.9596\nEpoch 90/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1484 - accuracy: 0.9595 - val_loss: 0.1404 - val_accuracy: 0.9522\nEpoch 91/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1324 - accuracy: 0.9641 - val_loss: 0.1678 - val_accuracy: 0.9577\nEpoch 92/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1372 - accuracy: 0.9650 - val_loss: 0.1523 - val_accuracy: 0.9540\nEpoch 93/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1339 - accuracy: 0.9618 - val_loss: 0.1617 - val_accuracy: 0.9540\nEpoch 94/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1309 - accuracy: 0.9655 - val_loss: 0.1774 - val_accuracy: 0.9522\nEpoch 95/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1277 - accuracy: 0.9683 - val_loss: 0.1852 - val_accuracy: 0.9412\nEpoch 96/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1227 - accuracy: 0.9706 - val_loss: 0.1567 - val_accuracy: 0.9596\nEpoch 97/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1346 - accuracy: 0.9660 - val_loss: 0.1525 - val_accuracy: 0.9540\nEpoch 98/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1252 - accuracy: 0.9696 - val_loss: 0.1443 - val_accuracy: 0.9596\nEpoch 99/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1273 - accuracy: 0.9646 - val_loss: 0.1362 - val_accuracy: 0.9596\nEpoch 100/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1203 - accuracy: 0.9678 - val_loss: 0.1284 - val_accuracy: 0.9669\nEpoch 101/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1378 - accuracy: 0.9623 - val_loss: 0.1246 - val_accuracy: 0.9559\nEpoch 102/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1171 - accuracy: 0.9673 - val_loss: 0.1627 - val_accuracy: 0.9522\nEpoch 103/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1134 - accuracy: 0.9724 - val_loss: 0.1367 - val_accuracy: 0.9559\nEpoch 104/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1170 - accuracy: 0.9678 - val_loss: 0.1551 - val_accuracy: 0.9540\nEpoch 105/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1187 - accuracy: 0.9683 - val_loss: 0.1459 - val_accuracy: 0.9540\nEpoch 106/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1250 - accuracy: 0.9623 - val_loss: 0.1624 - val_accuracy: 0.9522\nEpoch 107/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1272 - accuracy: 0.9660 - val_loss: 0.1498 - val_accuracy: 0.9393\nEpoch 108/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1295 - accuracy: 0.9650 - val_loss: 0.1397 - val_accuracy: 0.9504\nEpoch 109/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1262 - accuracy: 0.9646 - val_loss: 0.1318 - val_accuracy: 0.9522\nEpoch 110/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1310 - accuracy: 0.9627 - val_loss: 0.1545 - val_accuracy: 0.9504\nEpoch 111/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1176 - accuracy: 0.9660 - val_loss: 0.1698 - val_accuracy: 0.9540\nEpoch 112/120\n68/68 [==============================] - 14s 204ms/step - loss: 0.1317 - accuracy: 0.9655 - val_loss: 0.1528 - val_accuracy: 0.9467\nEpoch 113/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1133 - accuracy: 0.9724 - val_loss: 0.2576 - val_accuracy: 0.9246\nEpoch 114/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1218 - accuracy: 0.9724 - val_loss: 0.1688 - val_accuracy: 0.9467\nEpoch 115/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1132 - accuracy: 0.9687 - val_loss: 0.1309 - val_accuracy: 0.9540\nEpoch 116/120\n68/68 [==============================] - 14s 206ms/step - loss: 0.1124 - accuracy: 0.9733 - val_loss: 0.1756 - val_accuracy: 0.9540\nEpoch 117/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1132 - accuracy: 0.9641 - val_loss: 0.1880 - val_accuracy: 0.9393\nEpoch 118/120\n68/68 [==============================] - 14s 205ms/step - loss: 0.1149 - accuracy: 0.9706 - val_loss: 0.1356 - val_accuracy: 0.9614\nEpoch 119/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1209 - accuracy: 0.9637 - val_loss: 0.1292 - val_accuracy: 0.9614\nEpoch 120/120\n68/68 [==============================] - 14s 203ms/step - loss: 0.1176 - accuracy: 0.9664 - val_loss: 0.1348 - val_accuracy: 0.9614\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### EfficientNet B4","metadata":{}},{"cell_type":"code","source":"enb4_wo_top = keras.applications.efficientnet.EfficientNetB4(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb4_wo_top.trainable = False\n\nenb4_model = keras.models.Sequential()\nenb4_model.add(enb4_wo_top)\nenb4_model.add(keras.layers.Flatten())\nenb4_model.add(keras.layers.Dense(128, activation='relu'))\nenb4_model.add(keras.layers.Dense(64, activation='relu'))\nenb4_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb4_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb4_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T16:13:54.113603Z","iopub.status.busy":"2023-08-08T16:13:54.113228Z","iopub.status.idle":"2023-08-08T16:24:36.452653Z","shell.execute_reply":"2023-08-08T16:24:36.451603Z","shell.execute_reply.started":"2023-08-08T16:13:54.113571Z"}},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n\n71686520/71686520 [==============================] - 0s 0us/step\n\nEpoch 1/30\n"},{"name":"stderr","output_type":"stream","text":"2023-08-08 16:14:13.795919: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/efficientnetb4/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"},{"name":"stdout","output_type":"stream","text":"52/52 [==============================] - 39s 496ms/step - loss: 16.2970 - accuracy: 0.7723 - val_loss: 2.4699 - val_accuracy: 0.8170\n\nEpoch 2/30\n\n52/52 [==============================] - 22s 424ms/step - loss: 1.6939 - accuracy: 0.8611 - val_loss: 0.9919 - val_accuracy: 0.8668\n\nEpoch 3/30\n\n52/52 [==============================] - 18s 353ms/step - loss: 1.0390 - accuracy: 0.8907 - val_loss: 1.7910 - val_accuracy: 0.8053\n\nEpoch 4/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.8329 - accuracy: 0.9070 - val_loss: 0.9539 - val_accuracy: 0.9139\n\nEpoch 5/30\n\n52/52 [==============================] - 22s 422ms/step - loss: 1.1016 - accuracy: 0.8979 - val_loss: 1.6362 - val_accuracy: 0.8931\n\nEpoch 6/30\n\n52/52 [==============================] - 18s 352ms/step - loss: 0.9206 - accuracy: 0.9046 - val_loss: 0.7742 - val_accuracy: 0.9130\n\nEpoch 7/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.4585 - accuracy: 0.9330 - val_loss: 0.6695 - val_accuracy: 0.9221\n\nEpoch 8/30\n\n52/52 [==============================] - 18s 352ms/step - loss: 0.3021 - accuracy: 0.9529 - val_loss: 0.6316 - val_accuracy: 0.9248\n\nEpoch 9/30\n\n52/52 [==============================] - 22s 422ms/step - loss: 0.3068 - accuracy: 0.9475 - val_loss: 0.3934 - val_accuracy: 0.9493\n\nEpoch 10/30\n\n52/52 [==============================] - 18s 353ms/step - loss: 0.5822 - accuracy: 0.9306 - val_loss: 0.8503 - val_accuracy: 0.9203\n\nEpoch 11/30\n\n52/52 [==============================] - 18s 352ms/step - loss: 0.5044 - accuracy: 0.9348 - val_loss: 0.5839 - val_accuracy: 0.9266\n\nEpoch 12/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.4016 - accuracy: 0.9420 - val_loss: 2.0281 - val_accuracy: 0.8931\n\nEpoch 13/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.5542 - accuracy: 0.9390 - val_loss: 1.0836 - val_accuracy: 0.8940\n\nEpoch 14/30\n\n52/52 [==============================] - 22s 422ms/step - loss: 0.3785 - accuracy: 0.9511 - val_loss: 0.5732 - val_accuracy: 0.9330\n\nEpoch 15/30\n\n52/52 [==============================] - 22s 422ms/step - loss: 0.2578 - accuracy: 0.9499 - val_loss: 0.6669 - val_accuracy: 0.8995\n\nEpoch 16/30\n\n52/52 [==============================] - 18s 353ms/step - loss: 0.3288 - accuracy: 0.9505 - val_loss: 0.6144 - val_accuracy: 0.9438\n\nEpoch 17/30\n\n52/52 [==============================] - 18s 352ms/step - loss: 0.2504 - accuracy: 0.9626 - val_loss: 0.4677 - val_accuracy: 0.9357\n\nEpoch 18/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.3787 - accuracy: 0.9505 - val_loss: 1.0466 - val_accuracy: 0.8542\n\nEpoch 19/30\n\n52/52 [==============================] - 18s 354ms/step - loss: 0.2267 - accuracy: 0.9607 - val_loss: 0.4676 - val_accuracy: 0.9511\n\nEpoch 20/30\n\n52/52 [==============================] - 18s 353ms/step - loss: 0.1656 - accuracy: 0.9674 - val_loss: 0.5972 - val_accuracy: 0.9330\n\nEpoch 21/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.2028 - accuracy: 0.9710 - val_loss: 0.5200 - val_accuracy: 0.9466\n\nEpoch 22/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.0902 - accuracy: 0.9795 - val_loss: 0.7485 - val_accuracy: 0.9366\n\nEpoch 23/30\n\n52/52 [==============================] - 18s 352ms/step - loss: 0.1333 - accuracy: 0.9740 - val_loss: 0.3964 - val_accuracy: 0.9457\n\nEpoch 24/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.2764 - accuracy: 0.9565 - val_loss: 0.3097 - val_accuracy: 0.9556\n\nEpoch 25/30\n\n52/52 [==============================] - 18s 352ms/step - loss: 0.1467 - accuracy: 0.9656 - val_loss: 0.6119 - val_accuracy: 0.9366\n\nEpoch 26/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.1342 - accuracy: 0.9740 - val_loss: 0.5946 - val_accuracy: 0.9257\n\nEpoch 27/30\n\n52/52 [==============================] - 22s 423ms/step - loss: 0.1692 - accuracy: 0.9650 - val_loss: 0.4676 - val_accuracy: 0.9357\n\nEpoch 28/30\n\n52/52 [==============================] - 22s 422ms/step - loss: 0.1914 - accuracy: 0.9692 - val_loss: 0.3298 - val_accuracy: 0.9438\n\nEpoch 29/30\n\n52/52 [==============================] - 22s 422ms/step - loss: 0.0901 - accuracy: 0.9789 - val_loss: 0.3811 - val_accuracy: 0.9511\n\nEpoch 30/30\n\n52/52 [==============================] - 22s 422ms/step - loss: 0.1905 - accuracy: 0.9650 - val_loss: 0.7595 - val_accuracy: 0.9384\n"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x79b0e60ae3e0>"]},"metadata":{}}]},{"cell_type":"markdown","source":"### EfficientNet B5","metadata":{}},{"cell_type":"code","source":"enb5_wo_top = keras.applications.efficientnet.EfficientNetB5(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb5_wo_top.trainable = False\n\nenb5_model = keras.models.Sequential()\nenb5_model.add(enb5_wo_top)\nenb5_model.add(keras.layers.Flatten())\nenb5_model.add(keras.layers.Dense(128, activation='relu'))\nenb5_model.add(keras.layers.Dense(64, activation='relu'))\nenb5_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb5_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb5_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T16:25:14.414558Z","iopub.status.busy":"2023-08-08T16:25:14.413483Z","iopub.status.idle":"2023-08-08T16:38:53.342629Z","shell.execute_reply":"2023-08-08T16:38:53.341476Z","shell.execute_reply.started":"2023-08-08T16:25:14.414504Z"}},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n\n115263384/115263384 [==============================] - 1s 0us/step\n\nEpoch 1/30\n"},{"name":"stderr","output_type":"stream","text":"2023-08-08 16:25:39.832937: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/efficientnetb5/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"},{"name":"stdout","output_type":"stream","text":"52/52 [==============================] - 58s 794ms/step - loss: 14.6170 - accuracy: 0.7874 - val_loss: 3.3066 - val_accuracy: 0.8623\n\nEpoch 2/30\n\n52/52 [==============================] - 25s 492ms/step - loss: 3.8318 - accuracy: 0.8315 - val_loss: 3.0094 - val_accuracy: 0.8877\n\nEpoch 3/30\n\n52/52 [==============================] - 25s 491ms/step - loss: 3.3900 - accuracy: 0.8521 - val_loss: 5.2666 - val_accuracy: 0.8261\n\nEpoch 4/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 1.6034 - accuracy: 0.8792 - val_loss: 1.1937 - val_accuracy: 0.8886\n\nEpoch 5/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 1.3244 - accuracy: 0.8913 - val_loss: 0.9691 - val_accuracy: 0.9022\n\nEpoch 6/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 0.6872 - accuracy: 0.9215 - val_loss: 0.8188 - val_accuracy: 0.9121\n\nEpoch 7/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.6973 - accuracy: 0.9257 - val_loss: 0.7954 - val_accuracy: 0.9149\n\nEpoch 8/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 1.1151 - accuracy: 0.9016 - val_loss: 1.6219 - val_accuracy: 0.8822\n\nEpoch 9/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 1.0556 - accuracy: 0.9076 - val_loss: 0.8859 - val_accuracy: 0.9139\n\nEpoch 10/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 0.9023 - accuracy: 0.9179 - val_loss: 0.4620 - val_accuracy: 0.9303\n\nEpoch 11/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 0.2743 - accuracy: 0.9475 - val_loss: 0.4171 - val_accuracy: 0.9275\n\nEpoch 12/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 0.3568 - accuracy: 0.9469 - val_loss: 0.3610 - val_accuracy: 0.9366\n\nEpoch 13/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 0.4385 - accuracy: 0.9457 - val_loss: 0.6243 - val_accuracy: 0.9384\n\nEpoch 14/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 0.5887 - accuracy: 0.9203 - val_loss: 0.4403 - val_accuracy: 0.9303\n\nEpoch 15/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.5186 - accuracy: 0.9245 - val_loss: 0.7303 - val_accuracy: 0.9049\n\nEpoch 16/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 0.3024 - accuracy: 0.9511 - val_loss: 0.5021 - val_accuracy: 0.9357\n\nEpoch 17/30\n\n52/52 [==============================] - 25s 490ms/step - loss: 0.2561 - accuracy: 0.9529 - val_loss: 0.2227 - val_accuracy: 0.9493\n\nEpoch 18/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.2373 - accuracy: 0.9481 - val_loss: 0.4880 - val_accuracy: 0.9248\n\nEpoch 19/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.5427 - accuracy: 0.9179 - val_loss: 0.6189 - val_accuracy: 0.9158\n\nEpoch 20/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 0.3141 - accuracy: 0.9390 - val_loss: 0.8658 - val_accuracy: 0.9031\n\nEpoch 21/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 0.2598 - accuracy: 0.9511 - val_loss: 0.3488 - val_accuracy: 0.9384\n\nEpoch 22/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 0.1560 - accuracy: 0.9674 - val_loss: 0.4051 - val_accuracy: 0.9357\n\nEpoch 23/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.2912 - accuracy: 0.9457 - val_loss: 1.3913 - val_accuracy: 0.9149\n\nEpoch 24/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.3184 - accuracy: 0.9541 - val_loss: 0.3680 - val_accuracy: 0.9375\n\nEpoch 25/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.1682 - accuracy: 0.9632 - val_loss: 0.3580 - val_accuracy: 0.9402\n\nEpoch 26/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 0.4220 - accuracy: 0.9426 - val_loss: 0.3209 - val_accuracy: 0.9447\n\nEpoch 27/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 0.2451 - accuracy: 0.9547 - val_loss: 0.7629 - val_accuracy: 0.9040\n\nEpoch 28/30\n\n52/52 [==============================] - 26s 508ms/step - loss: 0.2362 - accuracy: 0.9529 - val_loss: 0.7903 - val_accuracy: 0.9230\n\nEpoch 29/30\n\n52/52 [==============================] - 25s 491ms/step - loss: 0.2235 - accuracy: 0.9571 - val_loss: 0.2815 - val_accuracy: 0.9529\n\nEpoch 30/30\n\n52/52 [==============================] - 26s 509ms/step - loss: 0.1570 - accuracy: 0.9680 - val_loss: 0.4111 - val_accuracy: 0.9375\n"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x79b0e2f33dc0>"]},"metadata":{}}]},{"cell_type":"markdown","source":"### EfficientNet B6","metadata":{}},{"cell_type":"code","source":"enb6_wo_top = keras.applications.efficientnet.EfficientNetB6(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb6_wo_top.trainable = False\n\nenb6_model = keras.models.Sequential()\nenb6_model.add(enb6_wo_top)\nenb6_model.add(keras.layers.Flatten())\nenb6_model.add(keras.layers.Dense(128, activation='relu'))\nenb6_model.add(keras.layers.Dense(64, activation='relu'))\nenb6_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb6_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb6_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T16:38:53.345419Z","iopub.status.busy":"2023-08-08T16:38:53.345048Z","iopub.status.idle":"2023-08-08T16:59:44.213450Z","shell.execute_reply":"2023-08-08T16:59:44.211622Z","shell.execute_reply.started":"2023-08-08T16:38:53.345383Z"}},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5\n\n165234480/165234480 [==============================] - 1s 0us/step\n\nEpoch 1/30\n"},{"name":"stderr","output_type":"stream","text":"2023-08-08 16:39:21.370535: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_3/efficientnetb6/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"},{"name":"stdout","output_type":"stream","text":"52/52 [==============================] - 66s 898ms/step - loss: 20.4183 - accuracy: 0.7627 - val_loss: 4.0538 - val_accuracy: 0.8659\n\nEpoch 2/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 3.9529 - accuracy: 0.8339 - val_loss: 5.1934 - val_accuracy: 0.5806\n\nEpoch 3/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 2.7704 - accuracy: 0.8460 - val_loss: 1.9560 - val_accuracy: 0.8832\n\nEpoch 4/30\n\n52/52 [==============================] - 33s 638ms/step - loss: 1.8828 - accuracy: 0.8581 - val_loss: 5.3015 - val_accuracy: 0.5761\n\nEpoch 5/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 1.5618 - accuracy: 0.8702 - val_loss: 1.5069 - val_accuracy: 0.8560\n\nEpoch 6/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 1.7237 - accuracy: 0.8798 - val_loss: 1.6337 - val_accuracy: 0.8786\n\nEpoch 7/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 1.1548 - accuracy: 0.8998 - val_loss: 1.5990 - val_accuracy: 0.8976\n\nEpoch 8/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 1.2232 - accuracy: 0.8937 - val_loss: 1.1802 - val_accuracy: 0.8904\n\nEpoch 9/30\n\n52/52 [==============================] - 33s 639ms/step - loss: 1.2221 - accuracy: 0.9004 - val_loss: 1.3182 - val_accuracy: 0.8958\n\nEpoch 10/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 0.6280 - accuracy: 0.9287 - val_loss: 0.8672 - val_accuracy: 0.8986\n\nEpoch 11/30\n\n52/52 [==============================] - 33s 638ms/step - loss: 1.0145 - accuracy: 0.9106 - val_loss: 1.2674 - val_accuracy: 0.8976\n\nEpoch 12/30\n\n52/52 [==============================] - 33s 639ms/step - loss: 0.8810 - accuracy: 0.9136 - val_loss: 0.8386 - val_accuracy: 0.9185\n\nEpoch 13/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 0.7672 - accuracy: 0.9203 - val_loss: 0.7996 - val_accuracy: 0.9158\n\nEpoch 14/30\n\n52/52 [==============================] - 33s 639ms/step - loss: 0.6955 - accuracy: 0.9221 - val_loss: 1.8366 - val_accuracy: 0.8714\n\nEpoch 15/30\n\n52/52 [==============================] - 33s 639ms/step - loss: 0.9098 - accuracy: 0.9167 - val_loss: 1.0636 - val_accuracy: 0.9158\n\nEpoch 16/30\n\n52/52 [==============================] - 41s 803ms/step - loss: 0.7765 - accuracy: 0.9191 - val_loss: 0.8637 - val_accuracy: 0.9040\n\nEpoch 17/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 0.4665 - accuracy: 0.9342 - val_loss: 0.6938 - val_accuracy: 0.9149\n\nEpoch 18/30\n\n52/52 [==============================] - 41s 803ms/step - loss: 0.6344 - accuracy: 0.9227 - val_loss: 1.7362 - val_accuracy: 0.9167\n\nEpoch 19/30\n\n52/52 [==============================] - 33s 639ms/step - loss: 0.3599 - accuracy: 0.9493 - val_loss: 0.6368 - val_accuracy: 0.9185\n\nEpoch 20/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 0.3544 - accuracy: 0.9481 - val_loss: 0.9718 - val_accuracy: 0.9158\n\nEpoch 21/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 0.3587 - accuracy: 0.9511 - val_loss: 0.8504 - val_accuracy: 0.9149\n\nEpoch 22/30\n\n52/52 [==============================] - 41s 803ms/step - loss: 1.2400 - accuracy: 0.9167 - val_loss: 2.3856 - val_accuracy: 0.8714\n\nEpoch 23/30\n\n52/52 [==============================] - 41s 801ms/step - loss: 0.8124 - accuracy: 0.9143 - val_loss: 1.4851 - val_accuracy: 0.8460\n\nEpoch 24/30\n\n52/52 [==============================] - 41s 803ms/step - loss: 0.4989 - accuracy: 0.9360 - val_loss: 2.3869 - val_accuracy: 0.8813\n\nEpoch 25/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 0.7575 - accuracy: 0.9239 - val_loss: 1.4215 - val_accuracy: 0.9076\n\nEpoch 26/30\n\n52/52 [==============================] - 41s 803ms/step - loss: 0.6724 - accuracy: 0.9275 - val_loss: 1.4842 - val_accuracy: 0.8913\n\nEpoch 27/30\n\n52/52 [==============================] - 41s 803ms/step - loss: 0.3315 - accuracy: 0.9469 - val_loss: 0.7131 - val_accuracy: 0.9031\n\nEpoch 28/30\n\n52/52 [==============================] - 41s 801ms/step - loss: 0.1992 - accuracy: 0.9529 - val_loss: 0.6861 - val_accuracy: 0.9275\n\nEpoch 29/30\n\n52/52 [==============================] - 41s 802ms/step - loss: 0.3118 - accuracy: 0.9463 - val_loss: 0.8560 - val_accuracy: 0.8659\n\nEpoch 30/30\n\n52/52 [==============================] - 33s 639ms/step - loss: 0.2137 - accuracy: 0.9565 - val_loss: 0.5036 - val_accuracy: 0.9330\n"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x79b0df381270>"]},"metadata":{}}]},{"cell_type":"markdown","source":"### EfficientNet B7","metadata":{}},{"cell_type":"code","source":"enb7_wo_top = keras.applications.efficientnet.EfficientNetB7(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nenb7_wo_top.trainable = False\n\nenb7_model = keras.models.Sequential()\nenb7_model.add(enb7_wo_top)\nenb7_model.add(keras.layers.Flatten())\nenb7_model.add(keras.layers.Dense(128, activation='relu'))\nenb7_model.add(keras.layers.Dense(64, activation='relu'))\nenb7_model.add(keras.layers.Dense(8, activation='softmax'))\n\nenb7_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nenb7_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T16:59:44.215591Z","iopub.status.busy":"2023-08-08T16:59:44.215017Z","iopub.status.idle":"2023-08-08T17:24:42.082755Z","shell.execute_reply":"2023-08-08T17:24:42.081673Z","shell.execute_reply.started":"2023-08-08T16:59:44.215552Z"}},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n\n258076736/258076736 [==============================] - 1s 0us/step\n\nEpoch 1/30\n"},{"name":"stderr","output_type":"stream","text":"2023-08-08 17:00:18.938491: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_4/efficientnetb7/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"},{"name":"stdout","output_type":"stream","text":"52/52 [==============================] - 76s 1s/step - loss: 10.4503 - accuracy: 0.7766 - val_loss: 6.5107 - val_accuracy: 0.8632\n\nEpoch 2/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 3.7033 - accuracy: 0.8484 - val_loss: 1.7107 - val_accuracy: 0.8723\n\nEpoch 3/30\n\n52/52 [==============================] - 44s 853ms/step - loss: 1.8217 - accuracy: 0.8545 - val_loss: 1.4632 - val_accuracy: 0.8587\n\nEpoch 4/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 1.4227 - accuracy: 0.8738 - val_loss: 1.0049 - val_accuracy: 0.8859\n\nEpoch 5/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.8374 - accuracy: 0.8859 - val_loss: 0.7162 - val_accuracy: 0.9049\n\nEpoch 6/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.5495 - accuracy: 0.9064 - val_loss: 0.8654 - val_accuracy: 0.8949\n\nEpoch 7/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.5023 - accuracy: 0.9058 - val_loss: 0.7524 - val_accuracy: 0.9022\n\nEpoch 8/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.4077 - accuracy: 0.9161 - val_loss: 0.9410 - val_accuracy: 0.8832\n\nEpoch 9/30\n\n52/52 [==============================] - 48s 932ms/step - loss: 0.5059 - accuracy: 0.9185 - val_loss: 1.4617 - val_accuracy: 0.7246\n\nEpoch 10/30\n\n52/52 [==============================] - 44s 854ms/step - loss: 0.4325 - accuracy: 0.9173 - val_loss: 0.8119 - val_accuracy: 0.8877\n\nEpoch 11/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.6825 - accuracy: 0.8937 - val_loss: 0.6470 - val_accuracy: 0.9194\n\nEpoch 12/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.5215 - accuracy: 0.9124 - val_loss: 0.6693 - val_accuracy: 0.8668\n\nEpoch 13/30\n\n52/52 [==============================] - 44s 853ms/step - loss: 0.2647 - accuracy: 0.9426 - val_loss: 0.5267 - val_accuracy: 0.9085\n\nEpoch 14/30\n\n52/52 [==============================] - 44s 853ms/step - loss: 0.2088 - accuracy: 0.9481 - val_loss: 0.3519 - val_accuracy: 0.9293\n\nEpoch 15/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.1818 - accuracy: 0.9559 - val_loss: 0.3727 - val_accuracy: 0.9293\n\nEpoch 16/30\n\n52/52 [==============================] - 44s 851ms/step - loss: 0.2094 - accuracy: 0.9505 - val_loss: 0.4651 - val_accuracy: 0.9004\n\nEpoch 17/30\n\n52/52 [==============================] - 44s 852ms/step - loss: 0.1686 - accuracy: 0.9553 - val_loss: 0.3692 - val_accuracy: 0.9447\n\nEpoch 18/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.2183 - accuracy: 0.9463 - val_loss: 0.2417 - val_accuracy: 0.9420\n\nEpoch 19/30\n\n52/52 [==============================] - 44s 851ms/step - loss: 0.2760 - accuracy: 0.9384 - val_loss: 0.4739 - val_accuracy: 0.9004\n\nEpoch 20/30\n\n52/52 [==============================] - 44s 850ms/step - loss: 0.2889 - accuracy: 0.9354 - val_loss: 0.5266 - val_accuracy: 0.9221\n\nEpoch 21/30\n\n52/52 [==============================] - 48s 932ms/step - loss: 0.1261 - accuracy: 0.9614 - val_loss: 0.2756 - val_accuracy: 0.9420\n\nEpoch 22/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.0899 - accuracy: 0.9698 - val_loss: 0.2228 - val_accuracy: 0.9520\n\nEpoch 23/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.1861 - accuracy: 0.9595 - val_loss: 0.3230 - val_accuracy: 0.9393\n\nEpoch 24/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.0788 - accuracy: 0.9783 - val_loss: 0.3621 - val_accuracy: 0.9076\n\nEpoch 25/30\n\n52/52 [==============================] - 48s 930ms/step - loss: 0.1802 - accuracy: 0.9499 - val_loss: 0.6997 - val_accuracy: 0.9094\n\nEpoch 26/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.2497 - accuracy: 0.9444 - val_loss: 0.2623 - val_accuracy: 0.9466\n\nEpoch 27/30\n\n52/52 [==============================] - 48s 932ms/step - loss: 0.1368 - accuracy: 0.9638 - val_loss: 0.3445 - val_accuracy: 0.9303\n\nEpoch 28/30\n\n52/52 [==============================] - 48s 932ms/step - loss: 0.2623 - accuracy: 0.9475 - val_loss: 0.3760 - val_accuracy: 0.9366\n\nEpoch 29/30\n\n52/52 [==============================] - 48s 933ms/step - loss: 0.1526 - accuracy: 0.9559 - val_loss: 0.2582 - val_accuracy: 0.9366\n\nEpoch 30/30\n\n52/52 [==============================] - 44s 854ms/step - loss: 0.1258 - accuracy: 0.9662 - val_loss: 0.3750 - val_accuracy: 0.9348\n"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x79b0db3cd810>"]},"metadata":{}}]},{"cell_type":"code","source":"resnet_wo_top = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n\nresnet_wo_top.trainable = False\n\nresnet_model = keras.models.Sequential()\nresnet_model.add(resnet_wo_top)\nresnet_model.add(keras.layers.Flatten())\nresnet_model.add(keras.layers.Dense(128, activation='relu'))\nresnet_model.add(keras.layers.Dense(64, activation='relu'))\nresnet_model.add(keras.layers.Dense(8, activation='softmax'))\n\nresnet_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n\nresnet_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2023-08-08T17:24:42.166382Z","iopub.status.busy":"2023-08-08T17:24:42.165559Z","iopub.status.idle":"2023-08-08T17:30:11.913714Z","shell.execute_reply":"2023-08-08T17:30:11.912702Z","shell.execute_reply.started":"2023-08-08T17:24:42.166345Z"}},"execution_count":19,"outputs":[{"name":"stdout","output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n94765736/94765736 [==============================] - 1s 0us/step\n\nEpoch 1/30\n\n52/52 [==============================] - 19s 266ms/step - loss: 9.5755 - accuracy: 0.8110 - val_loss: 4.1062 - val_accuracy: 0.8940\n\nEpoch 2/30\n\n52/52 [==============================] - 10s 196ms/step - loss: 3.2696 - accuracy: 0.9004 - val_loss: 2.3007 - val_accuracy: 0.9221\n\nEpoch 3/30\n\n52/52 [==============================] - 11s 219ms/step - loss: 0.8234 - accuracy: 0.9457 - val_loss: 1.0823 - val_accuracy: 0.9429\n\nEpoch 4/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.9841 - accuracy: 0.9499 - val_loss: 1.3779 - val_accuracy: 0.9167\n\nEpoch 5/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 1.6150 - accuracy: 0.9372 - val_loss: 1.8411 - val_accuracy: 0.9239\n\nEpoch 6/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.3091 - accuracy: 0.9746 - val_loss: 1.3336 - val_accuracy: 0.9447\n\nEpoch 7/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.1924 - accuracy: 0.9855 - val_loss: 0.6305 - val_accuracy: 0.9629\n\nEpoch 8/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 0.3304 - accuracy: 0.9813 - val_loss: 1.5922 - val_accuracy: 0.9176\n\nEpoch 9/30\n\n52/52 [==============================] - 10s 198ms/step - loss: 0.1440 - accuracy: 0.9855 - val_loss: 1.3015 - val_accuracy: 0.9493\n\nEpoch 10/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.0374 - accuracy: 0.9952 - val_loss: 0.8229 - val_accuracy: 0.9692\n\nEpoch 11/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 0.0228 - accuracy: 0.9964 - val_loss: 0.9358 - val_accuracy: 0.9638\n\nEpoch 12/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.1565 - accuracy: 0.9903 - val_loss: 1.0917 - val_accuracy: 0.9592\n\nEpoch 13/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.1788 - accuracy: 0.9795 - val_loss: 0.8329 - val_accuracy: 0.9620\n\nEpoch 14/30\n\n52/52 [==============================] - 11s 221ms/step - loss: 0.1477 - accuracy: 0.9861 - val_loss: 1.7373 - val_accuracy: 0.9511\n\nEpoch 15/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 0.1521 - accuracy: 0.9855 - val_loss: 2.8388 - val_accuracy: 0.9149\n\nEpoch 16/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.4384 - accuracy: 0.9783 - val_loss: 1.2302 - val_accuracy: 0.9583\n\nEpoch 17/30\n\n52/52 [==============================] - 10s 199ms/step - loss: 0.2270 - accuracy: 0.9867 - val_loss: 1.0613 - val_accuracy: 0.9529\n\nEpoch 18/30\n\n52/52 [==============================] - 10s 199ms/step - loss: 0.0851 - accuracy: 0.9946 - val_loss: 1.8737 - val_accuracy: 0.9420\n\nEpoch 19/30\n\n52/52 [==============================] - 10s 197ms/step - loss: 0.0338 - accuracy: 0.9976 - val_loss: 1.1597 - val_accuracy: 0.9683\n\nEpoch 20/30\n\n52/52 [==============================] - 10s 198ms/step - loss: 0.0018 - accuracy: 0.9988 - val_loss: 1.4556 - val_accuracy: 0.9529\n\nEpoch 21/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 0.0495 - accuracy: 0.9952 - val_loss: 0.7854 - val_accuracy: 0.9692\n\nEpoch 22/30\n\n52/52 [==============================] - 10s 198ms/step - loss: 6.2951e-05 - accuracy: 1.0000 - val_loss: 0.6948 - val_accuracy: 0.9692\n\nEpoch 23/30\n\n52/52 [==============================] - 10s 198ms/step - loss: 2.7427e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n\nEpoch 24/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 2.7355e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n\nEpoch 25/30\n\n52/52 [==============================] - 10s 198ms/step - loss: 2.7211e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n\nEpoch 26/30\n\n52/52 [==============================] - 10s 198ms/step - loss: 2.7139e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n\nEpoch 27/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 2.6995e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n\nEpoch 28/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 2.6923e-08 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.9692\n\nEpoch 29/30\n\n52/52 [==============================] - 11s 220ms/step - loss: 2.6707e-08 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.9692\n\nEpoch 30/30\n\n52/52 [==============================] - 10s 199ms/step - loss: 2.6563e-08 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.9692\n"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x79b0e0910e80>"]},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}