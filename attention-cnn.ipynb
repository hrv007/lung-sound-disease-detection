{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:42.860527Z",
     "iopub.status.busy": "2023-08-17T14:29:42.860186Z",
     "iopub.status.idle": "2023-08-17T14:29:51.215228Z",
     "shell.execute_reply": "2023-08-17T14:29:51.214292Z",
     "shell.execute_reply.started": "2023-08-17T14:29:42.860496Z"
    },
    "id": "EXy5Ggoqczap"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.217413Z",
     "iopub.status.busy": "2023-08-17T14:29:51.216802Z",
     "iopub.status.idle": "2023-08-17T14:29:51.251250Z",
     "shell.execute_reply": "2023-08-17T14:29:51.250518Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.217388Z"
    },
    "id": "ArtW5CVwuApp",
    "outputId": "fe2935a3-842f-4305-996a-bb1edcb03165"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>101</th>\n",
       "      <th>URTI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102</td>\n",
       "      <td>Healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "      <td>Asthma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>COPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105</td>\n",
       "      <td>URTI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>COPD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   101     URTI\n",
       "0  102  Healthy\n",
       "1  103   Asthma\n",
       "2  104     COPD\n",
       "3  105     URTI\n",
       "4  106     COPD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/attention-cnn-model/patient_diagnosis.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.252811Z",
     "iopub.status.busy": "2023-08-17T14:29:51.252199Z",
     "iopub.status.idle": "2023-08-17T14:29:51.260918Z",
     "shell.execute_reply": "2023-08-17T14:29:51.259871Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.252784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Healthy', 'Asthma', 'COPD', 'URTI', 'LRTI', 'Bronchiectasis',\n",
       "       'Pneumonia', 'Bronchiolitis'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['URTI'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.264106Z",
     "iopub.status.busy": "2023-08-17T14:29:51.263794Z",
     "iopub.status.idle": "2023-08-17T14:29:51.274298Z",
     "shell.execute_reply": "2023-08-17T14:29:51.273307Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.264083Z"
    },
    "id": "eAdDHbX6u_iz"
   },
   "outputs": [],
   "source": [
    "sr_no = {'101':'URTI'}\n",
    "for i, j in zip(df['101'].unique(), df['URTI']):\n",
    "    sr_no[str(i)] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.276313Z",
     "iopub.status.busy": "2023-08-17T14:29:51.275392Z",
     "iopub.status.idle": "2023-08-17T14:29:51.289332Z",
     "shell.execute_reply": "2023-08-17T14:29:51.288135Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.276283Z"
    },
    "id": "1j95yfuivIOL",
    "outputId": "c4ce184c-bb32-43e4-acbb-ef143254077e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_no.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.291752Z",
     "iopub.status.busy": "2023-08-17T14:29:51.291113Z",
     "iopub.status.idle": "2023-08-17T14:29:51.804308Z",
     "shell.execute_reply": "2023-08-17T14:29:51.803666Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.291719Z"
    },
    "id": "k_apddzYvK_5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "sound_files = os.listdir('/kaggle/input/attention-cnn-model/Mel Spectrogram/Mel Spectrogram/Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.806287Z",
     "iopub.status.busy": "2023-08-17T14:29:51.805687Z",
     "iopub.status.idle": "2023-08-17T14:29:51.832003Z",
     "shell.execute_reply": "2023-08-17T14:29:51.831154Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.806250Z"
    },
    "id": "jNTE66gVvVEg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['176_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['112_1p1_Pr_sc_Litt3200.png'],\n",
       " ['130_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['193_7b3_Ll_mc_AKGC417L.png'],\n",
       " ['156_5b3_Pl_mc_AKGC417L.png'],\n",
       " ['162_2b2_Tc_mc_AKGC417L.png'],\n",
       " ['134_2b3_Ar_mc_LittC2SE.png'],\n",
       " ['160_1b4_Tc_mc_AKGC417L.png'],\n",
       " ['200_2p4_Pl_mc_AKGC417L.png'],\n",
       " ['169_1b2_Ll_sc_Meditron.png'],\n",
       " ['144_1b1_Tc_sc_Meditron.png'],\n",
       " ['160_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['133_2p3_Pr_mc_AKGC417L.png'],\n",
       " ['205_1b3_Al_mc_AKGC417L.png'],\n",
       " ['130_3p4_Tc_mc_AKGC417L.png'],\n",
       " ['130_1p4_Lr_mc_AKGC417L.png'],\n",
       " ['200_3p4_Pl_mc_AKGC417L.png'],\n",
       " ['107_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['203_2p3_Al_mc_AKGC417L.png'],\n",
       " ['113_1b1_Ar_sc_Litt3200.png'],\n",
       " ['174_2p3_Al_mc_AKGC417L.png'],\n",
       " ['178_1b2_Lr_mc_AKGC417L.png'],\n",
       " ['172_1b5_Ll_mc_AKGC417L.png'],\n",
       " ['216_1b1_Al_sc_Meditron.png'],\n",
       " ['205_4b2_Pl_mc_AKGC417L.png'],\n",
       " ['177_1b2_Lr_mc_AKGC417L.png'],\n",
       " ['151_2p4_Ll_mc_AKGC417L.png'],\n",
       " ['141_1b1_Pr_mc_LittC2SE.png'],\n",
       " ['162_1b2_Lr_mc_AKGC417L.png'],\n",
       " ['133_3p4_Tc_mc_AKGC417L.png'],\n",
       " ['147_2b3_Ll_mc_AKGC417L.png'],\n",
       " ['143_1b1_Al_sc_Meditron.png'],\n",
       " ['218_1p1_Pr_sc_Litt3200.png'],\n",
       " ['138_2p2_Al_mc_AKGC417L.png'],\n",
       " ['113_1b1_Al_sc_Litt3200.png'],\n",
       " ['144_1b1_Al_sc_Meditron.png'],\n",
       " ['130_1p4_Pl_mc_AKGC417L.png'],\n",
       " ['185_1b1_Ar_sc_Litt3200.png'],\n",
       " ['138_1p3_Pl_mc_AKGC417L.png'],\n",
       " ['122_2b2_Ar_mc_LittC2SE.png'],\n",
       " ['126_1b1_Al_sc_Meditron.png'],\n",
       " ['140_2b3_Ll_mc_LittC2SE.png'],\n",
       " ['160_1b3_Al_mc_AKGC417L.png'],\n",
       " ['186_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['176_2b3_Pr_mc_AKGC417L.png'],\n",
       " ['160_1b2_Al_mc_AKGC417L.png'],\n",
       " ['136_1b1_Ar_sc_Meditron.png'],\n",
       " ['172_2b5_Tc_mc_AKGC417L.png'],\n",
       " ['176_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['178_2b2_Lr_mc_AKGC417L.png'],\n",
       " ['162_1b2_Al_mc_AKGC417L.png'],\n",
       " ['133_2p2_Tc_mc_AKGC417L.png'],\n",
       " ['205_4b2_Al_mc_AKGC417L.png'],\n",
       " ['113_1b1_Pl_sc_Litt3200.png'],\n",
       " ['178_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['156_5b3_Pr_mc_AKGC417L.png'],\n",
       " ['198_6p1_Ar_mc_AKGC417L.png'],\n",
       " ['213_1p3_Pr_mc_AKGC417L.png'],\n",
       " ['170_1b2_Ar_mc_AKGC417L.png'],\n",
       " ['204_7p5_Lr_mc_AKGC417L.png'],\n",
       " ['107_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['176_2b3_Tc_mc_AKGC417L.png'],\n",
       " ['197_1b1_Tc_sc_Meditron.png'],\n",
       " ['196_1b1_Pr_sc_Meditron.png'],\n",
       " ['130_1p2_Pr_mc_AKGC417L.png'],\n",
       " ['146_8p3_Ar_mc_AKGC417L.png'],\n",
       " ['151_3p2_Ar_mc_AKGC417L.png'],\n",
       " ['141_1b2_Tc_mc_LittC2SE.png'],\n",
       " ['103_2b2_Ar_mc_LittC2SE.png'],\n",
       " ['211_2p4_Tc_mc_AKGC417L.png'],\n",
       " ['160_1b4_Pr_mc_AKGC417L.png'],\n",
       " ['176_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['156_8b3_Ll_mc_AKGC417L.png'],\n",
       " ['205_4b2_Pr_mc_AKGC417L.png'],\n",
       " ['172_1b3_Tc_mc_AKGC417L.png'],\n",
       " ['133_2p3_Pl_mc_AKGC417L.png'],\n",
       " ['154_4b4_Pl_mc_AKGC417L.png'],\n",
       " ['176_1b4_Tc_mc_AKGC417L.png'],\n",
       " ['107_3p2_Tc_mc_AKGC417L.png'],\n",
       " ['114_1b4_Al_mc_AKGC417L.png'],\n",
       " ['207_2b4_Tc_mc_AKGC417L.png'],\n",
       " ['130_2p5_Lr_mc_AKGC417L.png'],\n",
       " ['198_6p1_Pl_mc_AKGC417L.png'],\n",
       " ['138_1p4_Ar_mc_AKGC417L.png'],\n",
       " ['151_2p3_Ar_mc_AKGC417L.png'],\n",
       " ['147_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['198_1b5_Lr_mc_AKGC417L.png'],\n",
       " ['135_2b1_Tc_mc_LittC2SE.png'],\n",
       " ['200_2p2_Tc_mc_AKGC417L.png'],\n",
       " ['159_1b1_Pr_sc_Meditron.png'],\n",
       " ['186_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['156_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['116_1b2_Tc_sc_Meditron.png'],\n",
       " ['156_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['213_2p2_Ar_mc_AKGC417L.png'],\n",
       " ['213_1p3_Ar_mc_AKGC417L.png'],\n",
       " ['120_1b1_Al_sc_Meditron.png'],\n",
       " ['160_1b3_Lr_mc_AKGC417L.png'],\n",
       " ['207_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['177_1b2_Al_mc_AKGC417L.png'],\n",
       " ['221_2b2_Al_mc_LittC2SE.png'],\n",
       " ['133_2p4_Al_mc_AKGC417L.png'],\n",
       " ['154_2b4_Tc_mc_AKGC417L.png'],\n",
       " ['205_2b3_Ll_mc_AKGC417L.png'],\n",
       " ['178_1b3_Tc_mc_AKGC417L.png'],\n",
       " ['158_1p4_Al_mc_AKGC417L.png'],\n",
       " ['205_3b4_Ar_mc_AKGC417L.png'],\n",
       " ['159_1b1_Ll_sc_Meditron.png'],\n",
       " ['138_2p2_Ll_mc_AKGC417L.png'],\n",
       " ['205_1b3_Ll_mc_AKGC417L.png'],\n",
       " ['176_1b4_Pl_mc_AKGC417L.png'],\n",
       " ['162_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['151_2p4_Lr_mc_AKGC417L.png'],\n",
       " ['207_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['135_2b3_Al_mc_LittC2SE.png'],\n",
       " ['151_2p3_Ll_mc_AKGC417L.png'],\n",
       " ['101_1b1_Pr_sc_Meditron.png'],\n",
       " ['104_1b1_Ar_sc_Litt3200.png'],\n",
       " ['135_2b3_Pr_mc_LittC2SE.png'],\n",
       " ['151_3p2_Tc_mc_AKGC417L.png'],\n",
       " ['170_2b2_Lr_mc_AKGC417L.png'],\n",
       " ['140_2b2_Tc_mc_LittC2SE.png'],\n",
       " ['218_1p1_Pl_sc_Litt3200.png'],\n",
       " ['170_1b4_Tc_mc_AKGC417L.png'],\n",
       " ['192_2b3_Ar_mc_LittC2SE.png'],\n",
       " ['200_2p4_Al_mc_AKGC417L.png'],\n",
       " ['204_7p5_Ar_mc_AKGC417L.png'],\n",
       " ['178_2b2_Tc_mc_AKGC417L.png'],\n",
       " ['135_2b3_Tc_mc_LittC2SE.png'],\n",
       " ['221_2b2_Lr_mc_LittC2SE.png'],\n",
       " ['130_3b4_Pr_mc_AKGC417L.png'],\n",
       " ['162_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['138_1p2_Ll_mc_AKGC417L.png'],\n",
       " ['109_1b1_Lr_sc_Litt3200.png'],\n",
       " ['213_1p2_Tc_mc_AKGC417L.png'],\n",
       " ['122_2b2_Tc_mc_LittC2SE.png'],\n",
       " ['174_1p4_Tc_mc_AKGC417L.png'],\n",
       " ['122_2b1_Tc_mc_LittC2SE.png'],\n",
       " ['141_1b2_Ar_mc_LittC2SE.png'],\n",
       " ['198_6p1_Lr_mc_AKGC417L.png'],\n",
       " ['149_1b1_Al_sc_Meditron.png'],\n",
       " ['176_2b3_Al_mc_AKGC417L.png'],\n",
       " ['107_2b5_Tc_mc_AKGC417L.png'],\n",
       " ['109_1b1_Al_sc_Litt3200.png'],\n",
       " ['213_1p5_Pl_mc_AKGC417L.png'],\n",
       " ['158_1p4_Tc_mc_AKGC417L.png'],\n",
       " ['200_2p3_Pl_mc_AKGC417L.png'],\n",
       " ['177_1b2_Pl_mc_AKGC417L.png'],\n",
       " ['151_2p3_Al_mc_AKGC417L.png'],\n",
       " ['147_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['175_1b1_Pr_sc_Litt3200.png'],\n",
       " ['157_1b1_Ar_sc_Meditron.png'],\n",
       " ['122_2b3_Tc_mc_LittC2SE.png'],\n",
       " ['203_1p3_Tc_mc_AKGC417L.png'],\n",
       " ['166_1p1_Pl_sc_Meditron.png'],\n",
       " ['213_1p2_Al_mc_AKGC417L.png'],\n",
       " ['215_1b3_Tc_sc_Meditron.png'],\n",
       " ['110_1p1_Ll_sc_Meditron.png'],\n",
       " ['200_2p2_Ar_mc_AKGC417L.png'],\n",
       " ['202_1b1_Ar_sc_Meditron.png'],\n",
       " ['158_1p2_Lr_mc_AKGC417L.png'],\n",
       " ['147_1b2_Tc_mc_AKGC417L.png'],\n",
       " ['147_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['174_2p3_Tc_mc_AKGC417L.png'],\n",
       " ['200_3p4_Al_mc_AKGC417L.png'],\n",
       " ['213_2p2_Pl_mc_AKGC417L.png'],\n",
       " ['177_1b2_Pr_mc_AKGC417L.png'],\n",
       " ['204_7p5_Ll_mc_AKGC417L.png'],\n",
       " ['162_2b3_Pr_mc_AKGC417L.png'],\n",
       " ['117_1b2_Tc_mc_LittC2SE.png'],\n",
       " ['140_2b2_Ll_mc_LittC2SE.png'],\n",
       " ['222_1b1_Lr_sc_Meditron.png'],\n",
       " ['130_2b2_Ll_mc_AKGC417L.png'],\n",
       " ['130_2p5_Pl_mc_AKGC417L.png'],\n",
       " ['172_2b5_Al_mc_AKGC417L.png'],\n",
       " ['147_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['138_1p3_Ll_mc_AKGC417L.png'],\n",
       " ['138_1p4_Ll_mc_AKGC417L.png'],\n",
       " ['160_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['147_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['141_1b3_Ar_mc_LittC2SE.png'],\n",
       " ['124_1b1_Ll_sc_Litt3200.png'],\n",
       " ['178_1b3_Pl_mc_AKGC417L.png'],\n",
       " ['107_3p2_Ll_mc_AKGC417L.png'],\n",
       " ['130_2p5_Tc_mc_AKGC417L.png'],\n",
       " ['106_2b1_Pl_mc_LittC2SE.png'],\n",
       " ['198_1b5_Ar_mc_AKGC417L.png'],\n",
       " ['193_7b3_Pr_mc_AKGC417L.png'],\n",
       " ['198_1b5_Pr_mc_AKGC417L.png'],\n",
       " ['203_1p4_Al_mc_AKGC417L.png'],\n",
       " ['201_1b3_Ar_sc_Meditron.png'],\n",
       " ['146_2b4_Ll_mc_AKGC417L.png'],\n",
       " ['208_1b1_Ll_sc_Meditron.png'],\n",
       " ['138_2p2_Tc_mc_AKGC417L.png'],\n",
       " ['158_1p3_Ll_mc_AKGC417L.png'],\n",
       " ['178_1b6_Ll_mc_AKGC417L.png'],\n",
       " ['130_2p5_Pr_mc_AKGC417L.png'],\n",
       " ['133_2p4_Pr_mc_AKGC417L.png'],\n",
       " ['195_1b1_Pr_sc_Litt3200.png'],\n",
       " ['170_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['107_2b5_Lr_mc_AKGC417L.png'],\n",
       " ['170_1b3_Lr_mc_AKGC417L.png'],\n",
       " ['174_1p3_Pl_mc_AKGC417L.png'],\n",
       " ['200_3p4_Ar_mc_AKGC417L.png'],\n",
       " ['178_1b6_Pr_mc_AKGC417L.png'],\n",
       " ['205_1b3_Ar_mc_AKGC417L.png'],\n",
       " ['198_6p1_Tc_mc_AKGC417L.png'],\n",
       " ['107_3p2_Pr_mc_AKGC417L.png'],\n",
       " ['174_1p2_Pr_mc_AKGC417L.png'],\n",
       " ['205_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['188_1b1_Ar_sc_Meditron.png'],\n",
       " ['151_2p4_Pr_mc_AKGC417L.png'],\n",
       " ['176_1b3_Tc_mc_AKGC417L.png'],\n",
       " ['138_1p2_Pr_mc_AKGC417L.png'],\n",
       " ['174_1p4_Ar_mc_AKGC417L.png'],\n",
       " ['158_1p4_Pl_mc_AKGC417L.png'],\n",
       " ['158_1p3_Pl_mc_AKGC417L.png'],\n",
       " ['205_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['198_6p1_Al_mc_AKGC417L.png'],\n",
       " ['170_1b2_Pl_mc_AKGC417L.png'],\n",
       " ['203_2p3_Tc_mc_AKGC417L.png'],\n",
       " ['130_3p2_Ar_mc_AKGC417L.png'],\n",
       " ['178_1b3_Al_mc_AKGC417L.png'],\n",
       " ['188_1b1_Tc_sc_Meditron.png'],\n",
       " ['147_2b2_Al_mc_AKGC417L.png'],\n",
       " ['218_1b1_Ar_sc_Meditron.png'],\n",
       " ['130_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['205_1b3_Pr_mc_AKGC417L.png'],\n",
       " ['130_2p3_Pl_mc_AKGC417L.png'],\n",
       " ['172_1b3_Pr_mc_AKGC417L.png'],\n",
       " ['200_2p2_Pr_mc_AKGC417L.png'],\n",
       " ['205_1b3_Lr_mc_AKGC417L.png'],\n",
       " ['156_8b3_Lr_mc_AKGC417L.png'],\n",
       " ['200_2p2_Al_mc_AKGC417L.png'],\n",
       " ['133_3p2_Pr_mc_AKGC417L.png'],\n",
       " ['176_1b3_Lr_mc_AKGC417L.png'],\n",
       " ['135_2b2_Tc_mc_LittC2SE.png'],\n",
       " ['207_2b2_Tc_mc_AKGC417L.png'],\n",
       " ['180_1b4_Al_mc_AKGC417L.png'],\n",
       " ['120_1b1_Lr_sc_Meditron.png'],\n",
       " ['158_1p3_Pr_mc_AKGC417L.png'],\n",
       " ['154_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['130_3b4_Pl_mc_AKGC417L.png'],\n",
       " ['107_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['107_2b5_Ll_mc_AKGC417L.png'],\n",
       " ['211_1p2_Pr_mc_AKGC417L.png'],\n",
       " ['225_1b1_Pl_sc_Meditron.png'],\n",
       " ['207_2b4_Al_mc_AKGC417L.png'],\n",
       " ['154_3b3_Ar_mc_AKGC417L.png'],\n",
       " ['113_1b1_Pr_sc_Litt3200.png'],\n",
       " ['135_2b2_Al_mc_LittC2SE.png'],\n",
       " ['213_1p2_Pr_mc_AKGC417L.png'],\n",
       " ['207_2b3_Pr_mc_AKGC417L.png'],\n",
       " ['172_2b5_Lr_mc_AKGC417L.png'],\n",
       " ['191_2b2_Tc_mc_LittC2SE.png'],\n",
       " ['130_1p3_Ar_mc_AKGC417L.png'],\n",
       " ['162_1b2_Ll_mc_AKGC417L.png'],\n",
       " ['201_1b3_Al_sc_Meditron.png'],\n",
       " ['201_1b1_Ar_sc_Meditron.png'],\n",
       " ['213_2p2_Pr_mc_AKGC417L.png'],\n",
       " ['206_1b1_Ar_sc_Meditron.png'],\n",
       " ['178_1b3_Lr_mc_AKGC417L.png'],\n",
       " ['221_2b3_Lr_mc_LittC2SE.png'],\n",
       " ['200_2p3_Lr_mc_AKGC417L.png'],\n",
       " ['129_1b1_Ar_sc_Meditron.png'],\n",
       " ['192_2b3_Al_mc_LittC2SE.png'],\n",
       " ['221_2b3_Pr_mc_LittC2SE.png'],\n",
       " ['160_1b3_Pr_mc_AKGC417L.png'],\n",
       " ['130_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['170_1b3_Al_mc_AKGC417L.png'],\n",
       " ['181_1b3_Tc_mc_LittC2SE.png'],\n",
       " ['141_1b3_Al_mc_LittC2SE.png'],\n",
       " ['186_3b3_Ar_mc_AKGC417L.png'],\n",
       " ['133_3p2_Ar_mc_AKGC417L.png'],\n",
       " ['162_1b2_Ar_mc_AKGC417L.png'],\n",
       " ['198_6p1_Ll_mc_AKGC417L.png'],\n",
       " ['130_3p2_Al_mc_AKGC417L.png'],\n",
       " ['203_2p3_Pl_mc_AKGC417L.png'],\n",
       " ['118_1b1_Al_sc_Litt3200.png'],\n",
       " ['162_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['130_1p2_Tc_mc_AKGC417L.png'],\n",
       " ['104_1b1_Al_sc_Litt3200.png'],\n",
       " ['151_3p2_Lr_mc_AKGC417L.png'],\n",
       " ['205_3b4_Pr_mc_AKGC417L.png'],\n",
       " ['213_1p2_Pl_mc_AKGC417L.png'],\n",
       " ['158_1p4_Lr_mc_AKGC417L.png'],\n",
       " ['124_1b1_Lr_sc_Litt3200.png'],\n",
       " ['193_7b3_Lr_mc_AKGC417L.png'],\n",
       " ['192_2b1_Al_mc_LittC2SE.png'],\n",
       " ['220_1b1_Tc_mc_LittC2SE.png'],\n",
       " ['164_1b1_Ll_sc_Meditron.png'],\n",
       " ['154_4b4_Al_mc_AKGC417L.png'],\n",
       " ['198_1b5_Ll_mc_AKGC417L.png'],\n",
       " ['147_1b4_Tc_mc_AKGC417L.png'],\n",
       " ['130_3p3_Al_mc_AKGC417L.png'],\n",
       " ['170_1b2_Lr_mc_AKGC417L.png'],\n",
       " ['200_2p3_Tc_mc_AKGC417L.png'],\n",
       " ['181_1b2_Ar_mc_LittC2SE.png'],\n",
       " ['124_1b1_Ar_sc_Litt3200.png'],\n",
       " ['130_3p3_Pr_mc_AKGC417L.png'],\n",
       " ['166_1p1_Ar_sc_Meditron.png'],\n",
       " ['224_1b1_Tc_sc_Meditron.png'],\n",
       " ['130_3p4_Pl_mc_AKGC417L.png'],\n",
       " ['203_2p3_Ar_mc_AKGC417L.png'],\n",
       " ['215_1b2_Ar_sc_Meditron.png'],\n",
       " ['211_1p2_Pl_mc_AKGC417L.png'],\n",
       " ['172_2b5_Ar_mc_AKGC417L.png'],\n",
       " ['147_1b3_Tc_mc_AKGC417L.png'],\n",
       " ['176_1b3_Ar_mc_AKGC417L.png'],\n",
       " ['154_3b3_Ll_mc_AKGC417L.png'],\n",
       " ['122_2b1_Ar_mc_LittC2SE.png'],\n",
       " ['192_2b2_Al_mc_LittC2SE.png'],\n",
       " ['172_1b3_Lr_mc_AKGC417L.png'],\n",
       " ['130_2b3_Al_mc_AKGC417L.png'],\n",
       " ['207_2b2_Al_mc_AKGC417L.png'],\n",
       " ['112_1p1_Ll_sc_Litt3200.png'],\n",
       " ['117_1b3_Tc_mc_LittC2SE.png'],\n",
       " ['203_1p4_Tc_mc_AKGC417L.png'],\n",
       " ['135_2b2_Pl_mc_LittC2SE.png'],\n",
       " ['156_8b3_Pl_mc_AKGC417L.png'],\n",
       " ['133_3p2_Al_mc_AKGC417L.png'],\n",
       " ['130_2p5_Al_mc_AKGC417L.png'],\n",
       " ['162_2b2_Al_mc_AKGC417L.png'],\n",
       " ['141_1b3_Pr_mc_LittC2SE.png'],\n",
       " ['195_1b1_Ll_sc_Litt3200.png'],\n",
       " ['199_2b3_Ll_mc_LittC2SE.png'],\n",
       " ['108_1b1_Al_sc_Meditron.png'],\n",
       " ['138_1p3_Lr_mc_AKGC417L.png'],\n",
       " ['122_2b1_Al_mc_LittC2SE.png'],\n",
       " ['130_1p3_Tc_mc_AKGC417L.png'],\n",
       " ['163_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['158_1p2_Ll_mc_AKGC417L.png'],\n",
       " ['163_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['133_2p4_Pl_mc_AKGC417L.png'],\n",
       " ['174_1p3_Ll_mc_AKGC417L.png'],\n",
       " ['162_1b2_Pl_mc_AKGC417L.png'],\n",
       " ['162_1b2_Tc_mc_AKGC417L.png'],\n",
       " ['172_1b4_Ll_mc_AKGC417L.png'],\n",
       " ['191_2b1_Pr_mc_LittC2SE.png'],\n",
       " ['204_7p5_Pr_mc_AKGC417L.png'],\n",
       " ['193_1b2_Al_mc_AKGC417L.png'],\n",
       " ['135_2b1_Al_mc_LittC2SE.png'],\n",
       " ['107_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['154_1b3_Lr_mc_AKGC417L.png'],\n",
       " ['107_3p2_Pl_mc_AKGC417L.png'],\n",
       " ['219_2b2_Tc_mc_LittC2SE.png'],\n",
       " ['170_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['158_1p4_Ar_mc_AKGC417L.png'],\n",
       " ['135_2b2_Ar_mc_LittC2SE.png'],\n",
       " ['207_2b3_Tc_mc_AKGC417L.png'],\n",
       " ['163_2b2_Ll_mc_AKGC417L.png'],\n",
       " ['130_2b3_Tc_mc_AKGC417L.png'],\n",
       " ['123_1b1_Al_sc_Meditron.png'],\n",
       " ['145_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['145_3b4_Pl_mc_AKGC417L.png'],\n",
       " ['160_1b3_Pl_mc_AKGC417L.png'],\n",
       " ['150_1b2_Al_sc_Meditron.png'],\n",
       " ['151_2p3_Tc_mc_AKGC417L.png'],\n",
       " ['187_1b1_Ll_sc_Meditron.png'],\n",
       " ['204_2b5_Ll_mc_AKGC417L.png'],\n",
       " ['138_1p3_Tc_mc_AKGC417L.png'],\n",
       " ['133_2p4_Tc_mc_AKGC417L.png'],\n",
       " ['160_2b4_Tc_mc_AKGC417L.png'],\n",
       " ['156_2b3_Pr_mc_AKGC417L.png'],\n",
       " ['160_1b4_Al_mc_AKGC417L.png'],\n",
       " ['176_1b4_Pr_mc_AKGC417L.png'],\n",
       " ['207_3b2_Ar_mc_AKGC417L.png'],\n",
       " ['176_1b3_Pr_mc_AKGC417L.png'],\n",
       " ['176_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['186_3b3_Pl_mc_AKGC417L.png'],\n",
       " ['148_1b1_Al_sc_Meditron.png'],\n",
       " ['167_1b1_Al_sc_Meditron.png'],\n",
       " ['176_1b4_Al_mc_AKGC417L.png'],\n",
       " ['141_1b2_Pr_mc_LittC2SE.png'],\n",
       " ['170_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['139_1b1_Al_sc_Litt3200.png'],\n",
       " ['178_1b2_Ar_mc_AKGC417L.png'],\n",
       " ['151_2p2_Ll_mc_AKGC417L.png'],\n",
       " ['200_2p3_Pr_mc_AKGC417L.png'],\n",
       " ['221_2b2_Pl_mc_LittC2SE.png'],\n",
       " ['154_4b4_Ar_mc_AKGC417L.png'],\n",
       " ['138_1p2_Lr_mc_AKGC417L.png'],\n",
       " ['133_2p4_Ar_mc_AKGC417L.png'],\n",
       " ['172_1b4_Pl_mc_AKGC417L.png'],\n",
       " ['217_1b1_Tc_sc_Meditron.png'],\n",
       " ['222_1b1_Ar_sc_Meditron.png'],\n",
       " ['162_2b4_Al_mc_AKGC417L.png'],\n",
       " ['199_2b1_Ll_mc_LittC2SE.png'],\n",
       " ['130_1p4_Al_mc_AKGC417L.png'],\n",
       " ['193_1b2_Pr_mc_AKGC417L.png'],\n",
       " ['166_1p1_Ll_sc_Meditron.png'],\n",
       " ['174_1p2_Ar_mc_AKGC417L.png'],\n",
       " ['153_1b1_Al_sc_Meditron.png'],\n",
       " ['101_1b1_Al_sc_Meditron.png'],\n",
       " ['130_3b4_Ar_mc_AKGC417L.png'],\n",
       " ['146_2b4_Al_mc_AKGC417L.png'],\n",
       " ['130_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['120_1b1_Ar_sc_Meditron.png'],\n",
       " ['177_1b2_Tc_mc_AKGC417L.png'],\n",
       " ['178_1b6_Tc_mc_AKGC417L.png'],\n",
       " ['207_3b2_Tc_mc_AKGC417L.png'],\n",
       " ['197_1b1_Al_sc_Meditron.png'],\n",
       " ['207_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['200_2p4_Pr_mc_AKGC417L.png'],\n",
       " ['174_1p2_Lr_mc_AKGC417L.png'],\n",
       " ['130_2b4_Al_mc_AKGC417L.png'],\n",
       " ['107_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['122_2b3_Al_mc_LittC2SE.png'],\n",
       " ['213_1p5_Ar_mc_AKGC417L.png'],\n",
       " ['138_1p4_Tc_mc_AKGC417L.png'],\n",
       " ['203_1p4_Pl_mc_AKGC417L.png'],\n",
       " ['177_1b4_Ar_mc_AKGC417L.png'],\n",
       " ['163_2b2_Lr_mc_AKGC417L.png'],\n",
       " ['130_1p4_Ll_mc_AKGC417L.png'],\n",
       " ['146_8p3_Lr_mc_AKGC417L.png'],\n",
       " ['122_2b2_Al_mc_LittC2SE.png'],\n",
       " ['151_2p4_Tc_mc_AKGC417L.png'],\n",
       " ['146_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['107_3p2_Ar_mc_AKGC417L.png'],\n",
       " ['206_1b1_Lr_sc_Meditron.png'],\n",
       " ['176_1b3_Ll_mc_AKGC417L.png'],\n",
       " ['158_1p2_Al_mc_AKGC417L.png'],\n",
       " ['213_1p5_Tc_mc_AKGC417L.png'],\n",
       " ['157_1b1_Lr_sc_Meditron.png'],\n",
       " ['170_1b2_Tc_mc_AKGC417L.png'],\n",
       " ['198_1b5_Tc_mc_AKGC417L.png'],\n",
       " ['172_1b4_Pr_mc_AKGC417L.png'],\n",
       " ['130_3p2_Pl_mc_AKGC417L.png'],\n",
       " ['161_1b1_Al_sc_Meditron.png'],\n",
       " ['172_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['213_1p2_Ar_mc_AKGC417L.png'],\n",
       " ['151_3p2_Pl_mc_AKGC417L.png'],\n",
       " ['204_7p5_Tc_mc_AKGC417L.png'],\n",
       " ['165_1b1_Pl_sc_Meditron.png'],\n",
       " ['170_1b4_Al_mc_AKGC417L.png'],\n",
       " ['211_1p2_Ar_mc_AKGC417L.png'],\n",
       " ['178_1b2_Tc_mc_AKGC417L.png'],\n",
       " ['139_1b1_Ll_sc_Litt3200.png'],\n",
       " ['151_2p2_Pl_mc_AKGC417L.png'],\n",
       " ['177_1b2_Ar_mc_AKGC417L.png'],\n",
       " ['130_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['160_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['174_1p3_Pr_mc_AKGC417L.png'],\n",
       " ['160_1b2_Ar_mc_AKGC417L.png'],\n",
       " ['201_1b2_Ar_sc_Meditron.png'],\n",
       " ['172_1b5_Al_mc_AKGC417L.png'],\n",
       " ['114_1b4_Pr_mc_AKGC417L.png'],\n",
       " ['161_1b1_Pl_sc_Meditron.png'],\n",
       " ['107_2b5_Pl_mc_AKGC417L.png'],\n",
       " ['200_2p4_Lr_mc_AKGC417L.png'],\n",
       " ['205_3b4_Pl_mc_AKGC417L.png'],\n",
       " ['195_1b1_Ar_sc_Litt3200.png'],\n",
       " ['130_1p2_Ll_mc_AKGC417L.png'],\n",
       " ['104_1b1_Pl_sc_Litt3200.png'],\n",
       " ['114_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['203_1p2_Al_mc_AKGC417L.png'],\n",
       " ['211_2p2_Tc_mc_AKGC417L.png'],\n",
       " ['223_1b1_Pl_sc_Meditron.png'],\n",
       " ['218_1b1_Pr_sc_Meditron.png'],\n",
       " ['158_1p2_Pr_mc_AKGC417L.png'],\n",
       " ['158_1b3_Ar_mc_LittC2SE.png'],\n",
       " ['172_1b5_Lr_mc_AKGC417L.png'],\n",
       " ['151_2p2_Lr_mc_AKGC417L.png'],\n",
       " ['163_2b2_Tc_mc_AKGC417L.png'],\n",
       " ['114_1b4_Ar_mc_AKGC417L.png'],\n",
       " ['176_1b3_Pl_mc_AKGC417L.png'],\n",
       " ['130_1p4_Tc_mc_AKGC417L.png'],\n",
       " ['193_7b3_Ar_mc_AKGC417L.png'],\n",
       " ['194_1b1_Pr_sc_Meditron.png'],\n",
       " ['186_2b2_Tc_mc_AKGC417L.png'],\n",
       " ['172_1b5_Tc_mc_AKGC417L.png'],\n",
       " ['211_2p3_Tc_mc_AKGC417L.png'],\n",
       " ['167_1b1_Pr_sc_Meditron.png'],\n",
       " ['174_2p3_Ar_mc_AKGC417L.png'],\n",
       " ['156_5b3_Al_mc_AKGC417L.png'],\n",
       " ['130_1p3_Ll_mc_AKGC417L.png'],\n",
       " ['205_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['146_8p3_Pr_mc_AKGC417L.png'],\n",
       " ['124_1b1_Al_sc_Litt3200.png'],\n",
       " ['162_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['119_1b1_Ar_sc_Meditron.png'],\n",
       " ['121_1p1_Tc_sc_Meditron.png'],\n",
       " ['178_1b6_Lr_mc_AKGC417L.png'],\n",
       " ['174_1p4_Ll_mc_AKGC417L.png'],\n",
       " ['138_1p3_Pr_mc_AKGC417L.png'],\n",
       " ['170_1b3_Ll_mc_AKGC417L.png'],\n",
       " ['179_1b1_Al_sc_Meditron.png'],\n",
       " ['139_1b1_Pl_sc_Litt3200.png'],\n",
       " ['130_2p5_Ar_mc_AKGC417L.png'],\n",
       " ['186_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['154_1b3_Tc_mc_AKGC417L.png'],\n",
       " ['130_1p3_Lr_mc_AKGC417L.png'],\n",
       " ['180_1b4_Pl_mc_AKGC417L.png'],\n",
       " ['222_1b1_Pr_sc_Meditron.png'],\n",
       " ['212_2b2_Tc_mc_LittC2SE.png'],\n",
       " ['133_2p3_Al_mc_AKGC417L.png'],\n",
       " ['130_3b4_Lr_mc_AKGC417L.png'],\n",
       " ['195_1b1_Al_sc_Litt3200.png'],\n",
       " ['111_1b2_Tc_sc_Meditron.png'],\n",
       " ['200_2p4_Ar_mc_AKGC417L.png'],\n",
       " ['204_2b5_Ar_mc_AKGC417L.png'],\n",
       " ['186_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['220_1b2_Al_mc_LittC2SE.png'],\n",
       " ['151_3p2_Pr_mc_AKGC417L.png'],\n",
       " ['162_2b3_Tc_mc_AKGC417L.png'],\n",
       " ['118_1b1_Lr_sc_Litt3200.png'],\n",
       " ['219_2b1_Ar_mc_LittC2SE.png'],\n",
       " ['200_2p3_Ar_mc_AKGC417L.png'],\n",
       " ['221_2b1_Al_mc_LittC2SE.png'],\n",
       " ['177_1b4_Pl_mc_AKGC417L.png'],\n",
       " ['207_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['213_1p5_Pr_mc_AKGC417L.png'],\n",
       " ['137_1b1_Ar_sc_Meditron.png'],\n",
       " ['193_1b2_Ar_mc_AKGC417L.png'],\n",
       " ['130_1p3_Pl_mc_AKGC417L.png'],\n",
       " ['207_3b2_Pr_mc_AKGC417L.png'],\n",
       " ['107_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['203_1p3_Ar_mc_AKGC417L.png'],\n",
       " ['178_1b2_Pl_mc_AKGC417L.png'],\n",
       " ['107_3p2_Lr_mc_AKGC417L.png'],\n",
       " ['154_1b3_Al_mc_AKGC417L.png'],\n",
       " ['162_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['207_2b3_Al_mc_AKGC417L.png'],\n",
       " ['174_2p3_Pr_mc_AKGC417L.png'],\n",
       " ['172_1b3_Al_mc_AKGC417L.png'],\n",
       " ['166_1p1_Pr_sc_Meditron.png'],\n",
       " ['186_3b3_Al_mc_AKGC417L.png'],\n",
       " ['154_1b3_Pl_mc_AKGC417L.png'],\n",
       " ['107_3p2_Al_mc_AKGC417L.png'],\n",
       " ['191_2b1_Pl_mc_LittC2SE.png'],\n",
       " ['180_1b4_Ar_mc_AKGC417L.png'],\n",
       " ['213_1p5_Al_mc_AKGC417L.png'],\n",
       " ['176_2b3_Ll_mc_AKGC417L.png'],\n",
       " ['130_2b2_Al_mc_AKGC417L.png'],\n",
       " ['138_1p4_Lr_mc_AKGC417L.png'],\n",
       " ['107_2b5_Al_mc_AKGC417L.png'],\n",
       " ['151_2p2_Ar_mc_AKGC417L.png'],\n",
       " ['163_8b3_Al_mc_AKGC417L.png'],\n",
       " ['163_8b3_Pl_mc_AKGC417L.png'],\n",
       " ['145_2b2_Al_mc_AKGC417L.png'],\n",
       " ['121_1b1_Tc_sc_Meditron.png'],\n",
       " ['118_1b1_Ar_sc_Litt3200.png'],\n",
       " ['130_1p3_Al_mc_AKGC417L.png'],\n",
       " ['112_1p1_Pl_sc_Litt3200.png'],\n",
       " ['138_1p2_Pl_mc_AKGC417L.png'],\n",
       " ['152_1b1_Al_sc_Meditron.png'],\n",
       " ['201_1b1_Al_sc_Meditron.png'],\n",
       " ['219_2b1_Tc_mc_LittC2SE.png'],\n",
       " ['147_2b3_Al_mc_AKGC417L.png'],\n",
       " ['130_2b3_Pr_mc_AKGC417L.png'],\n",
       " ['146_8p3_Pl_mc_AKGC417L.png'],\n",
       " ['221_2b1_Lr_mc_LittC2SE.png'],\n",
       " ['170_2b2_Tc_mc_AKGC417L.png'],\n",
       " ['162_2b3_Al_mc_AKGC417L.png'],\n",
       " ['156_5b3_Ar_mc_AKGC417L.png'],\n",
       " ['154_1b3_Ll_mc_AKGC417L.png'],\n",
       " ['158_1p2_Ar_mc_AKGC417L.png'],\n",
       " ['205_4b2_Lr_mc_AKGC417L.png'],\n",
       " ['201_1b2_Al_sc_Meditron.png'],\n",
       " ['203_1p2_Lr_mc_AKGC417L.png'],\n",
       " ['162_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['174_1p2_Pl_mc_AKGC417L.png'],\n",
       " ['158_1p4_Pr_mc_AKGC417L.png'],\n",
       " ['171_1b1_Al_sc_Meditron.png'],\n",
       " ['186_2b4_Tc_mc_AKGC417L.png'],\n",
       " ['118_1b1_Ll_sc_Litt3200.png'],\n",
       " ['159_1b1_Al_sc_Meditron.png'],\n",
       " ['188_1b1_Pl_sc_Meditron.png'],\n",
       " ['163_8b3_Pr_mc_AKGC417L.png'],\n",
       " ['151_2p3_Lr_mc_AKGC417L.png'],\n",
       " ['133_2p3_Ar_mc_AKGC417L.png'],\n",
       " ['226_1b1_Pl_sc_LittC2SE.png'],\n",
       " ['133_2p2_Ar_mc_AKGC417L.png'],\n",
       " ['188_1b1_Al_sc_Meditron.png'],\n",
       " ['213_1p3_Al_mc_AKGC417L.png'],\n",
       " ['110_1b1_Pr_sc_Meditron.png'],\n",
       " ['175_1b1_Pl_sc_Litt3200.png'],\n",
       " ['213_1p2_Lr_mc_AKGC417L.png'],\n",
       " ['145_3b2_Lr_mc_AKGC417L.png'],\n",
       " ['193_1b2_Pl_mc_AKGC417L.png'],\n",
       " ['213_1p3_Pl_mc_AKGC417L.png'],\n",
       " ['109_1b1_Pl_sc_Litt3200.png'],\n",
       " ['186_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['186_3b3_Lr_mc_AKGC417L.png'],\n",
       " ['130_3b3_Ll_mc_AKGC417L.png'],\n",
       " ['207_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['154_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['190_1b1_Tc_sc_Meditron.png'],\n",
       " ['168_1b1_Al_sc_Meditron.png'],\n",
       " ['158_1p2_Pl_mc_AKGC417L.png'],\n",
       " ['138_1p2_Ar_mc_AKGC417L.png'],\n",
       " ['145_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['203_1p3_Pl_mc_AKGC417L.png'],\n",
       " ['107_2b4_Tc_mc_AKGC417L.png'],\n",
       " ['130_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['193_7b3_Tc_mc_AKGC417L.png'],\n",
       " ['207_3b2_Al_mc_AKGC417L.png'],\n",
       " ['151_2p4_Ar_mc_AKGC417L.png'],\n",
       " ['109_1b1_Pr_sc_Litt3200.png'],\n",
       " ['130_2b3_Ll_mc_AKGC417L.png'],\n",
       " ['154_4b4_Pr_mc_AKGC417L.png'],\n",
       " ['139_1b1_Lr_sc_Litt3200.png'],\n",
       " ['160_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['151_3p3_Ll_mc_AKGC417L.png'],\n",
       " ['145_3b2_Ar_mc_AKGC417L.png'],\n",
       " ['170_1b4_Pr_mc_AKGC417L.png'],\n",
       " ['104_1b1_Pr_sc_Litt3200.png'],\n",
       " ['203_1p2_Tc_mc_AKGC417L.png'],\n",
       " ['162_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['134_2b2_Ar_mc_LittC2SE.png'],\n",
       " ['172_1b4_Ar_mc_AKGC417L.png'],\n",
       " ['186_2b3_Pr_mc_AKGC417L.png'],\n",
       " ['106_2b1_Pr_mc_LittC2SE.png'],\n",
       " ['206_1b1_Pl_sc_Meditron.png'],\n",
       " ['154_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['193_1b2_Ll_mc_AKGC417L.png'],\n",
       " ['102_1b1_Ar_sc_Meditron.png'],\n",
       " ['113_1b1_Ll_sc_Litt3200.png'],\n",
       " ['156_8b3_Al_mc_AKGC417L.png'],\n",
       " ['125_1b1_Tc_sc_Meditron.png'],\n",
       " ['200_2p4_Tc_mc_AKGC417L.png'],\n",
       " ['109_1b1_Ar_sc_Litt3200.png'],\n",
       " ['138_1p3_Al_mc_AKGC417L.png'],\n",
       " ['178_1b6_Ar_mc_AKGC417L.png'],\n",
       " ['147_2b4_Al_mc_AKGC417L.png'],\n",
       " ['156_2b3_Al_mc_AKGC417L.png'],\n",
       " ['163_8b3_Ll_mc_AKGC417L.png'],\n",
       " ['158_2p2_Ar_mc_AKGC417L.png'],\n",
       " ['151_2p4_Al_mc_AKGC417L.png'],\n",
       " ['210_1b1_Al_sc_Meditron.png'],\n",
       " ['107_2b3_Tc_mc_AKGC417L.png'],\n",
       " ['186_2b4_Al_mc_AKGC417L.png'],\n",
       " ['177_2b4_Al_mc_AKGC417L.png'],\n",
       " ['142_1b1_Pl_mc_LittC2SE.png'],\n",
       " ['111_1b3_Tc_sc_Meditron.png'],\n",
       " ['154_3b3_Al_mc_AKGC417L.png'],\n",
       " ['133_2p2_Al_mc_AKGC417L.png'],\n",
       " ['192_2b1_Ar_mc_LittC2SE.png'],\n",
       " ['177_1b4_Pr_mc_AKGC417L.png'],\n",
       " ['154_1b3_Pr_mc_AKGC417L.png'],\n",
       " ['170_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['157_1b1_Pr_sc_Meditron.png'],\n",
       " ['216_1b1_Pl_sc_Meditron.png'],\n",
       " ['193_7b3_Pl_mc_AKGC417L.png'],\n",
       " ['172_1b5_Pl_mc_AKGC417L.png'],\n",
       " ['174_2p3_Pl_mc_AKGC417L.png'],\n",
       " ['183_1b1_Tc_sc_Meditron.png'],\n",
       " ['179_1b1_Tc_sc_Meditron.png'],\n",
       " ['170_2b2_Al_mc_AKGC417L.png'],\n",
       " ['130_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['203_1p2_Ar_mc_AKGC417L.png'],\n",
       " ['128_1b3_Tc_mc_LittC2SE.png'],\n",
       " ['145_2b2_Lr_mc_AKGC417L.png'],\n",
       " ['165_1b1_Pr_sc_Meditron.png'],\n",
       " ['118_1b1_Pl_sc_Litt3200.png'],\n",
       " ['170_1b3_Tc_mc_AKGC417L.png'],\n",
       " ['130_1p2_Al_mc_AKGC417L.png'],\n",
       " ['147_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['205_1b3_Pl_mc_AKGC417L.png'],\n",
       " ['219_2b2_Ar_mc_LittC2SE.png'],\n",
       " ['154_4b4_Ll_mc_AKGC417L.png'],\n",
       " ['154_2b4_Ll_mc_AKGC417L.png'],\n",
       " ['172_1b5_Ar_mc_AKGC417L.png'],\n",
       " ['185_1b1_Ll_sc_Litt3200.png'],\n",
       " ['176_1b4_Ll_mc_AKGC417L.png'],\n",
       " ['182_1b1_Tc_sc_Meditron.png'],\n",
       " ['156_2b3_Ll_mc_AKGC417L.png'],\n",
       " ['122_2b3_Ar_mc_LittC2SE.png'],\n",
       " ['175_1b1_Ll_sc_Litt3200.png'],\n",
       " ['170_1b4_Pl_mc_AKGC417L.png'],\n",
       " ['160_1b2_Pr_mc_AKGC417L.png'],\n",
       " ['176_1b3_Al_mc_AKGC417L.png'],\n",
       " ['147_2b4_Ll_mc_AKGC417L.png'],\n",
       " ['149_1b1_Pl_sc_Meditron.png'],\n",
       " ['178_1b2_Pr_mc_AKGC417L.png'],\n",
       " ['170_1b3_Pl_mc_AKGC417L.png'],\n",
       " ['174_1p3_Tc_mc_AKGC417L.png'],\n",
       " ['158_1p3_Al_mc_AKGC417L.png'],\n",
       " ['186_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['130_3p4_Al_mc_AKGC417L.png'],\n",
       " ['130_3p2_Pr_mc_AKGC417L.png'],\n",
       " ['133_2p3_Tc_mc_AKGC417L.png'],\n",
       " ['133_3p2_Pl_mc_AKGC417L.png'],\n",
       " ['207_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['151_2p3_Pr_mc_AKGC417L.png'],\n",
       " ['154_2b4_Al_mc_AKGC417L.png'],\n",
       " ['116_1b2_Pl_sc_Meditron.png'],\n",
       " ['210_1b1_Ar_sc_Meditron.png'],\n",
       " ['137_1b1_Ll_sc_Meditron.png'],\n",
       " ['177_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['151_3p2_Al_mc_AKGC417L.png'],\n",
       " ['131_1b1_Al_sc_Meditron.png'],\n",
       " ['151_2p2_Al_mc_AKGC417L.png'],\n",
       " ['226_1b1_Ll_sc_Meditron.png'],\n",
       " ['178_1b3_Ar_mc_AKGC417L.png'],\n",
       " ['219_2b3_Tc_mc_LittC2SE.png'],\n",
       " ['186_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['157_1b1_Al_sc_Meditron.png'],\n",
       " ['203_2p3_Pr_mc_AKGC417L.png'],\n",
       " ['213_2p2_Tc_mc_AKGC417L.png'],\n",
       " ['218_1b1_Pl_sc_Meditron.png'],\n",
       " ['151_2p3_Pl_mc_AKGC417L.png'],\n",
       " ['172_1b3_Ll_mc_AKGC417L.png'],\n",
       " ['132_2b2_Lr_mc_LittC2SE.png'],\n",
       " ['218_1b1_Al_sc_Meditron.png'],\n",
       " ['180_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['185_1b1_Pl_sc_Litt3200.png'],\n",
       " ['138_1p4_Pr_mc_AKGC417L.png'],\n",
       " ['110_1p1_Al_sc_Meditron.png'],\n",
       " ['170_1b3_Pr_mc_AKGC417L.png'],\n",
       " ['221_2b3_Al_mc_LittC2SE.png'],\n",
       " ['130_3p3_Pl_mc_AKGC417L.png'],\n",
       " ['214_1b1_Ar_sc_Meditron.png'],\n",
       " ['130_1p2_Ar_mc_AKGC417L.png'],\n",
       " ['107_2b3_Pr_mc_AKGC417L.png'],\n",
       " ['186_3b3_Tc_mc_AKGC417L.png'],\n",
       " ['127_1b1_Ar_sc_Meditron.png'],\n",
       " ['120_1b1_Pr_sc_Meditron.png'],\n",
       " ['107_2b4_Al_mc_AKGC417L.png'],\n",
       " ['162_2b4_Tc_mc_AKGC417L.png'],\n",
       " ['107_2b4_Ll_mc_AKGC417L.png'],\n",
       " ['107_2b3_Ll_mc_AKGC417L.png'],\n",
       " ['218_1p1_Ar_sc_Litt3200.png'],\n",
       " ['138_2p2_Lr_mc_AKGC417L.png'],\n",
       " ['178_1b3_Pr_mc_AKGC417L.png'],\n",
       " ['130_3b4_Al_mc_AKGC417L.png'],\n",
       " ['112_1b1_Ar_sc_Meditron.png'],\n",
       " ['160_1b2_Lr_mc_AKGC417L.png'],\n",
       " ['195_1b1_Pl_sc_Litt3200.png'],\n",
       " ['177_1b4_Al_mc_AKGC417L.png'],\n",
       " ['115_1b1_Ar_sc_Meditron.png'],\n",
       " ['204_2b5_Al_mc_AKGC417L.png'],\n",
       " ['186_2b3_Tc_mc_AKGC417L.png'],\n",
       " ['183_1b1_Pl_sc_Meditron.png'],\n",
       " ['147_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['200_3p4_Tc_mc_AKGC417L.png'],\n",
       " ['203_1p3_Al_mc_AKGC417L.png'],\n",
       " ['104_1b1_Ll_sc_Litt3200.png'],\n",
       " ['110_1p1_Lr_sc_Meditron.png'],\n",
       " ['172_1b4_Tc_mc_AKGC417L.png'],\n",
       " ['170_1b2_Al_mc_AKGC417L.png'],\n",
       " ['157_1b1_Pl_sc_Meditron.png'],\n",
       " ['172_2b5_Pl_mc_AKGC417L.png'],\n",
       " ['163_8b3_Lr_mc_AKGC417L.png'],\n",
       " ['174_1p3_Ar_mc_AKGC417L.png'],\n",
       " ['177_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['120_1b1_Pl_sc_Meditron.png'],\n",
       " ['193_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['186_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['107_2b5_Pr_mc_AKGC417L.png'],\n",
       " ['175_1b1_Al_sc_Litt3200.png'],\n",
       " ['140_2b3_Tc_mc_LittC2SE.png'],\n",
       " ['200_2p2_Pl_mc_AKGC417L.png'],\n",
       " ['162_2b3_Ar_mc_AKGC417L.png'],\n",
       " ['158_1p3_Ar_mc_AKGC417L.png'],\n",
       " ['205_3b4_Al_mc_AKGC417L.png'],\n",
       " ['130_2b4_Ll_mc_AKGC417L.png'],\n",
       " ['130_3p3_Tc_mc_AKGC417L.png'],\n",
       " ['130_3p2_Tc_mc_AKGC417L.png'],\n",
       " ['203_1p3_Pr_mc_AKGC417L.png'],\n",
       " ['172_1b5_Pr_mc_AKGC417L.png'],\n",
       " ['130_1p2_Pl_mc_AKGC417L.png'],\n",
       " ['151_2p2_Tc_mc_AKGC417L.png'],\n",
       " ['193_7b3_Al_mc_AKGC417L.png'],\n",
       " ['207_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['118_1b1_Pr_sc_Litt3200.png'],\n",
       " ['107_2b3_Al_mc_AKGC417L.png'],\n",
       " ['170_1b3_Ar_mc_AKGC417L.png'],\n",
       " ['174_1p4_Pl_mc_AKGC417L.png'],\n",
       " ['178_1b6_Pl_mc_AKGC417L.png'],\n",
       " ['223_1b1_Lr_sc_Meditron.png'],\n",
       " ['138_2p2_Ar_mc_AKGC417L.png'],\n",
       " ['130_1p4_Pr_mc_AKGC417L.png'],\n",
       " ['186_2b3_Al_mc_AKGC417L.png'],\n",
       " ['178_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['172_1b4_Al_mc_AKGC417L.png'],\n",
       " ['223_1b1_Ll_sc_Meditron.png'],\n",
       " ['186_3b3_Pr_mc_AKGC417L.png'],\n",
       " ['170_1b2_Pr_mc_AKGC417L.png'],\n",
       " ['221_2b2_Ar_mc_LittC2SE.png'],\n",
       " ['135_2b1_Pl_mc_LittC2SE.png'],\n",
       " ['211_1p5_Ar_mc_AKGC417L.png'],\n",
       " ['221_2b1_Ar_mc_LittC2SE.png'],\n",
       " ['162_2b2_Ar_mc_AKGC417L.png'],\n",
       " ['204_7p5_Al_mc_AKGC417L.png'],\n",
       " ['107_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['135_2b3_Pl_mc_LittC2SE.png'],\n",
       " ['147_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['223_1b1_Pr_sc_Meditron.png'],\n",
       " ['209_1b1_Tc_sc_Meditron.png'],\n",
       " ['224_1b2_Al_sc_Meditron.png'],\n",
       " ['203_1p4_Pr_mc_AKGC417L.png'],\n",
       " ['163_2b2_Al_mc_AKGC417L.png'],\n",
       " ['169_1b1_Lr_sc_Meditron.png'],\n",
       " ['160_1b3_Tc_mc_AKGC417L.png'],\n",
       " ['178_2b2_Al_mc_AKGC417L.png'],\n",
       " ['207_2b3_Pl_mc_AKGC417L.png'],\n",
       " ['218_1b1_Lr_sc_Meditron.png'],\n",
       " ['154_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['207_3b2_Pl_mc_AKGC417L.png'],\n",
       " ['135_2b3_Ar_mc_LittC2SE.png'],\n",
       " ['195_1b1_Lr_sc_Litt3200.png'],\n",
       " ['138_1p3_Ar_mc_AKGC417L.png'],\n",
       " ['200_2p3_Al_mc_AKGC417L.png'],\n",
       " ['149_1b1_Lr_sc_Meditron.png'],\n",
       " ['172_1b3_Pl_mc_AKGC417L.png'],\n",
       " ['159_1b1_Ar_sc_Meditron.png'],\n",
       " ['130_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['105_1b1_Tc_sc_Meditron.png'],\n",
       " ['189_1b2_Lr_mc_LittC2SE.png'],\n",
       " ['193_1b2_Tc_mc_AKGC417L.png'],\n",
       " ['160_1b2_Tc_mc_AKGC417L.png'],\n",
       " ['156_8b3_Ar_mc_AKGC417L.png'],\n",
       " ['130_1p2_Lr_mc_AKGC417L.png'],\n",
       " ['174_1p4_Pr_mc_AKGC417L.png'],\n",
       " ['180_1b4_Pr_mc_AKGC417L.png'],\n",
       " ['160_1b4_Pl_mc_AKGC417L.png'],\n",
       " ['110_1p1_Pr_sc_Meditron.png'],\n",
       " ['185_1b1_Al_sc_Litt3200.png'],\n",
       " ['194_1b1_Lr_sc_Meditron.png'],\n",
       " ['163_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['198_1b5_Al_mc_AKGC417L.png'],\n",
       " ['200_2p2_Lr_mc_AKGC417L.png'],\n",
       " ['203_1p2_Pl_mc_AKGC417L.png'],\n",
       " ['186_2b2_Pr_mc_AKGC417L.png'],\n",
       " ['151_2p4_Pl_mc_AKGC417L.png'],\n",
       " ['154_4b4_Lr_mc_AKGC417L.png'],\n",
       " ['177_1b4_Tc_mc_AKGC417L.png'],\n",
       " ['205_4b2_Ar_mc_AKGC417L.png'],\n",
       " ['172_2b5_Pr_mc_AKGC417L.png'],\n",
       " ['176_1b4_Ar_mc_AKGC417L.png'],\n",
       " ['163_8b3_Ar_mc_AKGC417L.png'],\n",
       " ['223_1b1_Al_sc_Meditron.png'],\n",
       " ['205_2b3_Al_mc_AKGC417L.png'],\n",
       " ['221_2b3_Ar_mc_LittC2SE.png'],\n",
       " ['198_6p1_Pr_mc_AKGC417L.png'],\n",
       " ['221_2b1_Pl_mc_LittC2SE.png'],\n",
       " ['134_2b1_Ar_mc_LittC2SE.png'],\n",
       " ['186_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['160_1b4_Lr_mc_AKGC417L.png'],\n",
       " ['146_8p3_Al_mc_AKGC417L.png'],\n",
       " ['139_1b1_Pr_sc_Litt3200.png'],\n",
       " ['155_2b1_Al_mc_LittC2SE.png'],\n",
       " ['154_1b3_Ar_mc_AKGC417L.png'],\n",
       " ['156_5b3_Ll_mc_AKGC417L.png'],\n",
       " ['113_1b1_Lr_sc_Litt3200.png'],\n",
       " ['124_1b1_Pl_sc_Litt3200.png'],\n",
       " ['177_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['104_1b1_Lr_sc_Litt3200.png'],\n",
       " ['162_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['160_1b3_Ar_mc_AKGC417L.png'],\n",
       " ['226_1b1_Al_sc_Meditron.png'],\n",
       " ['207_3b2_Lr_mc_AKGC417L.png'],\n",
       " ['130_2b2_Tc_mc_AKGC417L.png'],\n",
       " ['192_2b2_Ar_mc_LittC2SE.png'],\n",
       " ['200_3p4_Pr_mc_AKGC417L.png'],\n",
       " ['156_2b3_Lr_mc_AKGC417L.png'],\n",
       " ['133_2p2_Pl_mc_AKGC417L.png'],\n",
       " ['178_1b2_Al_mc_AKGC417L.png'],\n",
       " ['177_2b4_Tc_mc_AKGC417L.png'],\n",
       " ['223_1b1_Ar_sc_Meditron.png'],\n",
       " ['135_2b1_Ar_mc_LittC2SE.png'],\n",
       " ['138_1p4_Pl_mc_AKGC417L.png'],\n",
       " ['178_1b6_Al_mc_AKGC417L.png'],\n",
       " ['130_2b2_Lr_mc_AKGC417L.png'],\n",
       " ['158_1p3_Lr_mc_AKGC417L.png'],\n",
       " ['138_1p2_Tc_mc_AKGC417L.png'],\n",
       " ['130_1p4_Ar_mc_AKGC417L.png'],\n",
       " ['141_1b2_Lr_mc_LittC2SE.png'],\n",
       " ['146_2b4_Ar_mc_AKGC417L.png'],\n",
       " ['177_2b4_Pr_mc_AKGC417L.png'],\n",
       " ['198_1b5_Pl_mc_AKGC417L.png'],\n",
       " ['175_1b1_Ar_sc_Litt3200.png'],\n",
       " ['134_2b1_Al_mc_LittC2SE.png'],\n",
       " ['114_1b4_Pl_mc_AKGC417L.png'],\n",
       " ['170_1b4_Ar_mc_AKGC417L.png'],\n",
       " ['139_1b1_Ar_sc_Litt3200.png'],\n",
       " ['158_2p3_Tc_mc_AKGC417L.png'],\n",
       " ['185_1b1_Lr_sc_Litt3200.png'],\n",
       " ['165_1b1_Ar_sc_Meditron.png'],\n",
       " ['174_1p2_Ll_mc_AKGC417L.png'],\n",
       " ['203_1p4_Ar_mc_AKGC417L.png'],\n",
       " ['112_1b1_Lr_sc_Meditron.png'],\n",
       " ['158_2p3_Lr_mc_AKGC417L.png'],\n",
       " ['130_3p4_Pr_mc_AKGC417L.png'],\n",
       " ['151_2p2_Pr_mc_AKGC417L.png'],\n",
       " ['146_2b2_Pl_mc_AKGC417L.png'],\n",
       " ['158_1p2_Tc_mc_AKGC417L.png'],\n",
       " ['181_1b1_Ar_mc_LittC2SE.png'],\n",
       " ['132_2b1_Lr_mc_LittC2SE.png'],\n",
       " ['130_1p3_Pr_mc_AKGC417L.png'],\n",
       " ['130_2b4_Pl_mc_AKGC417L.png'],\n",
       " ['160_1b4_Ar_mc_AKGC417L.png'],\n",
       " ['138_2p2_Pl_mc_AKGC417L.png'],\n",
       " ['162_1b2_Pr_mc_AKGC417L.png'],\n",
       " ['138_2p2_Pr_mc_AKGC417L.png'],\n",
       " ['138_1p2_Al_mc_AKGC417L.png'],\n",
       " ['146_2b4_Lr_mc_AKGC417L.png'],\n",
       " ['109_1b1_Ll_sc_Litt3200.png'],\n",
       " ['158_1p3_Tc_mc_AKGC417L.png'],\n",
       " ['186_2b2_Al_mc_AKGC417L.png'],\n",
       " ['173_1b1_Al_sc_Meditron.png'],\n",
       " ['181_1b1_Tc_mc_LittC2SE.png'],\n",
       " ['124_1b1_Pr_sc_Litt3200.png'],\n",
       " ['174_1p3_Lr_mc_AKGC417L.png'],\n",
       " ['156_5b3_Lr_mc_AKGC417L.png'],\n",
       " ['185_1b1_Pr_sc_Litt3200.png'],\n",
       " ['134_2b2_Al_mc_LittC2SE.png'],\n",
       " ['172_1b3_Ar_mc_AKGC417L.png'],\n",
       " ['174_1p4_Lr_mc_AKGC417L.png'],\n",
       " ['175_1b1_Lr_sc_Litt3200.png'],\n",
       " ['107_2b5_Ar_mc_AKGC417L.png'],\n",
       " ['174_1p2_Tc_mc_AKGC417L.png'],\n",
       " ['211_1p3_Ar_mc_AKGC417L.png'],\n",
       " ['166_1p1_Al_sc_Meditron.png'],\n",
       " ['186_2b2_Lr_mc_AKGC417L.png'],\n",
       " ['184_1b1_Ar_sc_Meditron.png'],\n",
       " ['203_1p2_Pr_mc_AKGC417L.png'],\n",
       " ['160_1b2_Pl_mc_AKGC417L.png'],\n",
       " ['213_2p2_Al_mc_AKGC417L.png']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_file_names = []\n",
    "\n",
    "for i in sound_files:\n",
    "      req_file_names.append([i])\n",
    "\n",
    "req_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.833893Z",
     "iopub.status.busy": "2023-08-17T14:29:51.833386Z",
     "iopub.status.idle": "2023-08-17T14:29:51.839728Z",
     "shell.execute_reply": "2023-08-17T14:29:51.838658Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.833864Z"
    },
    "id": "LCKsXMpUvggI"
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(req_file_names)):\n",
    "    req_file_names[i].append(sr_no[req_file_names[i][0][:3]])\n",
    "    labels.append(sr_no[req_file_names[i][0][:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.841062Z",
     "iopub.status.busy": "2023-08-17T14:29:51.840771Z",
     "iopub.status.idle": "2023-08-17T14:29:51.852661Z",
     "shell.execute_reply": "2023-08-17T14:29:51.851645Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.841036Z"
    }
   },
   "outputs": [],
   "source": [
    "labels *= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:29:51.855429Z",
     "iopub.status.busy": "2023-08-17T14:29:51.855008Z",
     "iopub.status.idle": "2023-08-17T14:31:04.878638Z",
     "shell.execute_reply": "2023-08-17T14:31:04.877758Z",
     "shell.execute_reply.started": "2023-08-17T14:29:51.855406Z"
    },
    "id": "_PoMb8etvpAk",
    "outputId": "65cd04d4-ba35-460e-b497-d9b5ac091122"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "\n",
    "for i in req_file_names:\n",
    "    img = cv2.imread('/kaggle/input/attention-cnn-model/Mel Spectrogram/Mel Spectrogram/Time Stretch/'+i[0])\n",
    "    img = cv2.resize(img, (350, 350))\n",
    "    x.append(img)\n",
    "\n",
    "for i in req_file_names:\n",
    "    img = cv2.imread('/kaggle/input/attention-cnn-model/Mel Spectrogram/Mel Spectrogram/Pitch Shift/'+i[0])\n",
    "    img = cv2.resize(img, (350, 350))\n",
    "    x.append(img)\n",
    "    \n",
    "for i in req_file_names:\n",
    "    img = cv2.imread('/kaggle/input/attention-cnn-model/Mel Spectrogram/Mel Spectrogram/Audio Shift/'+i[0])\n",
    "    img = cv2.resize(img, (350, 350))\n",
    "    x.append(img)\n",
    "\n",
    "# x = np.array(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:31:04.881145Z",
     "iopub.status.busy": "2023-08-17T14:31:04.880807Z",
     "iopub.status.idle": "2023-08-17T14:31:04.887427Z",
     "shell.execute_reply": "2023-08-17T14:31:04.886367Z",
     "shell.execute_reply.started": "2023-08-17T14:31:04.881122Z"
    }
   },
   "outputs": [],
   "source": [
    "x_new = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]=='Asthma' or labels[i]=='Bronchiolitis':\n",
    "        continue\n",
    "    x_new.append(x[i])\n",
    "    y.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:31:04.889162Z",
     "iopub.status.busy": "2023-08-17T14:31:04.888694Z",
     "iopub.status.idle": "2023-08-17T14:31:05.040987Z",
     "shell.execute_reply": "2023-08-17T14:31:05.040161Z",
     "shell.execute_reply.started": "2023-08-17T14:31:04.889133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2718, 350, 350, 3)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x_new)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:31:05.042813Z",
     "iopub.status.busy": "2023-08-17T14:31:05.041999Z",
     "iopub.status.idle": "2023-08-17T14:31:05.046437Z",
     "shell.execute_reply": "2023-08-17T14:31:05.045402Z",
     "shell.execute_reply.started": "2023-08-17T14:31:05.042789Z"
    }
   },
   "outputs": [],
   "source": [
    "req_file_names *= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-08-17T14:31:05.047555Z",
     "iopub.status.busy": "2023-08-17T14:31:05.047345Z",
     "iopub.status.idle": "2023-08-17T14:31:05.060724Z",
     "shell.execute_reply": "2023-08-17T14:31:05.059898Z",
     "shell.execute_reply.started": "2023-08-17T14:31:05.047536Z"
    },
    "id": "AbwY828LxkMZ",
    "outputId": "54c8937f-563d-41af-c159-3bd798c5cdf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2718,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:31:05.062791Z",
     "iopub.status.busy": "2023-08-17T14:31:05.062306Z",
     "iopub.status.idle": "2023-08-17T14:31:05.077398Z",
     "shell.execute_reply": "2023-08-17T14:31:05.076372Z",
     "shell.execute_reply.started": "2023-08-17T14:31:05.062762Z"
    },
    "id": "bTiogsRk3ujL"
   },
   "outputs": [],
   "source": [
    "one_hot_y = np.array(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-17T14:31:05.078859Z",
     "iopub.status.busy": "2023-08-17T14:31:05.078485Z",
     "iopub.status.idle": "2023-08-17T14:31:05.219594Z",
     "shell.execute_reply": "2023-08-17T14:31:05.218759Z",
     "shell.execute_reply.started": "2023-08-17T14:31:05.078836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2174, 350, 350, 3) (2174, 6) (544, 350, 350, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, one_hot_y, test_size=0.2, random_state=39, stratify=y)\n",
    "print(x_train.shape, y_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T09:33:07.283432Z",
     "iopub.status.busy": "2023-08-15T09:33:07.282871Z",
     "iopub.status.idle": "2023-08-15T09:33:11.132386Z",
     "shell.execute_reply": "2023-08-15T09:33:11.129210Z",
     "shell.execute_reply.started": "2023-08-15T09:33:07.283394Z"
    }
   },
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, (7,7), activation='relu', input_shape=(350, 350, 3)))\n",
    "model.add(keras.layers.MaxPool2D((3,3)))\n",
    "model.add(keras.layers.Conv2D(128, (5,5), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((3,3)))\n",
    "model.add(keras.layers.Conv2D(256, (3,3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:22:51.798458Z",
     "iopub.status.busy": "2023-08-16T11:22:51.798078Z",
     "iopub.status.idle": "2023-08-16T11:22:51.803723Z",
     "shell.execute_reply": "2023-08-16T11:22:51.802725Z",
     "shell.execute_reply.started": "2023-08-16T11:22:51.798422Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_cnn = keras.callbacks.ModelCheckpoint(\"/kaggle/working/CNN_model-{epoch:02d}.h5\", save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:22:51.806487Z",
     "iopub.status.busy": "2023-08-16T11:22:51.805409Z",
     "iopub.status.idle": "2023-08-16T11:22:54.779508Z",
     "shell.execute_reply": "2023-08-16T11:22:54.778769Z",
     "shell.execute_reply.started": "2023-08-16T11:22:51.806453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 350, 350, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 344, 344, 64  9472        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 172, 172, 64  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 168, 168, 12  204928      ['max_pooling2d[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 84, 84, 128)  0          ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 84, 84, 128)  0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 82, 82, 256)  295168      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 41, 41, 256)  0          ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 41, 41, 256)  0           ['max_pooling2d_2[0][0]',        \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 430336)       0           ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          55083136    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 128)         512         ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 6)            390         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,601,862\n",
      "Trainable params: 55,601,606\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = keras.layers.Input(shape=(350, 350, 3))\n",
    "conv1 = keras.layers.Conv2D(64, (7,7), activation='relu')(input1)\n",
    "mp1 = keras.layers.MaxPool2D((2,2))(conv1)\n",
    "conv2 = keras.layers.Conv2D(128, (5,5), activation='relu')(mp1)\n",
    "mp2 = keras.layers.MaxPool2D((2,2))(conv2)\n",
    "do1 = keras.layers.Dropout(0.2)(mp2)\n",
    "conv3 = keras.layers.Conv2D(256, (3,3), activation='relu')(do1)\n",
    "mp3 = keras.layers.MaxPool2D((2,2))(conv3)\n",
    "\n",
    "# random = np.random.random((mp3.shape[1:]))\n",
    "\n",
    "attention = keras.layers.Attention()([mp3, mp3])\n",
    "\n",
    "flatten = keras.layers.Flatten()(attention)\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "bn = keras.layers.BatchNormalization()(dense1)\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(bn)\n",
    "output = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "model = keras.Model(inputs=input1, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:22:54.780896Z",
     "iopub.status.busy": "2023-08-16T11:22:54.780527Z",
     "iopub.status.idle": "2023-08-16T11:22:54.810844Z",
     "shell.execute_reply": "2023-08-16T11:22:54.809908Z",
     "shell.execute_reply.started": "2023-08-16T11:22:54.780862Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-08-16T11:22:54.812814Z",
     "iopub.status.busy": "2023-08-16T11:22:54.812234Z",
     "iopub.status.idle": "2023-08-16T11:59:07.419018Z",
     "shell.execute_reply": "2023-08-16T11:59:07.418043Z",
     "shell.execute_reply.started": "2023-08-16T11:22:54.812781Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 11:22:57.582382: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 36s 340ms/step - loss: 2.4795 - accuracy: 0.1150 - val_loss: 1.2546 - val_accuracy: 0.7132\n",
      "Epoch 2/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 1.7493 - accuracy: 0.3712 - val_loss: 9.0239 - val_accuracy: 0.0386\n",
      "Epoch 3/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 1.3458 - accuracy: 0.5534 - val_loss: 3.3834 - val_accuracy: 0.0110\n",
      "Epoch 4/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 1.1325 - accuracy: 0.6886 - val_loss: 2.3341 - val_accuracy: 0.0919\n",
      "Epoch 5/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.9968 - accuracy: 0.7539 - val_loss: 1.8681 - val_accuracy: 0.1857\n",
      "Epoch 6/120\n",
      "68/68 [==============================] - 19s 287ms/step - loss: 0.9249 - accuracy: 0.7944 - val_loss: 0.8394 - val_accuracy: 0.7592\n",
      "Epoch 7/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.8654 - accuracy: 0.8063 - val_loss: 0.9946 - val_accuracy: 0.7224\n",
      "Epoch 8/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.8262 - accuracy: 0.8243 - val_loss: 0.8969 - val_accuracy: 0.7629\n",
      "Epoch 9/120\n",
      "68/68 [==============================] - 19s 286ms/step - loss: 0.7858 - accuracy: 0.8362 - val_loss: 0.7664 - val_accuracy: 0.7868\n",
      "Epoch 10/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.7614 - accuracy: 0.8464 - val_loss: 0.9338 - val_accuracy: 0.6434\n",
      "Epoch 11/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.7386 - accuracy: 0.8510 - val_loss: 0.9325 - val_accuracy: 0.7077\n",
      "Epoch 12/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.7114 - accuracy: 0.8537 - val_loss: 0.9454 - val_accuracy: 0.7096\n",
      "Epoch 13/120\n",
      "68/68 [==============================] - 19s 280ms/step - loss: 0.6964 - accuracy: 0.8514 - val_loss: 0.7561 - val_accuracy: 0.8051\n",
      "Epoch 14/120\n",
      "68/68 [==============================] - 20s 290ms/step - loss: 0.6682 - accuracy: 0.8556 - val_loss: 0.6049 - val_accuracy: 0.8346\n",
      "Epoch 15/120\n",
      "68/68 [==============================] - 20s 288ms/step - loss: 0.6576 - accuracy: 0.8556 - val_loss: 0.5791 - val_accuracy: 0.8585\n",
      "Epoch 16/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.6350 - accuracy: 0.8592 - val_loss: 0.6087 - val_accuracy: 0.8419\n",
      "Epoch 17/120\n",
      "68/68 [==============================] - 20s 290ms/step - loss: 0.6189 - accuracy: 0.8712 - val_loss: 0.5585 - val_accuracy: 0.8676\n",
      "Epoch 18/120\n",
      "68/68 [==============================] - 20s 290ms/step - loss: 0.5951 - accuracy: 0.8638 - val_loss: 0.5321 - val_accuracy: 0.8732\n",
      "Epoch 19/120\n",
      "68/68 [==============================] - 20s 294ms/step - loss: 0.5945 - accuracy: 0.8661 - val_loss: 0.5238 - val_accuracy: 0.8640\n",
      "Epoch 20/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.5742 - accuracy: 0.8694 - val_loss: 0.5463 - val_accuracy: 0.8438\n",
      "Epoch 21/120\n",
      "68/68 [==============================] - 20s 292ms/step - loss: 0.5599 - accuracy: 0.8657 - val_loss: 0.5185 - val_accuracy: 0.8695\n",
      "Epoch 22/120\n",
      "68/68 [==============================] - 19s 280ms/step - loss: 0.5513 - accuracy: 0.8753 - val_loss: 0.4941 - val_accuracy: 0.8732\n",
      "Epoch 23/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.5512 - accuracy: 0.8689 - val_loss: 0.5312 - val_accuracy: 0.8474\n",
      "Epoch 24/120\n",
      "68/68 [==============================] - 20s 294ms/step - loss: 0.5419 - accuracy: 0.8666 - val_loss: 0.4572 - val_accuracy: 0.8732\n",
      "Epoch 25/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.5156 - accuracy: 0.8781 - val_loss: 0.5113 - val_accuracy: 0.8603\n",
      "Epoch 26/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.5212 - accuracy: 0.8730 - val_loss: 0.5578 - val_accuracy: 0.8585\n",
      "Epoch 27/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.5277 - accuracy: 0.8698 - val_loss: 0.6977 - val_accuracy: 0.8162\n",
      "Epoch 28/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.5078 - accuracy: 0.8721 - val_loss: 0.4729 - val_accuracy: 0.8824\n",
      "Epoch 29/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.4816 - accuracy: 0.8763 - val_loss: 0.5106 - val_accuracy: 0.8750\n",
      "Epoch 30/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.4774 - accuracy: 0.8786 - val_loss: 0.5408 - val_accuracy: 0.8621\n",
      "Epoch 31/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.4677 - accuracy: 0.8781 - val_loss: 0.5305 - val_accuracy: 0.8566\n",
      "Epoch 32/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.4597 - accuracy: 0.8744 - val_loss: 0.5272 - val_accuracy: 0.8768\n",
      "Epoch 33/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.4654 - accuracy: 0.8786 - val_loss: 0.8541 - val_accuracy: 0.8088\n",
      "Epoch 34/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.4518 - accuracy: 0.8790 - val_loss: 0.4586 - val_accuracy: 0.8732\n",
      "Epoch 35/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.4438 - accuracy: 0.8799 - val_loss: 0.5110 - val_accuracy: 0.8750\n",
      "Epoch 36/120\n",
      "68/68 [==============================] - 20s 289ms/step - loss: 0.4430 - accuracy: 0.8795 - val_loss: 0.4496 - val_accuracy: 0.8897\n",
      "Epoch 37/120\n",
      "68/68 [==============================] - 19s 283ms/step - loss: 0.4441 - accuracy: 0.8832 - val_loss: 0.4413 - val_accuracy: 0.8787\n",
      "Epoch 38/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.4423 - accuracy: 0.8726 - val_loss: 0.5196 - val_accuracy: 0.8566\n",
      "Epoch 39/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.4197 - accuracy: 0.8887 - val_loss: 0.5307 - val_accuracy: 0.8768\n",
      "Epoch 40/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.4252 - accuracy: 0.8804 - val_loss: 0.5546 - val_accuracy: 0.8438\n",
      "Epoch 41/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.4244 - accuracy: 0.8832 - val_loss: 0.4619 - val_accuracy: 0.8676\n",
      "Epoch 42/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.4142 - accuracy: 0.8822 - val_loss: 0.4456 - val_accuracy: 0.8750\n",
      "Epoch 43/120\n",
      "68/68 [==============================] - 19s 280ms/step - loss: 0.4029 - accuracy: 0.8850 - val_loss: 0.4115 - val_accuracy: 0.8750\n",
      "Epoch 44/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.3901 - accuracy: 0.8887 - val_loss: 0.4209 - val_accuracy: 0.8824\n",
      "Epoch 45/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3951 - accuracy: 0.8818 - val_loss: 0.4348 - val_accuracy: 0.8842\n",
      "Epoch 46/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.3877 - accuracy: 0.8845 - val_loss: 0.6843 - val_accuracy: 0.8143\n",
      "Epoch 47/120\n",
      "68/68 [==============================] - 19s 281ms/step - loss: 0.3732 - accuracy: 0.8928 - val_loss: 0.4060 - val_accuracy: 0.8897\n",
      "Epoch 48/120\n",
      "68/68 [==============================] - 20s 291ms/step - loss: 0.3675 - accuracy: 0.8947 - val_loss: 0.4044 - val_accuracy: 0.8805\n",
      "Epoch 49/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.3675 - accuracy: 0.8919 - val_loss: 0.4046 - val_accuracy: 0.8713\n",
      "Epoch 50/120\n",
      "68/68 [==============================] - 19s 281ms/step - loss: 0.3599 - accuracy: 0.8947 - val_loss: 0.4021 - val_accuracy: 0.8842\n",
      "Epoch 51/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.3606 - accuracy: 0.8914 - val_loss: 0.4331 - val_accuracy: 0.8860\n",
      "Epoch 52/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3531 - accuracy: 0.8910 - val_loss: 0.4215 - val_accuracy: 0.8768\n",
      "Epoch 53/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3663 - accuracy: 0.8891 - val_loss: 0.4172 - val_accuracy: 0.8750\n",
      "Epoch 54/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3659 - accuracy: 0.8896 - val_loss: 0.4305 - val_accuracy: 0.8787\n",
      "Epoch 55/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.3470 - accuracy: 0.8859 - val_loss: 0.4575 - val_accuracy: 0.8768\n",
      "Epoch 56/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3465 - accuracy: 0.8951 - val_loss: 0.4093 - val_accuracy: 0.8768\n",
      "Epoch 57/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3313 - accuracy: 0.9034 - val_loss: 0.4414 - val_accuracy: 0.8768\n",
      "Epoch 58/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3321 - accuracy: 0.8919 - val_loss: 0.4274 - val_accuracy: 0.8787\n",
      "Epoch 59/120\n",
      "68/68 [==============================] - 19s 282ms/step - loss: 0.3357 - accuracy: 0.8919 - val_loss: 0.3672 - val_accuracy: 0.8805\n",
      "Epoch 60/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.3429 - accuracy: 0.8882 - val_loss: 0.4652 - val_accuracy: 0.8750\n",
      "Epoch 61/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3337 - accuracy: 0.8960 - val_loss: 0.3834 - val_accuracy: 0.8805\n",
      "Epoch 62/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3299 - accuracy: 0.8974 - val_loss: 0.4500 - val_accuracy: 0.8787\n",
      "Epoch 63/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.3301 - accuracy: 0.8942 - val_loss: 0.3899 - val_accuracy: 0.8768\n",
      "Epoch 64/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.3430 - accuracy: 0.8933 - val_loss: 0.8574 - val_accuracy: 0.7757\n",
      "Epoch 65/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3479 - accuracy: 0.8905 - val_loss: 0.5434 - val_accuracy: 0.8419\n",
      "Epoch 66/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3330 - accuracy: 0.8942 - val_loss: 0.4069 - val_accuracy: 0.8768\n",
      "Epoch 67/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3379 - accuracy: 0.8868 - val_loss: 0.4550 - val_accuracy: 0.8768\n",
      "Epoch 68/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3263 - accuracy: 0.8988 - val_loss: 0.5685 - val_accuracy: 0.8750\n",
      "Epoch 69/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3144 - accuracy: 0.9016 - val_loss: 0.5413 - val_accuracy: 0.8750\n",
      "Epoch 70/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3120 - accuracy: 0.8896 - val_loss: 0.5003 - val_accuracy: 0.8750\n",
      "Epoch 71/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3033 - accuracy: 0.9057 - val_loss: 0.4069 - val_accuracy: 0.8768\n",
      "Epoch 72/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.3301 - accuracy: 0.8905 - val_loss: 1.0771 - val_accuracy: 0.6783\n",
      "Epoch 73/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3297 - accuracy: 0.8924 - val_loss: 0.4834 - val_accuracy: 0.8713\n",
      "Epoch 74/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.3521 - accuracy: 0.8882 - val_loss: 0.4902 - val_accuracy: 0.8768\n",
      "Epoch 75/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.3489 - accuracy: 0.8878 - val_loss: 0.6638 - val_accuracy: 0.8750\n",
      "Epoch 76/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3222 - accuracy: 0.8919 - val_loss: 0.7869 - val_accuracy: 0.8750\n",
      "Epoch 77/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3045 - accuracy: 0.8993 - val_loss: 0.5757 - val_accuracy: 0.8750\n",
      "Epoch 78/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3114 - accuracy: 0.8956 - val_loss: 0.5500 - val_accuracy: 0.8732\n",
      "Epoch 79/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.3090 - accuracy: 0.8919 - val_loss: 0.4299 - val_accuracy: 0.8768\n",
      "Epoch 80/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.3053 - accuracy: 0.9002 - val_loss: 0.3833 - val_accuracy: 0.8805\n",
      "Epoch 81/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2981 - accuracy: 0.9071 - val_loss: 0.6377 - val_accuracy: 0.8676\n",
      "Epoch 82/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2963 - accuracy: 0.8974 - val_loss: 0.6579 - val_accuracy: 0.8750\n",
      "Epoch 83/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2956 - accuracy: 0.8974 - val_loss: 0.5705 - val_accuracy: 0.8750\n",
      "Epoch 84/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2896 - accuracy: 0.9006 - val_loss: 0.3811 - val_accuracy: 0.8805\n",
      "Epoch 85/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2824 - accuracy: 0.9034 - val_loss: 0.3985 - val_accuracy: 0.8787\n",
      "Epoch 86/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2780 - accuracy: 0.9029 - val_loss: 0.4756 - val_accuracy: 0.8787\n",
      "Epoch 87/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2802 - accuracy: 0.9071 - val_loss: 0.4988 - val_accuracy: 0.8750\n",
      "Epoch 88/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2717 - accuracy: 0.9103 - val_loss: 0.5956 - val_accuracy: 0.8750\n",
      "Epoch 89/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2709 - accuracy: 0.9108 - val_loss: 0.4509 - val_accuracy: 0.8824\n",
      "Epoch 90/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.2663 - accuracy: 0.9144 - val_loss: 0.4553 - val_accuracy: 0.8768\n",
      "Epoch 91/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2722 - accuracy: 0.9085 - val_loss: 0.3818 - val_accuracy: 0.8787\n",
      "Epoch 92/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.2645 - accuracy: 0.9098 - val_loss: 0.3808 - val_accuracy: 0.8805\n",
      "Epoch 93/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.2748 - accuracy: 0.9066 - val_loss: 0.3807 - val_accuracy: 0.8860\n",
      "Epoch 94/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.2588 - accuracy: 0.9131 - val_loss: 0.3845 - val_accuracy: 0.9007\n",
      "Epoch 95/120\n",
      "68/68 [==============================] - 19s 280ms/step - loss: 0.2523 - accuracy: 0.9121 - val_loss: 0.3519 - val_accuracy: 0.8915\n",
      "Epoch 96/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.2563 - accuracy: 0.9140 - val_loss: 0.4138 - val_accuracy: 0.8750\n",
      "Epoch 97/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2444 - accuracy: 0.9213 - val_loss: 0.4596 - val_accuracy: 0.8768\n",
      "Epoch 98/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2331 - accuracy: 0.9200 - val_loss: 0.4733 - val_accuracy: 0.8750\n",
      "Epoch 99/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2414 - accuracy: 0.9204 - val_loss: 0.5609 - val_accuracy: 0.8750\n",
      "Epoch 100/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2353 - accuracy: 0.9204 - val_loss: 0.5042 - val_accuracy: 0.8805\n",
      "Epoch 101/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.2587 - accuracy: 0.9117 - val_loss: 0.4741 - val_accuracy: 0.8750\n",
      "Epoch 102/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2556 - accuracy: 0.9112 - val_loss: 0.6186 - val_accuracy: 0.8824\n",
      "Epoch 103/120\n",
      "68/68 [==============================] - 19s 287ms/step - loss: 0.2502 - accuracy: 0.9112 - val_loss: 0.3447 - val_accuracy: 0.8750\n",
      "Epoch 104/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2467 - accuracy: 0.9186 - val_loss: 0.5305 - val_accuracy: 0.8732\n",
      "Epoch 105/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2427 - accuracy: 0.9204 - val_loss: 0.3548 - val_accuracy: 0.8768\n",
      "Epoch 106/120\n",
      "68/68 [==============================] - 19s 280ms/step - loss: 0.2468 - accuracy: 0.9163 - val_loss: 0.3162 - val_accuracy: 0.8768\n",
      "Epoch 107/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.2466 - accuracy: 0.9218 - val_loss: 0.4396 - val_accuracy: 0.8805\n",
      "Epoch 108/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2357 - accuracy: 0.9241 - val_loss: 0.4334 - val_accuracy: 0.8805\n",
      "Epoch 109/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.2369 - accuracy: 0.9246 - val_loss: 0.3267 - val_accuracy: 0.9026\n",
      "Epoch 110/120\n",
      "68/68 [==============================] - 17s 255ms/step - loss: 0.2468 - accuracy: 0.9190 - val_loss: 0.3743 - val_accuracy: 0.8787\n",
      "Epoch 111/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2733 - accuracy: 0.9039 - val_loss: 0.3713 - val_accuracy: 0.8989\n",
      "Epoch 112/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2721 - accuracy: 0.9029 - val_loss: 0.3501 - val_accuracy: 0.8713\n",
      "Epoch 113/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2297 - accuracy: 0.9255 - val_loss: 0.3404 - val_accuracy: 0.8787\n",
      "Epoch 114/120\n",
      "68/68 [==============================] - 19s 281ms/step - loss: 0.2300 - accuracy: 0.9287 - val_loss: 0.2963 - val_accuracy: 0.8952\n",
      "Epoch 115/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.2165 - accuracy: 0.9365 - val_loss: 0.4137 - val_accuracy: 0.8860\n",
      "Epoch 116/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.2224 - accuracy: 0.9273 - val_loss: 0.4656 - val_accuracy: 0.8805\n",
      "Epoch 117/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2172 - accuracy: 0.9278 - val_loss: 0.5317 - val_accuracy: 0.8787\n",
      "Epoch 118/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.2128 - accuracy: 0.9255 - val_loss: 0.3395 - val_accuracy: 0.8824\n",
      "Epoch 119/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2138 - accuracy: 0.9305 - val_loss: 0.3799 - val_accuracy: 0.8989\n",
      "Epoch 120/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.2126 - accuracy: 0.9333 - val_loss: 0.3262 - val_accuracy: 0.9026\n"
     ]
    }
   ],
   "source": [
    "history_cnn = model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:59:07.420928Z",
     "iopub.status.busy": "2023-08-16T11:59:07.420608Z",
     "iopub.status.idle": "2023-08-16T11:59:07.432495Z",
     "shell.execute_reply": "2023-08-16T11:59:07.431617Z",
     "shell.execute_reply.started": "2023-08-16T11:59:07.420903Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_cnn.history)\n",
    "df.to_csv('/kaggle/working/CNN_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:59:07.434534Z",
     "iopub.status.busy": "2023-08-16T11:59:07.433903Z",
     "iopub.status.idle": "2023-08-16T11:59:12.201984Z",
     "shell.execute_reply": "2023-08-16T11:59:12.200976Z",
     "shell.execute_reply.started": "2023-08-16T11:59:07.434501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83683744/83683744 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "xception_wo_top = keras.applications.xception.Xception(include_top=False, weights='imagenet', input_shape=(350, 350, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:59:12.203621Z",
     "iopub.status.busy": "2023-08-16T11:59:12.203256Z",
     "iopub.status.idle": "2023-08-16T11:59:12.214445Z",
     "shell.execute_reply": "2023-08-16T11:59:12.213421Z",
     "shell.execute_reply.started": "2023-08-16T11:59:12.203588Z"
    }
   },
   "outputs": [],
   "source": [
    "xception_wo_top.trainable = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T22:22:35.863120Z",
     "iopub.status.busy": "2023-08-08T22:22:35.862042Z",
     "iopub.status.idle": "2023-08-08T22:22:36.477488Z",
     "shell.execute_reply": "2023-08-08T22:22:36.476629Z",
     "shell.execute_reply.started": "2023-08-08T22:22:35.863076Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "xception_model = keras.models.Sequential()\n",
    "xception_model.add(xception_wo_top)\n",
    "xception_model.add(keras.layers.Flatten())\n",
    "xception_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "xception_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "xception_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:59:12.216338Z",
     "iopub.status.busy": "2023-08-16T11:59:12.215980Z",
     "iopub.status.idle": "2023-08-16T11:59:12.732758Z",
     "shell.execute_reply": "2023-08-16T11:59:12.731800Z",
     "shell.execute_reply.started": "2023-08-16T11:59:12.216306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 350, 350, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " xception (Functional)          (None, 11, 11, 2048  20861480    ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 11, 11, 2048  0           ['xception[0][0]',               \n",
      "                                )                                 'xception[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 247808)       0           ['attention_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          31719552    ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_3[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           8256        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 6)            390         ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 52,590,190\n",
      "Trainable params: 31,728,454\n",
      "Non-trainable params: 20,861,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_xception = keras.layers.Input(shape=(350, 350, 3))\n",
    "xception_layer = xception_wo_top(input_xception)\n",
    "\n",
    "# random = np.random.random(xception_layer.shape[1:])\n",
    "\n",
    "attention = keras.layers.Attention()([xception_layer, xception_layer])\n",
    "\n",
    "flatten = keras.layers.Flatten()(attention)\n",
    "\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "bn1 = keras.layers.BatchNormalization()(dense1)\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(bn1)\n",
    "output = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "xception_model = keras.Model(inputs=input_xception, outputs=output)\n",
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:59:12.734814Z",
     "iopub.status.busy": "2023-08-16T11:59:12.734443Z",
     "iopub.status.idle": "2023-08-16T11:59:12.741770Z",
     "shell.execute_reply": "2023-08-16T11:59:12.740650Z",
     "shell.execute_reply.started": "2023-08-16T11:59:12.734780Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_xception = keras.callbacks.ModelCheckpoint('/kaggle/working/Xception_Model.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:59:12.744185Z",
     "iopub.status.busy": "2023-08-16T11:59:12.743272Z",
     "iopub.status.idle": "2023-08-16T11:59:12.781204Z",
     "shell.execute_reply": "2023-08-16T11:59:12.780294Z",
     "shell.execute_reply.started": "2023-08-16T11:59:12.744151Z"
    }
   },
   "outputs": [],
   "source": [
    "xception_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T11:59:12.783038Z",
     "iopub.status.busy": "2023-08-16T11:59:12.782622Z",
     "iopub.status.idle": "2023-08-16T12:31:39.505264Z",
     "shell.execute_reply": "2023-08-16T12:31:39.504197Z",
     "shell.execute_reply.started": "2023-08-16T11:59:12.783006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "68/68 [==============================] - 27s 311ms/step - loss: 0.7140 - accuracy: 0.8119 - val_loss: 0.5357 - val_accuracy: 0.8750\n",
      "Epoch 2/120\n",
      "68/68 [==============================] - 18s 273ms/step - loss: 0.4189 - accuracy: 0.8855 - val_loss: 0.3988 - val_accuracy: 0.8842\n",
      "Epoch 3/120\n",
      "68/68 [==============================] - 16s 238ms/step - loss: 0.3461 - accuracy: 0.9103 - val_loss: 0.3546 - val_accuracy: 0.8934\n",
      "Epoch 4/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.2932 - accuracy: 0.9264 - val_loss: 0.3551 - val_accuracy: 0.9044\n",
      "Epoch 5/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.2529 - accuracy: 0.9480 - val_loss: 0.3622 - val_accuracy: 0.9081\n",
      "Epoch 6/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.2092 - accuracy: 0.9660 - val_loss: 0.3095 - val_accuracy: 0.9173\n",
      "Epoch 7/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.1737 - accuracy: 0.9770 - val_loss: 0.2903 - val_accuracy: 0.9246\n",
      "Epoch 8/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.1494 - accuracy: 0.9867 - val_loss: 0.2898 - val_accuracy: 0.9357\n",
      "Epoch 9/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.1284 - accuracy: 0.9913 - val_loss: 0.2314 - val_accuracy: 0.9301\n",
      "Epoch 10/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.1181 - accuracy: 0.9936 - val_loss: 0.2382 - val_accuracy: 0.9246\n",
      "Epoch 11/120\n",
      "68/68 [==============================] - 16s 239ms/step - loss: 0.1032 - accuracy: 0.9972 - val_loss: 0.2278 - val_accuracy: 0.9357\n",
      "Epoch 12/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.0885 - accuracy: 0.9977 - val_loss: 0.2054 - val_accuracy: 0.9467\n",
      "Epoch 13/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0867 - accuracy: 0.9982 - val_loss: 0.2142 - val_accuracy: 0.9485\n",
      "Epoch 14/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.0729 - accuracy: 0.9986 - val_loss: 0.1931 - val_accuracy: 0.9393\n",
      "Epoch 15/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0666 - accuracy: 0.9995 - val_loss: 0.2035 - val_accuracy: 0.9485\n",
      "Epoch 16/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0573 - accuracy: 0.9995 - val_loss: 0.1950 - val_accuracy: 0.9559\n",
      "Epoch 17/120\n",
      "68/68 [==============================] - 16s 234ms/step - loss: 0.0520 - accuracy: 0.9995 - val_loss: 0.1671 - val_accuracy: 0.9522\n",
      "Epoch 18/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0480 - accuracy: 1.0000 - val_loss: 0.1683 - val_accuracy: 0.9522\n",
      "Epoch 19/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.9596\n",
      "Epoch 20/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9596\n",
      "Epoch 21/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.1663 - val_accuracy: 0.9651\n",
      "Epoch 22/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9596\n",
      "Epoch 23/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9559\n",
      "Epoch 24/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9577\n",
      "Epoch 25/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0349 - accuracy: 0.9995 - val_loss: 0.1667 - val_accuracy: 0.9540\n",
      "Epoch 26/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9577\n",
      "Epoch 27/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9596\n",
      "Epoch 28/120\n",
      "68/68 [==============================] - 18s 268ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.1392 - val_accuracy: 0.9596\n",
      "Epoch 29/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.1394 - val_accuracy: 0.9669\n",
      "Epoch 30/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.1366 - val_accuracy: 0.9540\n",
      "Epoch 31/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9651\n",
      "Epoch 32/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 0.9651\n",
      "Epoch 33/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.1375 - val_accuracy: 0.9669\n",
      "Epoch 34/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.1319 - val_accuracy: 0.9688\n",
      "Epoch 35/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9614\n",
      "Epoch 36/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 0.9632\n",
      "Epoch 37/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9651\n",
      "Epoch 38/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.1318 - val_accuracy: 0.9669\n",
      "Epoch 39/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.1218 - val_accuracy: 0.9632\n",
      "Epoch 40/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.1236 - val_accuracy: 0.9688\n",
      "Epoch 41/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9651\n",
      "Epoch 42/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9614\n",
      "Epoch 43/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.1232 - val_accuracy: 0.9651\n",
      "Epoch 44/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9706\n",
      "Epoch 45/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.1194 - val_accuracy: 0.9651\n",
      "Epoch 46/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9651\n",
      "Epoch 47/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.1301 - val_accuracy: 0.9614\n",
      "Epoch 48/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.1283 - val_accuracy: 0.9632\n",
      "Epoch 49/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9688\n",
      "Epoch 50/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1232 - val_accuracy: 0.9651\n",
      "Epoch 51/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 0.9688\n",
      "Epoch 52/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9706\n",
      "Epoch 53/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 0.9651\n",
      "Epoch 54/120\n",
      "68/68 [==============================] - 18s 271ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 0.9651\n",
      "Epoch 55/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.1184 - val_accuracy: 0.9651\n",
      "Epoch 56/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9669\n",
      "Epoch 57/120\n",
      "68/68 [==============================] - 14s 213ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1133 - val_accuracy: 0.9688\n",
      "Epoch 58/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9651\n",
      "Epoch 59/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1164 - val_accuracy: 0.9614\n",
      "Epoch 60/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1217 - val_accuracy: 0.9596\n",
      "Epoch 61/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.1219 - val_accuracy: 0.9632\n",
      "Epoch 62/120\n",
      "68/68 [==============================] - 18s 270ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.9706\n",
      "Epoch 63/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9688\n",
      "Epoch 64/120\n",
      "68/68 [==============================] - 18s 270ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1045 - val_accuracy: 0.9688\n",
      "Epoch 65/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1101 - val_accuracy: 0.9688\n",
      "Epoch 66/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1011 - val_accuracy: 0.9688\n",
      "Epoch 67/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9669\n",
      "Epoch 68/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 0.9688\n",
      "Epoch 69/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9669\n",
      "Epoch 70/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9651\n",
      "Epoch 71/120\n",
      "68/68 [==============================] - 16s 234ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1006 - val_accuracy: 0.9669\n",
      "Epoch 72/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.1258 - val_accuracy: 0.9632\n",
      "Epoch 73/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0403 - accuracy: 0.9963 - val_loss: 0.3092 - val_accuracy: 0.9081\n",
      "Epoch 74/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0486 - accuracy: 0.9968 - val_loss: 0.2267 - val_accuracy: 0.9246\n",
      "Epoch 75/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.1535 - val_accuracy: 0.9540\n",
      "Epoch 76/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9651\n",
      "Epoch 77/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1185 - val_accuracy: 0.9688\n",
      "Epoch 78/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1628 - val_accuracy: 0.9540\n",
      "Epoch 79/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9688\n",
      "Epoch 80/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.9669\n",
      "Epoch 81/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1123 - val_accuracy: 0.9651\n",
      "Epoch 82/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1146 - val_accuracy: 0.9651\n",
      "Epoch 83/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9688\n",
      "Epoch 84/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9688\n",
      "Epoch 85/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0979 - val_accuracy: 0.9724\n",
      "Epoch 86/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1044 - val_accuracy: 0.9688\n",
      "Epoch 87/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9688\n",
      "Epoch 88/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9706\n",
      "Epoch 89/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1003 - val_accuracy: 0.9706\n",
      "Epoch 90/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1107 - val_accuracy: 0.9614\n",
      "Epoch 91/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 0.9706\n",
      "Epoch 92/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1002 - val_accuracy: 0.9743\n",
      "Epoch 93/120\n",
      "68/68 [==============================] - 16s 235ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9706\n",
      "Epoch 94/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9669\n",
      "Epoch 95/120\n",
      "68/68 [==============================] - 18s 270ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0938 - val_accuracy: 0.9743\n",
      "Epoch 96/120\n",
      "68/68 [==============================] - 18s 269ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9724\n",
      "Epoch 97/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9706\n",
      "Epoch 98/120\n",
      "68/68 [==============================] - 18s 272ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 0.9761\n",
      "Epoch 99/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9669\n",
      "Epoch 100/120\n",
      "68/68 [==============================] - 17s 247ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 0.9706\n",
      "Epoch 101/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0954 - val_accuracy: 0.9743\n",
      "Epoch 102/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9724\n",
      "Epoch 103/120\n",
      "68/68 [==============================] - 15s 215ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9724\n",
      "Epoch 104/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0937 - val_accuracy: 0.9724\n",
      "Epoch 105/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9743\n",
      "Epoch 106/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0981 - val_accuracy: 0.9743\n",
      "Epoch 107/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 0.9724\n",
      "Epoch 108/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0907 - val_accuracy: 0.9743\n",
      "Epoch 109/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0928 - val_accuracy: 0.9724\n",
      "Epoch 110/120\n",
      "68/68 [==============================] - 14s 213ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0907 - val_accuracy: 0.9724\n",
      "Epoch 111/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0952 - val_accuracy: 0.9743\n",
      "Epoch 112/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 0.9724\n",
      "Epoch 113/120\n",
      "68/68 [==============================] - 16s 236ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9743\n",
      "Epoch 114/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0925 - val_accuracy: 0.9724\n",
      "Epoch 115/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0902 - val_accuracy: 0.9706\n",
      "Epoch 116/120\n",
      "68/68 [==============================] - 15s 214ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9706\n",
      "Epoch 117/120\n",
      "68/68 [==============================] - 17s 249ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0927 - val_accuracy: 0.9724\n",
      "Epoch 118/120\n",
      "68/68 [==============================] - 14s 214ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9724\n",
      "Epoch 119/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 0.9779\n",
      "Epoch 120/120\n",
      "68/68 [==============================] - 17s 248ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9761\n"
     ]
    }
   ],
   "source": [
    "history_xception = xception_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test), callbacks=[checkpoint_xception])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T12:31:39.512340Z",
     "iopub.status.busy": "2023-08-16T12:31:39.512043Z",
     "iopub.status.idle": "2023-08-16T12:31:39.521054Z",
     "shell.execute_reply": "2023-08-16T12:31:39.520106Z",
     "shell.execute_reply.started": "2023-08-16T12:31:39.512314Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_xception.history)\n",
    "df.to_csv('/kaggle/working/Xception_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T12:31:39.523442Z",
     "iopub.status.busy": "2023-08-16T12:31:39.523165Z",
     "iopub.status.idle": "2023-08-16T12:31:42.690016Z",
     "shell.execute_reply": "2023-08-16T12:31:42.689051Z",
     "shell.execute_reply.started": "2023-08-16T12:31:39.523419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "vgg_wo_top = keras.applications.vgg19.VGG19(include_top=False, weights='imagenet', input_shape=(350, 350, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T12:31:42.691965Z",
     "iopub.status.busy": "2023-08-16T12:31:42.691611Z",
     "iopub.status.idle": "2023-08-16T12:31:42.698090Z",
     "shell.execute_reply": "2023-08-16T12:31:42.696839Z",
     "shell.execute_reply.started": "2023-08-16T12:31:42.691933Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_wo_top.trainable = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-15T07:53:13.364975Z",
     "iopub.status.busy": "2023-08-15T07:53:13.364155Z",
     "iopub.status.idle": "2023-08-15T07:53:13.540067Z",
     "shell.execute_reply": "2023-08-15T07:53:13.539304Z",
     "shell.execute_reply.started": "2023-08-15T07:53:13.364940Z"
    }
   },
   "source": [
    "vgg_model = keras.models.Sequential()\n",
    "vgg_model.add(vgg_wo_top)\n",
    "vgg_model.add(keras.layers.Flatten())\n",
    "vgg_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "vgg_model.add(keras.layers.BatchNormalization())\n",
    "vgg_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "vgg_model.add(keras.layers.BatchNormalization())\n",
    "vgg_model.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T12:31:42.700189Z",
     "iopub.status.busy": "2023-08-16T12:31:42.699548Z",
     "iopub.status.idle": "2023-08-16T12:31:42.864018Z",
     "shell.execute_reply": "2023-08-16T12:31:42.863257Z",
     "shell.execute_reply.started": "2023-08-16T12:31:42.700156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 350, 350, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " vgg19 (Functional)             (None, 10, 10, 512)  20024384    ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " attention_2 (Attention)        (None, 10, 10, 512)  0           ['vgg19[0][0]',                  \n",
      "                                                                  'vgg19[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 51200)        0           ['attention_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          6553728     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           8256        ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 6)            390         ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,587,270\n",
      "Trainable params: 6,562,630\n",
      "Non-trainable params: 20,024,640\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_vgg = keras.layers.Input(shape=(350, 350, 3))\n",
    "vgg_layer = vgg_wo_top(input_vgg)\n",
    "\n",
    "# random = np.random.random(vgg_layer.shape[1:])\n",
    "\n",
    "attention = keras.layers.Attention()([vgg_layer, vgg_layer])\n",
    "flatten = keras.layers.Flatten()(attention)\n",
    "\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "bn1 = keras.layers.BatchNormalization()(dense1)\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(bn1)\n",
    "output = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "vgg_model = keras.Model(inputs=input_vgg, outputs=output)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T12:31:42.865355Z",
     "iopub.status.busy": "2023-08-16T12:31:42.865025Z",
     "iopub.status.idle": "2023-08-16T12:31:42.878948Z",
     "shell.execute_reply": "2023-08-16T12:31:42.877630Z",
     "shell.execute_reply.started": "2023-08-16T12:31:42.865321Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_vgg = keras.callbacks.ModelCheckpoint('/kaggle/working/VGG_Model.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T12:31:42.880441Z",
     "iopub.status.busy": "2023-08-16T12:31:42.879905Z",
     "iopub.status.idle": "2023-08-16T12:31:42.895219Z",
     "shell.execute_reply": "2023-08-16T12:31:42.894107Z",
     "shell.execute_reply.started": "2023-08-16T12:31:42.880408Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T12:31:42.897167Z",
     "iopub.status.busy": "2023-08-16T12:31:42.896429Z",
     "iopub.status.idle": "2023-08-16T13:05:07.555923Z",
     "shell.execute_reply": "2023-08-16T13:05:07.554844Z",
     "shell.execute_reply.started": "2023-08-16T12:31:42.897134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "68/68 [==============================] - 28s 299ms/step - loss: 1.5538 - accuracy: 0.3910 - val_loss: 1.8324 - val_accuracy: 0.3695\n",
      "Epoch 2/120\n",
      "68/68 [==============================] - 16s 232ms/step - loss: 0.8592 - accuracy: 0.7691 - val_loss: 0.7530 - val_accuracy: 0.7390\n",
      "Epoch 3/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.6421 - accuracy: 0.8721 - val_loss: 0.4914 - val_accuracy: 0.8695\n",
      "Epoch 4/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.5210 - accuracy: 0.9006 - val_loss: 0.4650 - val_accuracy: 0.8879\n",
      "Epoch 5/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.4512 - accuracy: 0.9195 - val_loss: 0.4063 - val_accuracy: 0.8952\n",
      "Epoch 6/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.3892 - accuracy: 0.9434 - val_loss: 0.3617 - val_accuracy: 0.9265\n",
      "Epoch 7/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.3501 - accuracy: 0.9535 - val_loss: 0.3897 - val_accuracy: 0.9099\n",
      "Epoch 8/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.3123 - accuracy: 0.9641 - val_loss: 0.3485 - val_accuracy: 0.9246\n",
      "Epoch 9/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.2857 - accuracy: 0.9701 - val_loss: 0.4409 - val_accuracy: 0.8989\n",
      "Epoch 10/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.2665 - accuracy: 0.9701 - val_loss: 0.3278 - val_accuracy: 0.9375\n",
      "Epoch 11/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.2443 - accuracy: 0.9742 - val_loss: 0.3037 - val_accuracy: 0.9393\n",
      "Epoch 12/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.2229 - accuracy: 0.9811 - val_loss: 0.2777 - val_accuracy: 0.9449\n",
      "Epoch 13/120\n",
      "68/68 [==============================] - 16s 232ms/step - loss: 0.2074 - accuracy: 0.9844 - val_loss: 0.2745 - val_accuracy: 0.9540\n",
      "Epoch 14/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.1927 - accuracy: 0.9844 - val_loss: 0.2676 - val_accuracy: 0.9577\n",
      "Epoch 15/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.1808 - accuracy: 0.9899 - val_loss: 0.2386 - val_accuracy: 0.9596\n",
      "Epoch 16/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.1643 - accuracy: 0.9922 - val_loss: 0.2345 - val_accuracy: 0.9688\n",
      "Epoch 17/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.1518 - accuracy: 0.9922 - val_loss: 0.2272 - val_accuracy: 0.9688\n",
      "Epoch 18/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.1417 - accuracy: 0.9954 - val_loss: 0.2163 - val_accuracy: 0.9688\n",
      "Epoch 19/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.1325 - accuracy: 0.9949 - val_loss: 0.2078 - val_accuracy: 0.9706\n",
      "Epoch 20/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.1256 - accuracy: 0.9968 - val_loss: 0.1952 - val_accuracy: 0.9688\n",
      "Epoch 21/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.1178 - accuracy: 0.9972 - val_loss: 0.1836 - val_accuracy: 0.9724\n",
      "Epoch 22/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.1105 - accuracy: 0.9991 - val_loss: 0.1801 - val_accuracy: 0.9743\n",
      "Epoch 23/120\n",
      "68/68 [==============================] - 16s 232ms/step - loss: 0.0997 - accuracy: 0.9982 - val_loss: 0.1714 - val_accuracy: 0.9743\n",
      "Epoch 24/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0944 - accuracy: 0.9995 - val_loss: 0.1649 - val_accuracy: 0.9779\n",
      "Epoch 25/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0887 - accuracy: 0.9995 - val_loss: 0.1542 - val_accuracy: 0.9743\n",
      "Epoch 26/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0830 - accuracy: 0.9995 - val_loss: 0.1537 - val_accuracy: 0.9761\n",
      "Epoch 27/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0781 - accuracy: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.9798\n",
      "Epoch 28/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0734 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9798\n",
      "Epoch 29/120\n",
      "68/68 [==============================] - 16s 232ms/step - loss: 0.0688 - accuracy: 0.9995 - val_loss: 0.1357 - val_accuracy: 0.9761\n",
      "Epoch 30/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0679 - accuracy: 1.0000 - val_loss: 0.1317 - val_accuracy: 0.9779\n",
      "Epoch 31/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 0.1243 - val_accuracy: 0.9816\n",
      "Epoch 32/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0633 - accuracy: 0.9995 - val_loss: 0.1209 - val_accuracy: 0.9816\n",
      "Epoch 33/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.1166 - val_accuracy: 0.9835\n",
      "Epoch 34/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0529 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 0.9798\n",
      "Epoch 35/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9835\n",
      "Epoch 36/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9816\n",
      "Epoch 37/120\n",
      "68/68 [==============================] - 17s 258ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.1032 - val_accuracy: 0.9853\n",
      "Epoch 38/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9835\n",
      "Epoch 39/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 0.9835\n",
      "Epoch 40/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9853\n",
      "Epoch 41/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 0.0867 - val_accuracy: 0.9853\n",
      "Epoch 42/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9816\n",
      "Epoch 43/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.0847 - val_accuracy: 0.9871\n",
      "Epoch 44/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.0781 - val_accuracy: 0.9835\n",
      "Epoch 45/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.0793 - val_accuracy: 0.9871\n",
      "Epoch 46/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9835\n",
      "Epoch 47/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9890\n",
      "Epoch 48/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9835\n",
      "Epoch 49/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9890\n",
      "Epoch 50/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9853\n",
      "Epoch 51/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.0628 - val_accuracy: 0.9890\n",
      "Epoch 52/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9908\n",
      "Epoch 53/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9890\n",
      "Epoch 54/120\n",
      "68/68 [==============================] - 18s 264ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0561 - val_accuracy: 0.9926\n",
      "Epoch 55/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9945\n",
      "Epoch 56/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0559 - val_accuracy: 0.9890\n",
      "Epoch 57/120\n",
      "68/68 [==============================] - 18s 261ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.0552 - val_accuracy: 0.9926\n",
      "Epoch 58/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9890\n",
      "Epoch 59/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9908\n",
      "Epoch 60/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9926\n",
      "Epoch 61/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0497 - val_accuracy: 0.9945\n",
      "Epoch 62/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9926\n",
      "Epoch 63/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9926\n",
      "Epoch 64/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9945\n",
      "Epoch 65/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9926\n",
      "Epoch 66/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0438 - val_accuracy: 0.9926\n",
      "Epoch 67/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 0.9926\n",
      "Epoch 68/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9908\n",
      "Epoch 69/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9926\n",
      "Epoch 70/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0415 - val_accuracy: 0.9926\n",
      "Epoch 71/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 0.9945\n",
      "Epoch 72/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9926\n",
      "Epoch 73/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9945\n",
      "Epoch 74/120\n",
      "68/68 [==============================] - 17s 258ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9945\n",
      "Epoch 75/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9926\n",
      "Epoch 76/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9945\n",
      "Epoch 77/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9963\n",
      "Epoch 78/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0358 - val_accuracy: 0.9945\n",
      "Epoch 79/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 0.9963\n",
      "Epoch 80/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 0.9963\n",
      "Epoch 81/120\n",
      "68/68 [==============================] - 15s 226ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0339 - val_accuracy: 0.9945\n",
      "Epoch 82/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 0.9926\n",
      "Epoch 83/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 0.9926\n",
      "Epoch 84/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 0.9963\n",
      "Epoch 85/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0312 - val_accuracy: 0.9963\n",
      "Epoch 86/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 0.9963\n",
      "Epoch 87/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 0.9963\n",
      "Epoch 88/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 0.9963\n",
      "Epoch 89/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 0.9945\n",
      "Epoch 90/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 0.9945\n",
      "Epoch 91/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 0.9963\n",
      "Epoch 92/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9963\n",
      "Epoch 93/120\n",
      "68/68 [==============================] - 17s 258ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 0.9945\n",
      "Epoch 94/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 0.9945\n",
      "Epoch 95/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0291 - val_accuracy: 0.9926\n",
      "Epoch 96/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 0.9963\n",
      "Epoch 97/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0280 - val_accuracy: 0.9963\n",
      "Epoch 98/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 0.9908\n",
      "Epoch 99/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 0.9945\n",
      "Epoch 100/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 0.9945\n",
      "Epoch 101/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 0.9963\n",
      "Epoch 102/120\n",
      "68/68 [==============================] - 17s 258ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 0.9963\n",
      "Epoch 103/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 0.9871\n",
      "Epoch 104/120\n",
      "68/68 [==============================] - 18s 263ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 0.9945\n",
      "Epoch 105/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 0.9963\n",
      "Epoch 106/120\n",
      "68/68 [==============================] - 18s 262ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0235 - val_accuracy: 0.9963\n",
      "Epoch 107/120\n",
      "68/68 [==============================] - 17s 256ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 0.9963\n",
      "Epoch 108/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9963\n",
      "Epoch 109/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0238 - val_accuracy: 0.9963\n",
      "Epoch 110/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 0.9963\n",
      "Epoch 111/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0253 - val_accuracy: 0.9926\n",
      "Epoch 112/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9963\n",
      "Epoch 113/120\n",
      "68/68 [==============================] - 16s 231ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 0.9963\n",
      "Epoch 114/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 0.9963\n",
      "Epoch 115/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0216 - val_accuracy: 0.9963\n",
      "Epoch 116/120\n",
      "68/68 [==============================] - 17s 257ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9945\n",
      "Epoch 117/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 0.9963\n",
      "Epoch 118/120\n",
      "68/68 [==============================] - 15s 225ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 0.9963\n",
      "Epoch 119/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 0.9963\n",
      "Epoch 120/120\n",
      "68/68 [==============================] - 16s 230ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 0.9945\n"
     ]
    }
   ],
   "source": [
    "history_vgg = vgg_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test), callbacks=[checkpoint_vgg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T13:05:07.559375Z",
     "iopub.status.busy": "2023-08-16T13:05:07.559022Z",
     "iopub.status.idle": "2023-08-16T13:05:07.568586Z",
     "shell.execute_reply": "2023-08-16T13:05:07.567549Z",
     "shell.execute_reply.started": "2023-08-16T13:05:07.559348Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_vgg.history)\n",
    "df.to_csv('/kaggle/working/VGG_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T23:21:13.331078Z",
     "iopub.status.busy": "2023-08-08T23:21:13.330336Z",
     "iopub.status.idle": "2023-08-08T23:31:05.879965Z",
     "shell.execute_reply": "2023-08-08T23:31:05.878846Z",
     "shell.execute_reply.started": "2023-08-08T23:21:13.331044Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "enb0_wo_top = keras.applications.efficientnet.EfficientNetB0(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb0_wo_top.trainable = False\n",
    "\n",
    "enb0_model = keras.models.Sequential()\n",
    "enb0_model.add(enb0_wo_top)\n",
    "enb0_model.add(keras.layers.Flatten())\n",
    "enb0_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb0_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb0_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb0_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb0_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T13:05:07.570785Z",
     "iopub.status.busy": "2023-08-16T13:05:07.570407Z",
     "iopub.status.idle": "2023-08-16T13:05:12.149540Z",
     "shell.execute_reply": "2023-08-16T13:05:12.148556Z",
     "shell.execute_reply.started": "2023-08-16T13:05:07.570753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 1s 0us/step\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 350, 350, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnetb0 (Functional)    (None, 11, 11, 1280  4049571     ['input_7[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attention_3 (Attention)        (None, 11, 11, 1280  0           ['efficientnetb0[0][0]',         \n",
      "                                )                                 'efficientnetb0[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 154880)       0           ['attention_3[0][0]']            \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 128)          19824768    ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128)         512         ['dense_9[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           8256        ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 6)            390         ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,883,497\n",
      "Trainable params: 19,833,670\n",
      "Non-trainable params: 4,049,827\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enb0_wo_top = keras.applications.efficientnet.EfficientNetB0(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb0_wo_top.trainable = False\n",
    "\n",
    "input_enb0 = keras.layers.Input(shape=(350, 350, 3))\n",
    "enb0_layer = enb0_wo_top(input_enb0)\n",
    "\n",
    "# random = np.random.random(enb0_layer.shape[1:])\n",
    "\n",
    "attention = keras.layers.Attention()([enb0_layer, enb0_layer])\n",
    "flatten = keras.layers.Flatten()(attention)\n",
    "\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "bn1 = keras.layers.BatchNormalization()(dense1)\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(bn1)\n",
    "output = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "enb0_model = keras.Model(inputs=input_enb0, outputs=output)\n",
    "enb0_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T13:05:12.151741Z",
     "iopub.status.busy": "2023-08-16T13:05:12.151098Z",
     "iopub.status.idle": "2023-08-16T13:05:12.168362Z",
     "shell.execute_reply": "2023-08-16T13:05:12.167371Z",
     "shell.execute_reply.started": "2023-08-16T13:05:12.151706Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_enb0 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB0_Model.h5', save_best_only=True)\n",
    "\n",
    "enb0_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T13:05:12.170251Z",
     "iopub.status.busy": "2023-08-16T13:05:12.169911Z",
     "iopub.status.idle": "2023-08-16T13:21:14.981407Z",
     "shell.execute_reply": "2023-08-16T13:21:14.980422Z",
     "shell.execute_reply.started": "2023-08-16T13:05:12.170218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 13:05:20.786601: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_3/efficientnetb0/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 19s 157ms/step - loss: 0.8154 - accuracy: 0.7764 - val_loss: 0.8499 - val_accuracy: 0.7371\n",
      "Epoch 2/120\n",
      "68/68 [==============================] - 9s 128ms/step - loss: 0.4403 - accuracy: 0.9062 - val_loss: 0.4925 - val_accuracy: 0.8860\n",
      "Epoch 3/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.3738 - accuracy: 0.9218 - val_loss: 0.4444 - val_accuracy: 0.8934\n",
      "Epoch 4/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.3339 - accuracy: 0.9315 - val_loss: 0.3648 - val_accuracy: 0.9246\n",
      "Epoch 5/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.2922 - accuracy: 0.9407 - val_loss: 0.2817 - val_accuracy: 0.9522\n",
      "Epoch 6/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.2631 - accuracy: 0.9494 - val_loss: 0.2746 - val_accuracy: 0.9449\n",
      "Epoch 7/120\n",
      "68/68 [==============================] - 7s 111ms/step - loss: 0.2516 - accuracy: 0.9535 - val_loss: 0.3070 - val_accuracy: 0.9522\n",
      "Epoch 8/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.2349 - accuracy: 0.9595 - val_loss: 0.2955 - val_accuracy: 0.9357\n",
      "Epoch 9/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.2069 - accuracy: 0.9673 - val_loss: 0.2933 - val_accuracy: 0.9577\n",
      "Epoch 10/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.2063 - accuracy: 0.9650 - val_loss: 0.2220 - val_accuracy: 0.9614\n",
      "Epoch 11/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.1851 - accuracy: 0.9724 - val_loss: 0.2401 - val_accuracy: 0.9632\n",
      "Epoch 12/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.1668 - accuracy: 0.9793 - val_loss: 0.2395 - val_accuracy: 0.9559\n",
      "Epoch 13/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.1614 - accuracy: 0.9784 - val_loss: 0.2797 - val_accuracy: 0.9632\n",
      "Epoch 14/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.1425 - accuracy: 0.9867 - val_loss: 0.2361 - val_accuracy: 0.9798\n",
      "Epoch 15/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.1464 - accuracy: 0.9862 - val_loss: 0.1883 - val_accuracy: 0.9706\n",
      "Epoch 16/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.1336 - accuracy: 0.9876 - val_loss: 0.2270 - val_accuracy: 0.9651\n",
      "Epoch 17/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.1263 - accuracy: 0.9876 - val_loss: 0.2034 - val_accuracy: 0.9706\n",
      "Epoch 18/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.1193 - accuracy: 0.9903 - val_loss: 0.1375 - val_accuracy: 0.9853\n",
      "Epoch 19/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.1147 - accuracy: 0.9885 - val_loss: 0.1877 - val_accuracy: 0.9798\n",
      "Epoch 20/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.1040 - accuracy: 0.9922 - val_loss: 0.1714 - val_accuracy: 0.9853\n",
      "Epoch 21/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0976 - accuracy: 0.9917 - val_loss: 0.1634 - val_accuracy: 0.9816\n",
      "Epoch 22/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0921 - accuracy: 0.9931 - val_loss: 0.1516 - val_accuracy: 0.9798\n",
      "Epoch 23/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0946 - accuracy: 0.9917 - val_loss: 0.1606 - val_accuracy: 0.9908\n",
      "Epoch 24/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0827 - accuracy: 0.9936 - val_loss: 0.1598 - val_accuracy: 0.9835\n",
      "Epoch 25/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0793 - accuracy: 0.9959 - val_loss: 0.0934 - val_accuracy: 0.9871\n",
      "Epoch 26/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0740 - accuracy: 0.9968 - val_loss: 0.1812 - val_accuracy: 0.9761\n",
      "Epoch 27/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0769 - accuracy: 0.9931 - val_loss: 0.1545 - val_accuracy: 0.9908\n",
      "Epoch 28/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0671 - accuracy: 0.9977 - val_loss: 0.1120 - val_accuracy: 0.9871\n",
      "Epoch 29/120\n",
      "68/68 [==============================] - 9s 130ms/step - loss: 0.0653 - accuracy: 0.9977 - val_loss: 0.0920 - val_accuracy: 0.9945\n",
      "Epoch 30/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0630 - accuracy: 0.9968 - val_loss: 0.0779 - val_accuracy: 0.9945\n",
      "Epoch 31/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0649 - accuracy: 0.9945 - val_loss: 0.1710 - val_accuracy: 0.9706\n",
      "Epoch 32/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0584 - accuracy: 0.9968 - val_loss: 0.1201 - val_accuracy: 0.9835\n",
      "Epoch 33/120\n",
      "68/68 [==============================] - 9s 130ms/step - loss: 0.0546 - accuracy: 0.9963 - val_loss: 0.1173 - val_accuracy: 0.9835\n",
      "Epoch 34/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0570 - accuracy: 0.9949 - val_loss: 0.1334 - val_accuracy: 0.9871\n",
      "Epoch 35/120\n",
      "68/68 [==============================] - 9s 128ms/step - loss: 0.0537 - accuracy: 0.9949 - val_loss: 0.0945 - val_accuracy: 0.9963\n",
      "Epoch 36/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0551 - accuracy: 0.9945 - val_loss: 0.0818 - val_accuracy: 0.9945\n",
      "Epoch 37/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0523 - accuracy: 0.9959 - val_loss: 0.0887 - val_accuracy: 0.9871\n",
      "Epoch 38/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0493 - accuracy: 0.9954 - val_loss: 0.1857 - val_accuracy: 0.9559\n",
      "Epoch 39/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0428 - accuracy: 0.9995 - val_loss: 0.0703 - val_accuracy: 0.9853\n",
      "Epoch 40/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0471 - accuracy: 0.9945 - val_loss: 0.0856 - val_accuracy: 0.9963\n",
      "Epoch 41/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0500 - accuracy: 0.9954 - val_loss: 0.0654 - val_accuracy: 0.9926\n",
      "Epoch 42/120\n",
      "68/68 [==============================] - 9s 130ms/step - loss: 0.0421 - accuracy: 0.9968 - val_loss: 0.1959 - val_accuracy: 0.9596\n",
      "Epoch 43/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0444 - accuracy: 0.9954 - val_loss: 0.1389 - val_accuracy: 0.9871\n",
      "Epoch 44/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0387 - accuracy: 0.9972 - val_loss: 0.0677 - val_accuracy: 0.9908\n",
      "Epoch 45/120\n",
      "68/68 [==============================] - 9s 130ms/step - loss: 0.0398 - accuracy: 0.9963 - val_loss: 0.0786 - val_accuracy: 0.9908\n",
      "Epoch 46/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0382 - accuracy: 0.9972 - val_loss: 0.0746 - val_accuracy: 0.9890\n",
      "Epoch 47/120\n",
      "68/68 [==============================] - 7s 111ms/step - loss: 0.0305 - accuracy: 0.9995 - val_loss: 0.0678 - val_accuracy: 0.9982\n",
      "Epoch 48/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0327 - accuracy: 0.9982 - val_loss: 0.0633 - val_accuracy: 0.9890\n",
      "Epoch 49/120\n",
      "68/68 [==============================] - 9s 130ms/step - loss: 0.0320 - accuracy: 0.9968 - val_loss: 0.0826 - val_accuracy: 0.9798\n",
      "Epoch 50/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0404 - accuracy: 0.9949 - val_loss: 0.0699 - val_accuracy: 0.9890\n",
      "Epoch 51/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0331 - accuracy: 0.9986 - val_loss: 0.1152 - val_accuracy: 0.9779\n",
      "Epoch 52/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0281 - accuracy: 0.9991 - val_loss: 0.0625 - val_accuracy: 0.9945\n",
      "Epoch 53/120\n",
      "68/68 [==============================] - 9s 130ms/step - loss: 0.0340 - accuracy: 0.9945 - val_loss: 0.0633 - val_accuracy: 0.9871\n",
      "Epoch 54/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0340 - accuracy: 0.9963 - val_loss: 0.0604 - val_accuracy: 0.9798\n",
      "Epoch 55/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0362 - accuracy: 0.9945 - val_loss: 0.1605 - val_accuracy: 0.9632\n",
      "Epoch 56/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0315 - accuracy: 0.9972 - val_loss: 0.0667 - val_accuracy: 0.9871\n",
      "Epoch 57/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0345 - accuracy: 0.9972 - val_loss: 0.0515 - val_accuracy: 0.9963\n",
      "Epoch 58/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0282 - accuracy: 0.9977 - val_loss: 0.0426 - val_accuracy: 0.9963\n",
      "Epoch 59/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0253 - accuracy: 0.9995 - val_loss: 0.0595 - val_accuracy: 0.9908\n",
      "Epoch 60/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0254 - accuracy: 0.9982 - val_loss: 0.0454 - val_accuracy: 0.9945\n",
      "Epoch 61/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0240 - accuracy: 0.9982 - val_loss: 0.0595 - val_accuracy: 0.9945\n",
      "Epoch 62/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0235 - accuracy: 0.9991 - val_loss: 0.0542 - val_accuracy: 0.9853\n",
      "Epoch 63/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0280 - accuracy: 0.9977 - val_loss: 0.0406 - val_accuracy: 0.9945\n",
      "Epoch 64/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0303 - accuracy: 0.9963 - val_loss: 0.0543 - val_accuracy: 0.9908\n",
      "Epoch 65/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0236 - accuracy: 0.9991 - val_loss: 0.0693 - val_accuracy: 0.9908\n",
      "Epoch 66/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0180 - accuracy: 0.9982 - val_loss: 0.0407 - val_accuracy: 0.9963\n",
      "Epoch 67/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0285 - accuracy: 0.9977 - val_loss: 0.0563 - val_accuracy: 0.9982\n",
      "Epoch 68/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0274 - accuracy: 0.9963 - val_loss: 0.0934 - val_accuracy: 0.9816\n",
      "Epoch 69/120\n",
      "68/68 [==============================] - 8s 113ms/step - loss: 0.0324 - accuracy: 0.9963 - val_loss: 0.0539 - val_accuracy: 0.9871\n",
      "Epoch 70/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0282 - accuracy: 0.9968 - val_loss: 0.0669 - val_accuracy: 0.9890\n",
      "Epoch 71/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0209 - accuracy: 0.9982 - val_loss: 0.0425 - val_accuracy: 0.9945\n",
      "Epoch 72/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0217 - accuracy: 0.9977 - val_loss: 0.0504 - val_accuracy: 0.9816\n",
      "Epoch 73/120\n",
      "68/68 [==============================] - 9s 131ms/step - loss: 0.0215 - accuracy: 0.9986 - val_loss: 0.0518 - val_accuracy: 0.9945\n",
      "Epoch 74/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0270 - accuracy: 0.9959 - val_loss: 0.0501 - val_accuracy: 0.9908\n",
      "Epoch 75/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0208 - accuracy: 0.9991 - val_loss: 0.0668 - val_accuracy: 0.9835\n",
      "Epoch 76/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0245 - accuracy: 0.9963 - val_loss: 0.0617 - val_accuracy: 0.9853\n",
      "Epoch 77/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0207 - accuracy: 0.9986 - val_loss: 0.0538 - val_accuracy: 0.9853\n",
      "Epoch 78/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0168 - accuracy: 0.9995 - val_loss: 0.0718 - val_accuracy: 0.9890\n",
      "Epoch 79/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0236 - accuracy: 0.9972 - val_loss: 0.2060 - val_accuracy: 0.9375\n",
      "Epoch 80/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0166 - accuracy: 0.9986 - val_loss: 0.0513 - val_accuracy: 0.9926\n",
      "Epoch 81/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0191 - accuracy: 0.9977 - val_loss: 0.0436 - val_accuracy: 0.9926\n",
      "Epoch 82/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0257 - accuracy: 0.9963 - val_loss: 0.0396 - val_accuracy: 0.9945\n",
      "Epoch 83/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0199 - accuracy: 0.9977 - val_loss: 0.0317 - val_accuracy: 0.9945\n",
      "Epoch 84/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0189 - accuracy: 0.9982 - val_loss: 0.0984 - val_accuracy: 0.9835\n",
      "Epoch 85/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0234 - accuracy: 0.9959 - val_loss: 0.0399 - val_accuracy: 0.9926\n",
      "Epoch 86/120\n",
      "68/68 [==============================] - 9s 128ms/step - loss: 0.0137 - accuracy: 0.9995 - val_loss: 0.0309 - val_accuracy: 0.9963\n",
      "Epoch 87/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0163 - accuracy: 0.9986 - val_loss: 0.0426 - val_accuracy: 0.9871\n",
      "Epoch 88/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0136 - accuracy: 0.9986 - val_loss: 0.0530 - val_accuracy: 0.9890\n",
      "Epoch 89/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0239 - accuracy: 0.9954 - val_loss: 0.1264 - val_accuracy: 0.9743\n",
      "Epoch 90/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0173 - accuracy: 0.9986 - val_loss: 0.0404 - val_accuracy: 0.9890\n",
      "Epoch 91/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0125 - accuracy: 0.9986 - val_loss: 0.0314 - val_accuracy: 0.9963\n",
      "Epoch 92/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0223 - accuracy: 0.9959 - val_loss: 0.0399 - val_accuracy: 0.9963\n",
      "Epoch 93/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0175 - accuracy: 0.9982 - val_loss: 0.0516 - val_accuracy: 0.9890\n",
      "Epoch 94/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0162 - accuracy: 0.9977 - val_loss: 0.0409 - val_accuracy: 0.9871\n",
      "Epoch 95/120\n",
      "68/68 [==============================] - 9s 128ms/step - loss: 0.0131 - accuracy: 0.9986 - val_loss: 0.0853 - val_accuracy: 0.9743\n",
      "Epoch 96/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0193 - accuracy: 0.9968 - val_loss: 0.0450 - val_accuracy: 0.9853\n",
      "Epoch 97/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0214 - accuracy: 0.9959 - val_loss: 0.0435 - val_accuracy: 0.9890\n",
      "Epoch 98/120\n",
      "68/68 [==============================] - 7s 111ms/step - loss: 0.0156 - accuracy: 0.9986 - val_loss: 0.0327 - val_accuracy: 0.9963\n",
      "Epoch 99/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0138 - accuracy: 0.9986 - val_loss: 0.0302 - val_accuracy: 0.9908\n",
      "Epoch 100/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0156 - accuracy: 0.9982 - val_loss: 0.0328 - val_accuracy: 0.9926\n",
      "Epoch 101/120\n",
      "68/68 [==============================] - 9s 131ms/step - loss: 0.0155 - accuracy: 0.9991 - val_loss: 0.0310 - val_accuracy: 0.9945\n",
      "Epoch 102/120\n",
      "68/68 [==============================] - 7s 111ms/step - loss: 0.0196 - accuracy: 0.9959 - val_loss: 0.0528 - val_accuracy: 0.9890\n",
      "Epoch 103/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0181 - accuracy: 0.9982 - val_loss: 0.0346 - val_accuracy: 0.9926\n",
      "Epoch 104/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0212 - accuracy: 0.9963 - val_loss: 0.0449 - val_accuracy: 0.9908\n",
      "Epoch 105/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0158 - accuracy: 0.9977 - val_loss: 0.0400 - val_accuracy: 0.9890\n",
      "Epoch 106/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0134 - accuracy: 0.9986 - val_loss: 0.0463 - val_accuracy: 0.9890\n",
      "Epoch 107/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0117 - accuracy: 0.9986 - val_loss: 0.0392 - val_accuracy: 0.9963\n",
      "Epoch 108/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0132 - accuracy: 0.9995 - val_loss: 0.0547 - val_accuracy: 0.9908\n",
      "Epoch 109/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0158 - accuracy: 0.9963 - val_loss: 0.0520 - val_accuracy: 0.9816\n",
      "Epoch 110/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0160 - accuracy: 0.9972 - val_loss: 0.0702 - val_accuracy: 0.9835\n",
      "Epoch 111/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0111 - accuracy: 0.9995 - val_loss: 0.0315 - val_accuracy: 0.9963\n",
      "Epoch 112/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0139 - accuracy: 0.9972 - val_loss: 0.0271 - val_accuracy: 0.9945\n",
      "Epoch 113/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0138 - accuracy: 0.9986 - val_loss: 0.0393 - val_accuracy: 0.9963\n",
      "Epoch 114/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0165 - accuracy: 0.9963 - val_loss: 0.0611 - val_accuracy: 0.9779\n",
      "Epoch 115/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0186 - accuracy: 0.9959 - val_loss: 0.0496 - val_accuracy: 0.9871\n",
      "Epoch 116/120\n",
      "68/68 [==============================] - 9s 129ms/step - loss: 0.0141 - accuracy: 0.9986 - val_loss: 0.0415 - val_accuracy: 0.9890\n",
      "Epoch 117/120\n",
      "68/68 [==============================] - 8s 112ms/step - loss: 0.0155 - accuracy: 0.9982 - val_loss: 0.0236 - val_accuracy: 0.9963\n",
      "Epoch 118/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0112 - accuracy: 0.9991 - val_loss: 0.0248 - val_accuracy: 0.9945\n",
      "Epoch 119/120\n",
      "68/68 [==============================] - 8s 111ms/step - loss: 0.0092 - accuracy: 0.9991 - val_loss: 0.0251 - val_accuracy: 0.9945\n",
      "Epoch 120/120\n",
      "68/68 [==============================] - 7s 110ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history_enb0 = enb0_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T13:21:14.986899Z",
     "iopub.status.busy": "2023-08-16T13:21:14.986581Z",
     "iopub.status.idle": "2023-08-16T13:21:15.003628Z",
     "shell.execute_reply": "2023-08-16T13:21:15.002502Z",
     "shell.execute_reply.started": "2023-08-16T13:21:14.986872Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_enb0.history)\n",
    "df.to_csv('/kaggle/working/ENB0_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T23:33:52.744967Z",
     "iopub.status.busy": "2023-08-08T23:33:52.743959Z",
     "iopub.status.idle": "2023-08-08T23:45:29.747600Z",
     "shell.execute_reply": "2023-08-08T23:45:29.746515Z",
     "shell.execute_reply.started": "2023-08-08T23:33:52.744928Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "enb1_wo_top = keras.applications.efficientnet.EfficientNetB1(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb1_wo_top.trainable = False\n",
    "\n",
    "enb1_model = keras.models.Sequential()\n",
    "enb1_model.add(enb1_wo_top)\n",
    "enb1_model.add(keras.layers.Flatten())\n",
    "enb1_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb1_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb1_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb1_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb1_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T13:21:15.005687Z",
     "iopub.status.busy": "2023-08-16T13:21:15.004964Z",
     "iopub.status.idle": "2023-08-16T13:42:48.201765Z",
     "shell.execute_reply": "2023-08-16T13:42:48.200748Z",
     "shell.execute_reply.started": "2023-08-16T13:21:15.005655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "27018416/27018416 [==============================] - 2s 0us/step\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 350, 350, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnetb1 (Functional)    (None, 11, 11, 1280  6575239     ['input_9[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attention_4 (Attention)        (None, 11, 11, 1280  0           ['efficientnetb1[0][0]',         \n",
      "                                )                                 'efficientnetb1[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 154880)       0           ['attention_4[0][0]']            \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 128)          19824768    ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 128)         512         ['dense_12[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 64)           8256        ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 6)            390         ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,409,165\n",
      "Trainable params: 19,833,670\n",
      "Non-trainable params: 6,575,495\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 13:21:32.052864: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_4/efficientnetb1/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 24s 199ms/step - loss: 0.7892 - accuracy: 0.7705 - val_loss: 0.5624 - val_accuracy: 0.8603\n",
      "Epoch 2/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.4166 - accuracy: 0.8956 - val_loss: 0.3030 - val_accuracy: 0.9081\n",
      "Epoch 3/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.3203 - accuracy: 0.9177 - val_loss: 0.3200 - val_accuracy: 0.9338\n",
      "Epoch 4/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.2873 - accuracy: 0.9255 - val_loss: 0.2781 - val_accuracy: 0.9357\n",
      "Epoch 5/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.2489 - accuracy: 0.9351 - val_loss: 0.3211 - val_accuracy: 0.9357\n",
      "Epoch 6/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.2264 - accuracy: 0.9499 - val_loss: 0.2416 - val_accuracy: 0.9504\n",
      "Epoch 7/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.2067 - accuracy: 0.9545 - val_loss: 0.3634 - val_accuracy: 0.9357\n",
      "Epoch 8/120\n",
      "68/68 [==============================] - 11s 164ms/step - loss: 0.1938 - accuracy: 0.9577 - val_loss: 0.2275 - val_accuracy: 0.9651\n",
      "Epoch 9/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.1764 - accuracy: 0.9650 - val_loss: 0.3009 - val_accuracy: 0.9301\n",
      "Epoch 10/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.1627 - accuracy: 0.9673 - val_loss: 0.2246 - val_accuracy: 0.9504\n",
      "Epoch 11/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.1469 - accuracy: 0.9715 - val_loss: 0.2537 - val_accuracy: 0.9522\n",
      "Epoch 12/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.1333 - accuracy: 0.9802 - val_loss: 0.2078 - val_accuracy: 0.9724\n",
      "Epoch 13/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.1300 - accuracy: 0.9747 - val_loss: 0.2428 - val_accuracy: 0.9614\n",
      "Epoch 14/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.1358 - accuracy: 0.9784 - val_loss: 0.2531 - val_accuracy: 0.9632\n",
      "Epoch 15/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.1134 - accuracy: 0.9834 - val_loss: 0.2197 - val_accuracy: 0.9688\n",
      "Epoch 16/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.1040 - accuracy: 0.9844 - val_loss: 0.1731 - val_accuracy: 0.9688\n",
      "Epoch 17/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0994 - accuracy: 0.9880 - val_loss: 0.2083 - val_accuracy: 0.9779\n",
      "Epoch 18/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0994 - accuracy: 0.9830 - val_loss: 0.1116 - val_accuracy: 0.9835\n",
      "Epoch 19/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0838 - accuracy: 0.9903 - val_loss: 0.1713 - val_accuracy: 0.9688\n",
      "Epoch 20/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0840 - accuracy: 0.9894 - val_loss: 0.1740 - val_accuracy: 0.9779\n",
      "Epoch 21/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0860 - accuracy: 0.9862 - val_loss: 0.1799 - val_accuracy: 0.9761\n",
      "Epoch 22/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0763 - accuracy: 0.9922 - val_loss: 0.1287 - val_accuracy: 0.9761\n",
      "Epoch 23/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0818 - accuracy: 0.9876 - val_loss: 0.0915 - val_accuracy: 0.9871\n",
      "Epoch 24/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0780 - accuracy: 0.9894 - val_loss: 0.1382 - val_accuracy: 0.9743\n",
      "Epoch 25/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0774 - accuracy: 0.9894 - val_loss: 0.0984 - val_accuracy: 0.9853\n",
      "Epoch 26/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0683 - accuracy: 0.9931 - val_loss: 0.1164 - val_accuracy: 0.9835\n",
      "Epoch 27/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0682 - accuracy: 0.9917 - val_loss: 0.1520 - val_accuracy: 0.9706\n",
      "Epoch 28/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0635 - accuracy: 0.9908 - val_loss: 0.1943 - val_accuracy: 0.9706\n",
      "Epoch 29/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0646 - accuracy: 0.9922 - val_loss: 0.1650 - val_accuracy: 0.9798\n",
      "Epoch 30/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0586 - accuracy: 0.9940 - val_loss: 0.0914 - val_accuracy: 0.9945\n",
      "Epoch 31/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0594 - accuracy: 0.9931 - val_loss: 0.1878 - val_accuracy: 0.9632\n",
      "Epoch 32/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0589 - accuracy: 0.9926 - val_loss: 0.1244 - val_accuracy: 0.9890\n",
      "Epoch 33/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0585 - accuracy: 0.9936 - val_loss: 0.1856 - val_accuracy: 0.9614\n",
      "Epoch 34/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0543 - accuracy: 0.9926 - val_loss: 0.1187 - val_accuracy: 0.9835\n",
      "Epoch 35/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0432 - accuracy: 0.9963 - val_loss: 0.1015 - val_accuracy: 0.9816\n",
      "Epoch 36/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0445 - accuracy: 0.9968 - val_loss: 0.0680 - val_accuracy: 0.9890\n",
      "Epoch 37/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0487 - accuracy: 0.9931 - val_loss: 0.1768 - val_accuracy: 0.9779\n",
      "Epoch 38/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0447 - accuracy: 0.9949 - val_loss: 0.0901 - val_accuracy: 0.9963\n",
      "Epoch 39/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0438 - accuracy: 0.9972 - val_loss: 0.0993 - val_accuracy: 0.9890\n",
      "Epoch 40/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0460 - accuracy: 0.9940 - val_loss: 0.0632 - val_accuracy: 0.9963\n",
      "Epoch 41/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0435 - accuracy: 0.9936 - val_loss: 0.0864 - val_accuracy: 0.9890\n",
      "Epoch 42/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0412 - accuracy: 0.9949 - val_loss: 0.0588 - val_accuracy: 0.9945\n",
      "Epoch 43/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0390 - accuracy: 0.9963 - val_loss: 0.0706 - val_accuracy: 0.9890\n",
      "Epoch 44/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0415 - accuracy: 0.9931 - val_loss: 0.0822 - val_accuracy: 0.9908\n",
      "Epoch 45/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0385 - accuracy: 0.9963 - val_loss: 0.0687 - val_accuracy: 0.9871\n",
      "Epoch 46/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0385 - accuracy: 0.9954 - val_loss: 0.1360 - val_accuracy: 0.9835\n",
      "Epoch 47/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0334 - accuracy: 0.9968 - val_loss: 0.0649 - val_accuracy: 0.9908\n",
      "Epoch 48/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0425 - accuracy: 0.9936 - val_loss: 0.0567 - val_accuracy: 0.9890\n",
      "Epoch 49/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0346 - accuracy: 0.9968 - val_loss: 0.0908 - val_accuracy: 0.9853\n",
      "Epoch 50/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0372 - accuracy: 0.9940 - val_loss: 0.0956 - val_accuracy: 0.9908\n",
      "Epoch 51/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0407 - accuracy: 0.9917 - val_loss: 0.0685 - val_accuracy: 0.9853\n",
      "Epoch 52/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0301 - accuracy: 0.9982 - val_loss: 0.0755 - val_accuracy: 0.9871\n",
      "Epoch 53/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0306 - accuracy: 0.9972 - val_loss: 0.0493 - val_accuracy: 0.9963\n",
      "Epoch 54/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0284 - accuracy: 0.9986 - val_loss: 0.0721 - val_accuracy: 0.9890\n",
      "Epoch 55/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0292 - accuracy: 0.9972 - val_loss: 0.0824 - val_accuracy: 0.9890\n",
      "Epoch 56/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0254 - accuracy: 0.9991 - val_loss: 0.0686 - val_accuracy: 0.9890\n",
      "Epoch 57/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0274 - accuracy: 0.9963 - val_loss: 0.0533 - val_accuracy: 0.9908\n",
      "Epoch 58/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0289 - accuracy: 0.9963 - val_loss: 0.0681 - val_accuracy: 0.9871\n",
      "Epoch 59/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0278 - accuracy: 0.9977 - val_loss: 0.1662 - val_accuracy: 0.9651\n",
      "Epoch 60/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0230 - accuracy: 0.9986 - val_loss: 0.0499 - val_accuracy: 0.9926\n",
      "Epoch 61/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0305 - accuracy: 0.9954 - val_loss: 0.0578 - val_accuracy: 0.9926\n",
      "Epoch 62/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0325 - accuracy: 0.9945 - val_loss: 0.0566 - val_accuracy: 0.9853\n",
      "Epoch 63/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0275 - accuracy: 0.9959 - val_loss: 0.0687 - val_accuracy: 0.9871\n",
      "Epoch 64/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0294 - accuracy: 0.9959 - val_loss: 0.0781 - val_accuracy: 0.9853\n",
      "Epoch 65/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0207 - accuracy: 0.9986 - val_loss: 0.0503 - val_accuracy: 0.9908\n",
      "Epoch 66/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0296 - accuracy: 0.9959 - val_loss: 0.0758 - val_accuracy: 0.9908\n",
      "Epoch 67/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0256 - accuracy: 0.9977 - val_loss: 0.8874 - val_accuracy: 0.6985\n",
      "Epoch 68/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0282 - accuracy: 0.9949 - val_loss: 0.2814 - val_accuracy: 0.9467\n",
      "Epoch 69/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0288 - accuracy: 0.9949 - val_loss: 0.0880 - val_accuracy: 0.9853\n",
      "Epoch 70/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0216 - accuracy: 0.9972 - val_loss: 0.0423 - val_accuracy: 0.9945\n",
      "Epoch 71/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0271 - accuracy: 0.9963 - val_loss: 0.0454 - val_accuracy: 0.9945\n",
      "Epoch 72/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0210 - accuracy: 0.9982 - val_loss: 0.0576 - val_accuracy: 0.9908\n",
      "Epoch 73/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0317 - accuracy: 0.9949 - val_loss: 0.0545 - val_accuracy: 0.9871\n",
      "Epoch 74/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0332 - accuracy: 0.9931 - val_loss: 0.0809 - val_accuracy: 0.9926\n",
      "Epoch 75/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0283 - accuracy: 0.9945 - val_loss: 0.0457 - val_accuracy: 0.9945\n",
      "Epoch 76/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0242 - accuracy: 0.9949 - val_loss: 0.0410 - val_accuracy: 0.9926\n",
      "Epoch 77/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0236 - accuracy: 0.9963 - val_loss: 0.0530 - val_accuracy: 0.9853\n",
      "Epoch 78/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0220 - accuracy: 0.9982 - val_loss: 0.0857 - val_accuracy: 0.9871\n",
      "Epoch 79/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0257 - accuracy: 0.9959 - val_loss: 0.0963 - val_accuracy: 0.9761\n",
      "Epoch 80/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0247 - accuracy: 0.9972 - val_loss: 0.0674 - val_accuracy: 0.9963\n",
      "Epoch 81/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0212 - accuracy: 0.9982 - val_loss: 0.0401 - val_accuracy: 0.9908\n",
      "Epoch 82/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0245 - accuracy: 0.9982 - val_loss: 0.0494 - val_accuracy: 0.9926\n",
      "Epoch 83/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0277 - accuracy: 0.9963 - val_loss: 0.0481 - val_accuracy: 0.9945\n",
      "Epoch 84/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0269 - accuracy: 0.9963 - val_loss: 0.0427 - val_accuracy: 0.9890\n",
      "Epoch 85/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0280 - accuracy: 0.9959 - val_loss: 0.2600 - val_accuracy: 0.9412\n",
      "Epoch 86/120\n",
      "68/68 [==============================] - 10s 153ms/step - loss: 0.0280 - accuracy: 0.9959 - val_loss: 0.0478 - val_accuracy: 0.9926\n",
      "Epoch 87/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0237 - accuracy: 0.9977 - val_loss: 0.0392 - val_accuracy: 0.9908\n",
      "Epoch 88/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0199 - accuracy: 0.9986 - val_loss: 0.0350 - val_accuracy: 0.9926\n",
      "Epoch 89/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0260 - accuracy: 0.9977 - val_loss: 0.0883 - val_accuracy: 0.9779\n",
      "Epoch 90/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0216 - accuracy: 0.9986 - val_loss: 0.0363 - val_accuracy: 0.9982\n",
      "Epoch 91/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0168 - accuracy: 0.9995 - val_loss: 0.0321 - val_accuracy: 0.9963\n",
      "Epoch 92/120\n",
      "68/68 [==============================] - 10s 153ms/step - loss: 0.0176 - accuracy: 0.9977 - val_loss: 0.0326 - val_accuracy: 0.9926\n",
      "Epoch 93/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0177 - accuracy: 0.9991 - val_loss: 0.0355 - val_accuracy: 0.9945\n",
      "Epoch 94/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0264 - accuracy: 0.9940 - val_loss: 0.0640 - val_accuracy: 0.9871\n",
      "Epoch 95/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0225 - accuracy: 0.9986 - val_loss: 0.1361 - val_accuracy: 0.9761\n",
      "Epoch 96/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0184 - accuracy: 0.9991 - val_loss: 0.0447 - val_accuracy: 0.9926\n",
      "Epoch 97/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0179 - accuracy: 0.9986 - val_loss: 0.0702 - val_accuracy: 0.9945\n",
      "Epoch 98/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0178 - accuracy: 0.9977 - val_loss: 0.0737 - val_accuracy: 0.9945\n",
      "Epoch 99/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0174 - accuracy: 0.9991 - val_loss: 0.0418 - val_accuracy: 0.9945\n",
      "Epoch 100/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0172 - accuracy: 0.9991 - val_loss: 0.0329 - val_accuracy: 0.9963\n",
      "Epoch 101/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0184 - accuracy: 0.9977 - val_loss: 0.0358 - val_accuracy: 0.9963\n",
      "Epoch 102/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0170 - accuracy: 0.9977 - val_loss: 0.0402 - val_accuracy: 0.9963\n",
      "Epoch 103/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0235 - accuracy: 0.9954 - val_loss: 0.1186 - val_accuracy: 0.9761\n",
      "Epoch 104/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0202 - accuracy: 0.9972 - val_loss: 0.0444 - val_accuracy: 0.9926\n",
      "Epoch 105/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0183 - accuracy: 0.9977 - val_loss: 0.0689 - val_accuracy: 0.9945\n",
      "Epoch 106/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0168 - accuracy: 0.9977 - val_loss: 0.0380 - val_accuracy: 0.9963\n",
      "Epoch 107/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0155 - accuracy: 0.9977 - val_loss: 0.0392 - val_accuracy: 0.9963\n",
      "Epoch 108/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0132 - accuracy: 0.9991 - val_loss: 0.0280 - val_accuracy: 0.9982\n",
      "Epoch 109/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0124 - accuracy: 0.9995 - val_loss: 0.0327 - val_accuracy: 0.9926\n",
      "Epoch 110/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9926\n",
      "Epoch 111/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0113 - accuracy: 0.9991 - val_loss: 0.0398 - val_accuracy: 0.9963\n",
      "Epoch 112/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0123 - accuracy: 0.9991 - val_loss: 0.0302 - val_accuracy: 0.9963\n",
      "Epoch 113/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0144 - accuracy: 0.9986 - val_loss: 0.0282 - val_accuracy: 0.9982\n",
      "Epoch 114/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0127 - accuracy: 0.9991 - val_loss: 0.0272 - val_accuracy: 0.9963\n",
      "Epoch 115/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0168 - accuracy: 0.9977 - val_loss: 0.0474 - val_accuracy: 0.9890\n",
      "Epoch 116/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0123 - accuracy: 0.9991 - val_loss: 0.0453 - val_accuracy: 0.9926\n",
      "Epoch 117/120\n",
      "68/68 [==============================] - 10s 151ms/step - loss: 0.0214 - accuracy: 0.9959 - val_loss: 0.0648 - val_accuracy: 0.9853\n",
      "Epoch 118/120\n",
      "68/68 [==============================] - 11s 163ms/step - loss: 0.0152 - accuracy: 0.9991 - val_loss: 0.0462 - val_accuracy: 0.9853\n",
      "Epoch 119/120\n",
      "68/68 [==============================] - 10s 152ms/step - loss: 0.0131 - accuracy: 0.9991 - val_loss: 0.0242 - val_accuracy: 0.9982\n",
      "Epoch 120/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0156 - accuracy: 0.9968 - val_loss: 0.0290 - val_accuracy: 0.9926\n"
     ]
    }
   ],
   "source": [
    "enb1_wo_top = keras.applications.efficientnet.EfficientNetB1(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb1_wo_top.trainable = False\n",
    "\n",
    "input_enb1 = keras.layers.Input(shape=(350, 350, 3))\n",
    "enb1_layer = enb1_wo_top(input_enb1)\n",
    "\n",
    "# random = np.random.random(enb1_layer.shape[1:])\n",
    "\n",
    "attention = keras.layers.Attention()([enb1_layer, enb1_layer])\n",
    "flatten = keras.layers.Flatten()(attention)\n",
    "\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "bn1 = keras.layers.BatchNormalization()(dense1)\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(bn1)\n",
    "output = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "enb1_model = keras.Model(inputs=input_enb1, outputs=output)\n",
    "enb1_model.summary()\n",
    "\n",
    "checkpoint_enb1 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB1_Model.h5', save_best_only=True)\n",
    "\n",
    "enb1_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_enb1 = enb1_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))\n",
    "\n",
    "df = pd.DataFrame(history_enb1.history)\n",
    "df.to_csv('/kaggle/working/ENB1_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T23:45:29.750308Z",
     "iopub.status.busy": "2023-08-08T23:45:29.749927Z",
     "iopub.status.idle": "2023-08-08T23:55:09.765371Z",
     "shell.execute_reply": "2023-08-08T23:55:09.764195Z",
     "shell.execute_reply.started": "2023-08-08T23:45:29.750272Z"
    }
   },
   "source": [
    "enb2_wo_top = keras.applications.efficientnet.EfficientNetB2(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb2_wo_top.trainable = False\n",
    "\n",
    "enb2_model = keras.models.Sequential()\n",
    "enb2_model.add(enb0_wo_top)\n",
    "enb2_model.add(keras.layers.Flatten())\n",
    "enb2_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb2_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb2_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb2_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb2_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T13:42:48.203609Z",
     "iopub.status.busy": "2023-08-16T13:42:48.203225Z",
     "iopub.status.idle": "2023-08-16T14:05:27.559088Z",
     "shell.execute_reply": "2023-08-16T14:05:27.558057Z",
     "shell.execute_reply.started": "2023-08-16T13:42:48.203554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "31790344/31790344 [==============================] - 1s 0us/step\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 350, 350, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnetb2 (Functional)    (None, 11, 11, 1408  7768569     ['input_11[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attention_5 (Attention)        (None, 11, 11, 1408  0           ['efficientnetb2[0][0]',         \n",
      "                                )                                 'efficientnetb2[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 170368)       0           ['attention_5[0][0]']            \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 128)          21807232    ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 128)         512         ['dense_15[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64)           8256        ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 6)            390         ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,584,959\n",
      "Trainable params: 21,816,134\n",
      "Non-trainable params: 7,768,825\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 13:43:05.092803: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_5/efficientnetb2/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 25s 206ms/step - loss: 1.4207 - accuracy: 0.5359 - val_loss: 1.5732 - val_accuracy: 0.2941\n",
      "Epoch 2/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.8868 - accuracy: 0.8694 - val_loss: 1.1791 - val_accuracy: 0.7040\n",
      "Epoch 3/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.7519 - accuracy: 0.8988 - val_loss: 1.1884 - val_accuracy: 0.6820\n",
      "Epoch 4/120\n",
      "68/68 [==============================] - 11s 159ms/step - loss: 0.6845 - accuracy: 0.9089 - val_loss: 0.8390 - val_accuracy: 0.8934\n",
      "Epoch 5/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.6339 - accuracy: 0.9223 - val_loss: 0.9462 - val_accuracy: 0.8438\n",
      "Epoch 6/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.5919 - accuracy: 0.9209 - val_loss: 0.7649 - val_accuracy: 0.9246\n",
      "Epoch 7/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.5509 - accuracy: 0.9259 - val_loss: 0.7534 - val_accuracy: 0.9154\n",
      "Epoch 8/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.5191 - accuracy: 0.9305 - val_loss: 0.7033 - val_accuracy: 0.9210\n",
      "Epoch 9/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.4968 - accuracy: 0.9411 - val_loss: 0.9044 - val_accuracy: 0.8695\n",
      "Epoch 10/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.4681 - accuracy: 0.9466 - val_loss: 0.7331 - val_accuracy: 0.9467\n",
      "Epoch 11/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.4489 - accuracy: 0.9480 - val_loss: 0.6239 - val_accuracy: 0.9559\n",
      "Epoch 12/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.4282 - accuracy: 0.9540 - val_loss: 0.6049 - val_accuracy: 0.9485\n",
      "Epoch 13/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.4022 - accuracy: 0.9549 - val_loss: 0.6760 - val_accuracy: 0.9026\n",
      "Epoch 14/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.3862 - accuracy: 0.9581 - val_loss: 0.5672 - val_accuracy: 0.9596\n",
      "Epoch 15/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.3825 - accuracy: 0.9577 - val_loss: 0.6466 - val_accuracy: 0.9375\n",
      "Epoch 16/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.3499 - accuracy: 0.9687 - val_loss: 0.6418 - val_accuracy: 0.9540\n",
      "Epoch 17/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.3474 - accuracy: 0.9609 - val_loss: 0.5595 - val_accuracy: 0.9596\n",
      "Epoch 18/120\n",
      "68/68 [==============================] - 12s 171ms/step - loss: 0.3236 - accuracy: 0.9683 - val_loss: 0.5226 - val_accuracy: 0.9632\n",
      "Epoch 19/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.3043 - accuracy: 0.9733 - val_loss: 0.5434 - val_accuracy: 0.9669\n",
      "Epoch 20/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.2918 - accuracy: 0.9724 - val_loss: 0.6754 - val_accuracy: 0.9375\n",
      "Epoch 21/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.2785 - accuracy: 0.9770 - val_loss: 0.5364 - val_accuracy: 0.9743\n",
      "Epoch 22/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.2698 - accuracy: 0.9775 - val_loss: 0.6187 - val_accuracy: 0.9136\n",
      "Epoch 23/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.2597 - accuracy: 0.9830 - val_loss: 0.3832 - val_accuracy: 0.9816\n",
      "Epoch 24/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.2539 - accuracy: 0.9830 - val_loss: 0.5086 - val_accuracy: 0.9669\n",
      "Epoch 25/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.2367 - accuracy: 0.9880 - val_loss: 0.4623 - val_accuracy: 0.9761\n",
      "Epoch 26/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.2345 - accuracy: 0.9821 - val_loss: 0.4733 - val_accuracy: 0.9724\n",
      "Epoch 27/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.2271 - accuracy: 0.9816 - val_loss: 0.4957 - val_accuracy: 0.9743\n",
      "Epoch 28/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.2088 - accuracy: 0.9903 - val_loss: 0.4493 - val_accuracy: 0.9761\n",
      "Epoch 29/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.2053 - accuracy: 0.9857 - val_loss: 0.3228 - val_accuracy: 0.9835\n",
      "Epoch 30/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1978 - accuracy: 0.9894 - val_loss: 0.3663 - val_accuracy: 0.9835\n",
      "Epoch 31/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1939 - accuracy: 0.9876 - val_loss: 0.3258 - val_accuracy: 0.9835\n",
      "Epoch 32/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.1856 - accuracy: 0.9885 - val_loss: 0.3719 - val_accuracy: 0.9835\n",
      "Epoch 33/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1735 - accuracy: 0.9899 - val_loss: 0.3105 - val_accuracy: 0.9890\n",
      "Epoch 34/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1685 - accuracy: 0.9931 - val_loss: 1.0325 - val_accuracy: 0.6434\n",
      "Epoch 35/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.1614 - accuracy: 0.9913 - val_loss: 0.4835 - val_accuracy: 0.9779\n",
      "Epoch 36/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.1557 - accuracy: 0.9908 - val_loss: 0.5846 - val_accuracy: 0.9246\n",
      "Epoch 37/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1521 - accuracy: 0.9936 - val_loss: 0.6401 - val_accuracy: 0.9007\n",
      "Epoch 38/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.1491 - accuracy: 0.9913 - val_loss: 0.4383 - val_accuracy: 0.9651\n",
      "Epoch 39/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1452 - accuracy: 0.9936 - val_loss: 0.2581 - val_accuracy: 0.9835\n",
      "Epoch 40/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1401 - accuracy: 0.9885 - val_loss: 0.3874 - val_accuracy: 0.9706\n",
      "Epoch 41/120\n",
      "68/68 [==============================] - 12s 171ms/step - loss: 0.1326 - accuracy: 0.9931 - val_loss: 0.3572 - val_accuracy: 0.9853\n",
      "Epoch 42/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1236 - accuracy: 0.9963 - val_loss: 0.2715 - val_accuracy: 0.9871\n",
      "Epoch 43/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1182 - accuracy: 0.9949 - val_loss: 0.2799 - val_accuracy: 0.9908\n",
      "Epoch 44/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1179 - accuracy: 0.9945 - val_loss: 0.2170 - val_accuracy: 0.9945\n",
      "Epoch 45/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1138 - accuracy: 0.9917 - val_loss: 0.2113 - val_accuracy: 0.9926\n",
      "Epoch 46/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.1097 - accuracy: 0.9949 - val_loss: 0.3095 - val_accuracy: 0.9908\n",
      "Epoch 47/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1059 - accuracy: 0.9972 - val_loss: 0.2468 - val_accuracy: 0.9926\n",
      "Epoch 48/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.1019 - accuracy: 0.9940 - val_loss: 0.2658 - val_accuracy: 0.9871\n",
      "Epoch 49/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0993 - accuracy: 0.9936 - val_loss: 0.2506 - val_accuracy: 0.9908\n",
      "Epoch 50/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0947 - accuracy: 0.9972 - val_loss: 0.3230 - val_accuracy: 0.9779\n",
      "Epoch 51/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0856 - accuracy: 0.9977 - val_loss: 0.1539 - val_accuracy: 0.9908\n",
      "Epoch 52/120\n",
      "68/68 [==============================] - 11s 162ms/step - loss: 0.0843 - accuracy: 0.9982 - val_loss: 0.2320 - val_accuracy: 0.9945\n",
      "Epoch 53/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0808 - accuracy: 0.9963 - val_loss: 0.3416 - val_accuracy: 0.9853\n",
      "Epoch 54/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0783 - accuracy: 0.9972 - val_loss: 0.2303 - val_accuracy: 0.9926\n",
      "Epoch 55/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0783 - accuracy: 0.9968 - val_loss: 0.2161 - val_accuracy: 0.9945\n",
      "Epoch 56/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0768 - accuracy: 0.9954 - val_loss: 0.1965 - val_accuracy: 0.9890\n",
      "Epoch 57/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0705 - accuracy: 0.9977 - val_loss: 0.2399 - val_accuracy: 0.9835\n",
      "Epoch 58/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0700 - accuracy: 0.9982 - val_loss: 0.1779 - val_accuracy: 0.9945\n",
      "Epoch 59/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0676 - accuracy: 0.9968 - val_loss: 0.2514 - val_accuracy: 0.9945\n",
      "Epoch 60/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0684 - accuracy: 0.9977 - val_loss: 0.3033 - val_accuracy: 0.9743\n",
      "Epoch 61/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0678 - accuracy: 0.9949 - val_loss: 0.1690 - val_accuracy: 0.9945\n",
      "Epoch 62/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0649 - accuracy: 0.9940 - val_loss: 0.2466 - val_accuracy: 0.9908\n",
      "Epoch 63/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0595 - accuracy: 0.9986 - val_loss: 0.1610 - val_accuracy: 0.9982\n",
      "Epoch 64/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.0624 - accuracy: 0.9977 - val_loss: 0.1109 - val_accuracy: 0.9926\n",
      "Epoch 65/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0603 - accuracy: 0.9977 - val_loss: 0.2822 - val_accuracy: 0.9706\n",
      "Epoch 66/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0589 - accuracy: 0.9968 - val_loss: 0.1898 - val_accuracy: 0.9963\n",
      "Epoch 67/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0526 - accuracy: 0.9991 - val_loss: 0.3210 - val_accuracy: 0.9724\n",
      "Epoch 68/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0489 - accuracy: 0.9991 - val_loss: 0.1601 - val_accuracy: 0.9926\n",
      "Epoch 69/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0497 - accuracy: 0.9986 - val_loss: 0.2360 - val_accuracy: 0.9835\n",
      "Epoch 70/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0517 - accuracy: 0.9968 - val_loss: 0.1330 - val_accuracy: 0.9926\n",
      "Epoch 71/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0449 - accuracy: 0.9995 - val_loss: 0.0796 - val_accuracy: 0.9982\n",
      "Epoch 72/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0477 - accuracy: 0.9959 - val_loss: 0.2987 - val_accuracy: 0.9596\n",
      "Epoch 73/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0486 - accuracy: 0.9959 - val_loss: 0.2563 - val_accuracy: 0.9632\n",
      "Epoch 74/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0413 - accuracy: 0.9995 - val_loss: 0.1438 - val_accuracy: 0.9963\n",
      "Epoch 75/120\n",
      "68/68 [==============================] - 12s 171ms/step - loss: 0.0455 - accuracy: 0.9972 - val_loss: 0.1157 - val_accuracy: 0.9926\n",
      "Epoch 76/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0468 - accuracy: 0.9963 - val_loss: 0.1530 - val_accuracy: 0.9890\n",
      "Epoch 77/120\n",
      "68/68 [==============================] - 11s 159ms/step - loss: 0.0426 - accuracy: 0.9986 - val_loss: 0.1793 - val_accuracy: 0.9963\n",
      "Epoch 78/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.0358 - accuracy: 0.9991 - val_loss: 0.1516 - val_accuracy: 0.9945\n",
      "Epoch 79/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0440 - accuracy: 0.9972 - val_loss: 0.0819 - val_accuracy: 0.9982\n",
      "Epoch 80/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0406 - accuracy: 0.9972 - val_loss: 0.1448 - val_accuracy: 0.9926\n",
      "Epoch 81/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0359 - accuracy: 0.9982 - val_loss: 0.1372 - val_accuracy: 0.9945\n",
      "Epoch 82/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0335 - accuracy: 0.9986 - val_loss: 0.1816 - val_accuracy: 0.9908\n",
      "Epoch 83/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0359 - accuracy: 0.9986 - val_loss: 0.1623 - val_accuracy: 0.9963\n",
      "Epoch 84/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0349 - accuracy: 0.9982 - val_loss: 0.2800 - val_accuracy: 0.9779\n",
      "Epoch 85/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0345 - accuracy: 0.9982 - val_loss: 0.7003 - val_accuracy: 0.7224\n",
      "Epoch 86/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.0319 - accuracy: 0.9986 - val_loss: 0.0908 - val_accuracy: 0.9908\n",
      "Epoch 87/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0381 - accuracy: 0.9963 - val_loss: 0.2145 - val_accuracy: 0.9798\n",
      "Epoch 88/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0399 - accuracy: 0.9959 - val_loss: 0.1071 - val_accuracy: 0.9853\n",
      "Epoch 89/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0272 - accuracy: 0.9991 - val_loss: 0.2126 - val_accuracy: 0.9853\n",
      "Epoch 90/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0340 - accuracy: 0.9972 - val_loss: 0.1219 - val_accuracy: 0.9963\n",
      "Epoch 91/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0345 - accuracy: 0.9972 - val_loss: 0.1276 - val_accuracy: 0.9945\n",
      "Epoch 92/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.0335 - accuracy: 0.9959 - val_loss: 0.1148 - val_accuracy: 0.9908\n",
      "Epoch 93/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0272 - accuracy: 0.9991 - val_loss: 0.0904 - val_accuracy: 0.9945\n",
      "Epoch 94/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0244 - accuracy: 0.9995 - val_loss: 0.1082 - val_accuracy: 0.9945\n",
      "Epoch 95/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0242 - accuracy: 0.9991 - val_loss: 0.1132 - val_accuracy: 0.9945\n",
      "Epoch 96/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0256 - accuracy: 0.9982 - val_loss: 0.2633 - val_accuracy: 0.9724\n",
      "Epoch 97/120\n",
      "68/68 [==============================] - 11s 170ms/step - loss: 0.0338 - accuracy: 0.9959 - val_loss: 0.1897 - val_accuracy: 0.9669\n",
      "Epoch 98/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0348 - accuracy: 0.9968 - val_loss: 0.1220 - val_accuracy: 0.9779\n",
      "Epoch 99/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0251 - accuracy: 0.9963 - val_loss: 0.1545 - val_accuracy: 0.9890\n",
      "Epoch 100/120\n",
      "68/68 [==============================] - 11s 170ms/step - loss: 0.0272 - accuracy: 0.9977 - val_loss: 0.3483 - val_accuracy: 0.9430\n",
      "Epoch 101/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0251 - accuracy: 0.9977 - val_loss: 0.2039 - val_accuracy: 0.9816\n",
      "Epoch 102/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0296 - accuracy: 0.9968 - val_loss: 0.1257 - val_accuracy: 0.9908\n",
      "Epoch 103/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0201 - accuracy: 0.9991 - val_loss: 0.0740 - val_accuracy: 0.9963\n",
      "Epoch 104/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.0181 - accuracy: 0.9995 - val_loss: 0.0699 - val_accuracy: 0.9982\n",
      "Epoch 105/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0214 - accuracy: 0.9991 - val_loss: 0.0519 - val_accuracy: 0.9982\n",
      "Epoch 106/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0183 - accuracy: 0.9991 - val_loss: 0.0436 - val_accuracy: 0.9982\n",
      "Epoch 107/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0173 - accuracy: 0.9995 - val_loss: 0.0379 - val_accuracy: 0.9963\n",
      "Epoch 108/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9982\n",
      "Epoch 109/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0208 - accuracy: 0.9977 - val_loss: 0.1109 - val_accuracy: 0.9945\n",
      "Epoch 110/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0259 - accuracy: 0.9968 - val_loss: 0.0919 - val_accuracy: 0.9908\n",
      "Epoch 111/120\n",
      "68/68 [==============================] - 11s 168ms/step - loss: 0.0209 - accuracy: 0.9977 - val_loss: 0.0707 - val_accuracy: 0.9982\n",
      "Epoch 112/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.0184 - accuracy: 0.9977 - val_loss: 0.1019 - val_accuracy: 0.9945\n",
      "Epoch 113/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0187 - accuracy: 0.9986 - val_loss: 0.0945 - val_accuracy: 0.9890\n",
      "Epoch 114/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0152 - accuracy: 0.9991 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
      "Epoch 115/120\n",
      "68/68 [==============================] - 12s 170ms/step - loss: 0.0210 - accuracy: 0.9977 - val_loss: 0.2684 - val_accuracy: 0.9559\n",
      "Epoch 116/120\n",
      "68/68 [==============================] - 11s 161ms/step - loss: 0.0335 - accuracy: 0.9940 - val_loss: 0.7980 - val_accuracy: 0.7555\n",
      "Epoch 117/120\n",
      "68/68 [==============================] - 11s 169ms/step - loss: 0.0241 - accuracy: 0.9977 - val_loss: 0.1607 - val_accuracy: 0.9908\n",
      "Epoch 118/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0201 - accuracy: 0.9982 - val_loss: 0.4975 - val_accuracy: 0.8676\n",
      "Epoch 119/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0226 - accuracy: 0.9977 - val_loss: 0.1604 - val_accuracy: 0.9835\n",
      "Epoch 120/120\n",
      "68/68 [==============================] - 11s 160ms/step - loss: 0.0178 - accuracy: 0.9991 - val_loss: 0.2443 - val_accuracy: 0.9577\n"
     ]
    }
   ],
   "source": [
    "enb2_wo_top = keras.applications.efficientnet.EfficientNetB2(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb2_wo_top.trainable = False\n",
    "\n",
    "input_enb2 = keras.layers.Input(shape=(350, 350, 3))\n",
    "enb2_layer = enb2_wo_top(input_enb2)\n",
    "\n",
    "# random = np.random.random(enb2_layer.shape[1:])\n",
    "\n",
    "attention = keras.layers.Attention()([enb2_layer, enb2_layer])\n",
    "flatten = keras.layers.Flatten()(attention)\n",
    "\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "bn1 = keras.layers.BatchNormalization()(dense1)\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(bn1)\n",
    "output = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "enb2_model = keras.Model(inputs=input_enb2, outputs=output)\n",
    "enb2_model.summary()\n",
    "\n",
    "checkpoint_enb2 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB2_Model.h5', save_best_only=True)\n",
    "\n",
    "enb2_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_enb2 = enb2_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))\n",
    "\n",
    "df = pd.DataFrame(history_enb2.history)\n",
    "df.to_csv('/kaggle/working/ENB2_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T23:55:09.768882Z",
     "iopub.status.busy": "2023-08-08T23:55:09.768130Z",
     "iopub.status.idle": "2023-08-09T00:09:21.243960Z",
     "shell.execute_reply": "2023-08-09T00:09:21.242914Z",
     "shell.execute_reply.started": "2023-08-08T23:55:09.768806Z"
    }
   },
   "source": [
    "enb3_wo_top = keras.applications.efficientnet.EfficientNetB3(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb3_wo_top.trainable = False\n",
    "\n",
    "enb3_model = keras.models.Sequential()\n",
    "enb3_model.add(enb3_wo_top)\n",
    "enb3_model.add(keras.layers.Flatten())\n",
    "enb3_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb3_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb3_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb3_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb3_model.fit(x_train, y_train, batch_size=32, epochs=60, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-16T14:05:27.562038Z",
     "iopub.status.busy": "2023-08-16T14:05:27.561679Z",
     "iopub.status.idle": "2023-08-16T14:33:41.024119Z",
     "shell.execute_reply": "2023-08-16T14:33:41.023192Z",
     "shell.execute_reply.started": "2023-08-16T14:05:27.562003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941136/43941136 [==============================] - 2s 0us/step\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 350, 350, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " efficientnetb3 (Functional)    (None, 11, 11, 1536  10783535    ['input_13[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attention_6 (Attention)        (None, 11, 11, 1536  0           ['efficientnetb3[0][0]',         \n",
      "                                )                                 'efficientnetb3[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 185856)       0           ['attention_6[0][0]']            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 128)          23789696    ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 128)         512         ['dense_18[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 64)           8256        ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 6)            390         ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,582,389\n",
      "Trainable params: 23,798,598\n",
      "Non-trainable params: 10,783,791\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 14:05:46.567780: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_6/efficientnetb3/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 32s 287ms/step - loss: 0.6563 - accuracy: 0.8243 - val_loss: 0.4646 - val_accuracy: 0.8768\n",
      "Epoch 2/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.3623 - accuracy: 0.9080 - val_loss: 0.3580 - val_accuracy: 0.9154\n",
      "Epoch 3/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.3021 - accuracy: 0.9278 - val_loss: 0.3027 - val_accuracy: 0.9246\n",
      "Epoch 4/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.2750 - accuracy: 0.9328 - val_loss: 0.4061 - val_accuracy: 0.9081\n",
      "Epoch 5/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.2434 - accuracy: 0.9517 - val_loss: 0.2744 - val_accuracy: 0.9540\n",
      "Epoch 6/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.2240 - accuracy: 0.9517 - val_loss: 0.1943 - val_accuracy: 0.9651\n",
      "Epoch 7/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.1969 - accuracy: 0.9604 - val_loss: 0.1905 - val_accuracy: 0.9651\n",
      "Epoch 8/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1812 - accuracy: 0.9696 - val_loss: 0.2363 - val_accuracy: 0.9688\n",
      "Epoch 9/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1652 - accuracy: 0.9733 - val_loss: 0.1929 - val_accuracy: 0.9743\n",
      "Epoch 10/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1577 - accuracy: 0.9752 - val_loss: 0.2722 - val_accuracy: 0.9357\n",
      "Epoch 11/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.1511 - accuracy: 0.9802 - val_loss: 0.2338 - val_accuracy: 0.9632\n",
      "Epoch 12/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1450 - accuracy: 0.9770 - val_loss: 0.2234 - val_accuracy: 0.9614\n",
      "Epoch 13/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1395 - accuracy: 0.9834 - val_loss: 0.1705 - val_accuracy: 0.9761\n",
      "Epoch 14/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1272 - accuracy: 0.9885 - val_loss: 0.2236 - val_accuracy: 0.9816\n",
      "Epoch 15/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1180 - accuracy: 0.9876 - val_loss: 0.1987 - val_accuracy: 0.9761\n",
      "Epoch 16/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1154 - accuracy: 0.9885 - val_loss: 0.2051 - val_accuracy: 0.9688\n",
      "Epoch 17/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.1095 - accuracy: 0.9890 - val_loss: 0.1521 - val_accuracy: 0.9724\n",
      "Epoch 18/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.1008 - accuracy: 0.9903 - val_loss: 0.1761 - val_accuracy: 0.9835\n",
      "Epoch 19/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0998 - accuracy: 0.9903 - val_loss: 0.2022 - val_accuracy: 0.9669\n",
      "Epoch 20/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0848 - accuracy: 0.9926 - val_loss: 0.1126 - val_accuracy: 0.9871\n",
      "Epoch 21/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0890 - accuracy: 0.9903 - val_loss: 0.1621 - val_accuracy: 0.9816\n",
      "Epoch 22/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0875 - accuracy: 0.9899 - val_loss: 0.2341 - val_accuracy: 0.9485\n",
      "Epoch 23/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0805 - accuracy: 0.9945 - val_loss: 0.1089 - val_accuracy: 0.9853\n",
      "Epoch 24/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0770 - accuracy: 0.9922 - val_loss: 0.1001 - val_accuracy: 0.9835\n",
      "Epoch 25/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0670 - accuracy: 0.9954 - val_loss: 0.1396 - val_accuracy: 0.9816\n",
      "Epoch 26/120\n",
      "68/68 [==============================] - 14s 207ms/step - loss: 0.0687 - accuracy: 0.9936 - val_loss: 0.2284 - val_accuracy: 0.9743\n",
      "Epoch 27/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0700 - accuracy: 0.9922 - val_loss: 0.1373 - val_accuracy: 0.9853\n",
      "Epoch 28/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0640 - accuracy: 0.9963 - val_loss: 0.1259 - val_accuracy: 0.9853\n",
      "Epoch 29/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0638 - accuracy: 0.9931 - val_loss: 0.1753 - val_accuracy: 0.9688\n",
      "Epoch 30/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0609 - accuracy: 0.9949 - val_loss: 0.1300 - val_accuracy: 0.9871\n",
      "Epoch 31/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0621 - accuracy: 0.9931 - val_loss: 0.0752 - val_accuracy: 0.9835\n",
      "Epoch 32/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0518 - accuracy: 0.9949 - val_loss: 0.0765 - val_accuracy: 0.9890\n",
      "Epoch 33/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0582 - accuracy: 0.9936 - val_loss: 0.1041 - val_accuracy: 0.9798\n",
      "Epoch 34/120\n",
      "68/68 [==============================] - 14s 203ms/step - loss: 0.0513 - accuracy: 0.9963 - val_loss: 0.1067 - val_accuracy: 0.9798\n",
      "Epoch 35/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0517 - accuracy: 0.9954 - val_loss: 0.0616 - val_accuracy: 0.9871\n",
      "Epoch 36/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0468 - accuracy: 0.9972 - val_loss: 0.1198 - val_accuracy: 0.9890\n",
      "Epoch 37/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0443 - accuracy: 0.9986 - val_loss: 0.0855 - val_accuracy: 0.9871\n",
      "Epoch 38/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0517 - accuracy: 0.9949 - val_loss: 0.1315 - val_accuracy: 0.9853\n",
      "Epoch 39/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0407 - accuracy: 0.9986 - val_loss: 0.0765 - val_accuracy: 0.9871\n",
      "Epoch 40/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0437 - accuracy: 0.9959 - val_loss: 0.0727 - val_accuracy: 0.9816\n",
      "Epoch 41/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0426 - accuracy: 0.9959 - val_loss: 0.0720 - val_accuracy: 0.9926\n",
      "Epoch 42/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0387 - accuracy: 0.9977 - val_loss: 0.0763 - val_accuracy: 0.9835\n",
      "Epoch 43/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0388 - accuracy: 0.9977 - val_loss: 0.0678 - val_accuracy: 0.9871\n",
      "Epoch 44/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0382 - accuracy: 0.9977 - val_loss: 0.1164 - val_accuracy: 0.9835\n",
      "Epoch 45/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0354 - accuracy: 0.9986 - val_loss: 0.0802 - val_accuracy: 0.9853\n",
      "Epoch 46/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0354 - accuracy: 0.9963 - val_loss: 0.1045 - val_accuracy: 0.9908\n",
      "Epoch 47/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0405 - accuracy: 0.9959 - val_loss: 0.1307 - val_accuracy: 0.9816\n",
      "Epoch 48/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0321 - accuracy: 0.9982 - val_loss: 0.1661 - val_accuracy: 0.9835\n",
      "Epoch 49/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0294 - accuracy: 0.9991 - val_loss: 0.0789 - val_accuracy: 0.9890\n",
      "Epoch 50/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0298 - accuracy: 0.9991 - val_loss: 0.0772 - val_accuracy: 0.9853\n",
      "Epoch 51/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0282 - accuracy: 0.9982 - val_loss: 0.0960 - val_accuracy: 0.9871\n",
      "Epoch 52/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0297 - accuracy: 0.9972 - val_loss: 0.0805 - val_accuracy: 0.9963\n",
      "Epoch 53/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0277 - accuracy: 0.9982 - val_loss: 0.0648 - val_accuracy: 0.9908\n",
      "Epoch 54/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0283 - accuracy: 0.9977 - val_loss: 0.0673 - val_accuracy: 0.9945\n",
      "Epoch 55/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0344 - accuracy: 0.9954 - val_loss: 0.0466 - val_accuracy: 0.9945\n",
      "Epoch 56/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0264 - accuracy: 0.9991 - val_loss: 0.0443 - val_accuracy: 0.9963\n",
      "Epoch 57/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0245 - accuracy: 0.9991 - val_loss: 0.0932 - val_accuracy: 0.9816\n",
      "Epoch 58/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0233 - accuracy: 0.9991 - val_loss: 0.0584 - val_accuracy: 0.9853\n",
      "Epoch 59/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0225 - accuracy: 0.9991 - val_loss: 0.0309 - val_accuracy: 0.9945\n",
      "Epoch 60/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0202 - accuracy: 0.9986 - val_loss: 0.0326 - val_accuracy: 0.9926\n",
      "Epoch 61/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0191 - accuracy: 0.9995 - val_loss: 0.0332 - val_accuracy: 0.9963\n",
      "Epoch 62/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0234 - accuracy: 0.9986 - val_loss: 0.0381 - val_accuracy: 0.9982\n",
      "Epoch 63/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0261 - accuracy: 0.9968 - val_loss: 0.0305 - val_accuracy: 0.9963\n",
      "Epoch 64/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0287 - accuracy: 0.9963 - val_loss: 0.0786 - val_accuracy: 0.9798\n",
      "Epoch 65/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0242 - accuracy: 0.9968 - val_loss: 0.0398 - val_accuracy: 0.9982\n",
      "Epoch 66/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0203 - accuracy: 0.9977 - val_loss: 0.0349 - val_accuracy: 0.9945\n",
      "Epoch 67/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0176 - accuracy: 0.9995 - val_loss: 0.0520 - val_accuracy: 0.9908\n",
      "Epoch 68/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0184 - accuracy: 0.9982 - val_loss: 0.0417 - val_accuracy: 0.9945\n",
      "Epoch 69/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0197 - accuracy: 0.9991 - val_loss: 0.0341 - val_accuracy: 0.9963\n",
      "Epoch 70/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0158 - accuracy: 0.9991 - val_loss: 0.0481 - val_accuracy: 0.9945\n",
      "Epoch 71/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0239 - accuracy: 0.9963 - val_loss: 0.0536 - val_accuracy: 0.9908\n",
      "Epoch 72/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0261 - accuracy: 0.9972 - val_loss: 0.0649 - val_accuracy: 0.9871\n",
      "Epoch 73/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0189 - accuracy: 0.9991 - val_loss: 0.0461 - val_accuracy: 0.9926\n",
      "Epoch 74/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0196 - accuracy: 0.9986 - val_loss: 0.0769 - val_accuracy: 0.9798\n",
      "Epoch 75/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0192 - accuracy: 0.9972 - val_loss: 0.0758 - val_accuracy: 0.9890\n",
      "Epoch 76/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0197 - accuracy: 0.9972 - val_loss: 0.0537 - val_accuracy: 0.9908\n",
      "Epoch 77/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0163 - accuracy: 0.9995 - val_loss: 0.0495 - val_accuracy: 0.9945\n",
      "Epoch 78/120\n",
      "68/68 [==============================] - 14s 203ms/step - loss: 0.0193 - accuracy: 0.9982 - val_loss: 0.0363 - val_accuracy: 0.9890\n",
      "Epoch 79/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0156 - accuracy: 0.9991 - val_loss: 0.0292 - val_accuracy: 0.9963\n",
      "Epoch 80/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0185 - accuracy: 0.9968 - val_loss: 0.0534 - val_accuracy: 0.9835\n",
      "Epoch 81/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0177 - accuracy: 0.9977 - val_loss: 0.0642 - val_accuracy: 0.9871\n",
      "Epoch 82/120\n",
      "68/68 [==============================] - 14s 203ms/step - loss: 0.0154 - accuracy: 0.9995 - val_loss: 0.0288 - val_accuracy: 0.9963\n",
      "Epoch 83/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0151 - accuracy: 0.9982 - val_loss: 0.0360 - val_accuracy: 0.9945\n",
      "Epoch 84/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0178 - accuracy: 0.9977 - val_loss: 0.0364 - val_accuracy: 0.9926\n",
      "Epoch 85/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0148 - accuracy: 0.9982 - val_loss: 0.0438 - val_accuracy: 0.9945\n",
      "Epoch 86/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0140 - accuracy: 0.9995 - val_loss: 0.0632 - val_accuracy: 0.9926\n",
      "Epoch 87/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0419 - val_accuracy: 0.9890\n",
      "Epoch 88/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0120 - accuracy: 0.9986 - val_loss: 0.0434 - val_accuracy: 0.9908\n",
      "Epoch 89/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0119 - accuracy: 0.9991 - val_loss: 0.0346 - val_accuracy: 0.9963\n",
      "Epoch 90/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0188 - accuracy: 0.9977 - val_loss: 0.0302 - val_accuracy: 0.9945\n",
      "Epoch 91/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0126 - accuracy: 0.9991 - val_loss: 0.0574 - val_accuracy: 0.9926\n",
      "Epoch 92/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0223 - accuracy: 0.9972 - val_loss: 0.0553 - val_accuracy: 0.9853\n",
      "Epoch 93/120\n",
      "68/68 [==============================] - 14s 207ms/step - loss: 0.0160 - accuracy: 0.9986 - val_loss: 0.0941 - val_accuracy: 0.9761\n",
      "Epoch 94/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0126 - accuracy: 0.9986 - val_loss: 0.0235 - val_accuracy: 0.9963\n",
      "Epoch 95/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0126 - accuracy: 0.9991 - val_loss: 0.0285 - val_accuracy: 0.9963\n",
      "Epoch 96/120\n",
      "68/68 [==============================] - 14s 203ms/step - loss: 0.0152 - accuracy: 0.9977 - val_loss: 0.0348 - val_accuracy: 0.9982\n",
      "Epoch 97/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0150 - accuracy: 0.9977 - val_loss: 0.0416 - val_accuracy: 0.9926\n",
      "Epoch 98/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0135 - accuracy: 0.9991 - val_loss: 0.0604 - val_accuracy: 0.9926\n",
      "Epoch 99/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0163 - accuracy: 0.9977 - val_loss: 0.0649 - val_accuracy: 0.9871\n",
      "Epoch 100/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0144 - accuracy: 0.9986 - val_loss: 0.0487 - val_accuracy: 0.9853\n",
      "Epoch 101/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0131 - accuracy: 0.9982 - val_loss: 0.0336 - val_accuracy: 0.9871\n",
      "Epoch 102/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0141 - accuracy: 0.9977 - val_loss: 0.0456 - val_accuracy: 0.9926\n",
      "Epoch 103/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0102 - accuracy: 0.9995 - val_loss: 0.0185 - val_accuracy: 0.9982\n",
      "Epoch 104/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 0.9982\n",
      "Epoch 105/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 0.9945\n",
      "Epoch 106/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0118 - accuracy: 0.9991 - val_loss: 0.0393 - val_accuracy: 0.9963\n",
      "Epoch 107/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 0.9982\n",
      "Epoch 108/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9963\n",
      "Epoch 109/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 0.9963\n",
      "Epoch 110/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0074 - accuracy: 0.9995 - val_loss: 0.0324 - val_accuracy: 0.9908\n",
      "Epoch 111/120\n",
      "68/68 [==============================] - 14s 206ms/step - loss: 0.0121 - accuracy: 0.9977 - val_loss: 0.0563 - val_accuracy: 0.9835\n",
      "Epoch 112/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0125 - accuracy: 0.9991 - val_loss: 0.0550 - val_accuracy: 0.9890\n",
      "Epoch 113/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0093 - accuracy: 0.9995 - val_loss: 0.0262 - val_accuracy: 0.9963\n",
      "Epoch 114/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0142 - accuracy: 0.9968 - val_loss: 0.0214 - val_accuracy: 0.9963\n",
      "Epoch 115/120\n",
      "68/68 [==============================] - 14s 203ms/step - loss: 0.0091 - accuracy: 0.9995 - val_loss: 0.0192 - val_accuracy: 0.9982\n",
      "Epoch 116/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0085 - accuracy: 0.9991 - val_loss: 0.0369 - val_accuracy: 0.9871\n",
      "Epoch 117/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0086 - accuracy: 0.9991 - val_loss: 0.0303 - val_accuracy: 0.9926\n",
      "Epoch 118/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0120 - accuracy: 0.9977 - val_loss: 0.0582 - val_accuracy: 0.9798\n",
      "Epoch 119/120\n",
      "68/68 [==============================] - 14s 205ms/step - loss: 0.0112 - accuracy: 0.9986 - val_loss: 0.0299 - val_accuracy: 0.9908\n",
      "Epoch 120/120\n",
      "68/68 [==============================] - 14s 204ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "enb3_wo_top = keras.applications.efficientnet.EfficientNetB3(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb3_wo_top.trainable = False\n",
    "\n",
    "input_enb3 = keras.layers.Input(shape=(350, 350, 3))\n",
    "enb3_layer = enb3_wo_top(input_enb3)\n",
    "\n",
    "# random = np.random.random(enb3_layer.shape[1:])\n",
    "\n",
    "attention = keras.layers.Attention()([enb3_layer, enb3_layer])\n",
    "flatten = keras.layers.Flatten()(attention)\n",
    "\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(flatten)\n",
    "bn1 = keras.layers.BatchNormalization()(dense1)\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(bn1)\n",
    "output = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "enb3_model = keras.Model(inputs=input_enb3, outputs=output)\n",
    "enb3_model.summary()\n",
    "\n",
    "checkpoint_enb3 = keras.callbacks.ModelCheckpoint('/kaggle/working/ENB3_Model.h5', save_best_only=True)\n",
    "\n",
    "enb3_model.compile(optimizer=keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_enb3 = enb3_model.fit(x_train, y_train, batch_size=32, epochs=120, validation_data=(x_test, y_test))\n",
    "\n",
    "df = pd.DataFrame(history_enb3.history)\n",
    "df.to_csv('/kaggle/working/ENB3_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:13:54.113603Z",
     "iopub.status.busy": "2023-08-08T16:13:54.113228Z",
     "iopub.status.idle": "2023-08-08T16:24:36.452653Z",
     "shell.execute_reply": "2023-08-08T16:24:36.451603Z",
     "shell.execute_reply.started": "2023-08-08T16:13:54.113571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "\n",
      "71686520/71686520 [==============================] - 0s 0us/step\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:14:13.795919: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/efficientnetb4/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 39s 496ms/step - loss: 16.2970 - accuracy: 0.7723 - val_loss: 2.4699 - val_accuracy: 0.8170\n",
      "\n",
      "Epoch 2/30\n",
      "\n",
      "52/52 [==============================] - 22s 424ms/step - loss: 1.6939 - accuracy: 0.8611 - val_loss: 0.9919 - val_accuracy: 0.8668\n",
      "\n",
      "Epoch 3/30\n",
      "\n",
      "52/52 [==============================] - 18s 353ms/step - loss: 1.0390 - accuracy: 0.8907 - val_loss: 1.7910 - val_accuracy: 0.8053\n",
      "\n",
      "Epoch 4/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.8329 - accuracy: 0.9070 - val_loss: 0.9539 - val_accuracy: 0.9139\n",
      "\n",
      "Epoch 5/30\n",
      "\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 1.1016 - accuracy: 0.8979 - val_loss: 1.6362 - val_accuracy: 0.8931\n",
      "\n",
      "Epoch 6/30\n",
      "\n",
      "52/52 [==============================] - 18s 352ms/step - loss: 0.9206 - accuracy: 0.9046 - val_loss: 0.7742 - val_accuracy: 0.9130\n",
      "\n",
      "Epoch 7/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.4585 - accuracy: 0.9330 - val_loss: 0.6695 - val_accuracy: 0.9221\n",
      "\n",
      "Epoch 8/30\n",
      "\n",
      "52/52 [==============================] - 18s 352ms/step - loss: 0.3021 - accuracy: 0.9529 - val_loss: 0.6316 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 9/30\n",
      "\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 0.3068 - accuracy: 0.9475 - val_loss: 0.3934 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 10/30\n",
      "\n",
      "52/52 [==============================] - 18s 353ms/step - loss: 0.5822 - accuracy: 0.9306 - val_loss: 0.8503 - val_accuracy: 0.9203\n",
      "\n",
      "Epoch 11/30\n",
      "\n",
      "52/52 [==============================] - 18s 352ms/step - loss: 0.5044 - accuracy: 0.9348 - val_loss: 0.5839 - val_accuracy: 0.9266\n",
      "\n",
      "Epoch 12/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.4016 - accuracy: 0.9420 - val_loss: 2.0281 - val_accuracy: 0.8931\n",
      "\n",
      "Epoch 13/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.5542 - accuracy: 0.9390 - val_loss: 1.0836 - val_accuracy: 0.8940\n",
      "\n",
      "Epoch 14/30\n",
      "\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 0.3785 - accuracy: 0.9511 - val_loss: 0.5732 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 15/30\n",
      "\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 0.2578 - accuracy: 0.9499 - val_loss: 0.6669 - val_accuracy: 0.8995\n",
      "\n",
      "Epoch 16/30\n",
      "\n",
      "52/52 [==============================] - 18s 353ms/step - loss: 0.3288 - accuracy: 0.9505 - val_loss: 0.6144 - val_accuracy: 0.9438\n",
      "\n",
      "Epoch 17/30\n",
      "\n",
      "52/52 [==============================] - 18s 352ms/step - loss: 0.2504 - accuracy: 0.9626 - val_loss: 0.4677 - val_accuracy: 0.9357\n",
      "\n",
      "Epoch 18/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.3787 - accuracy: 0.9505 - val_loss: 1.0466 - val_accuracy: 0.8542\n",
      "\n",
      "Epoch 19/30\n",
      "\n",
      "52/52 [==============================] - 18s 354ms/step - loss: 0.2267 - accuracy: 0.9607 - val_loss: 0.4676 - val_accuracy: 0.9511\n",
      "\n",
      "Epoch 20/30\n",
      "\n",
      "52/52 [==============================] - 18s 353ms/step - loss: 0.1656 - accuracy: 0.9674 - val_loss: 0.5972 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 21/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.2028 - accuracy: 0.9710 - val_loss: 0.5200 - val_accuracy: 0.9466\n",
      "\n",
      "Epoch 22/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.0902 - accuracy: 0.9795 - val_loss: 0.7485 - val_accuracy: 0.9366\n",
      "\n",
      "Epoch 23/30\n",
      "\n",
      "52/52 [==============================] - 18s 352ms/step - loss: 0.1333 - accuracy: 0.9740 - val_loss: 0.3964 - val_accuracy: 0.9457\n",
      "\n",
      "Epoch 24/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.2764 - accuracy: 0.9565 - val_loss: 0.3097 - val_accuracy: 0.9556\n",
      "\n",
      "Epoch 25/30\n",
      "\n",
      "52/52 [==============================] - 18s 352ms/step - loss: 0.1467 - accuracy: 0.9656 - val_loss: 0.6119 - val_accuracy: 0.9366\n",
      "\n",
      "Epoch 26/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.1342 - accuracy: 0.9740 - val_loss: 0.5946 - val_accuracy: 0.9257\n",
      "\n",
      "Epoch 27/30\n",
      "\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.1692 - accuracy: 0.9650 - val_loss: 0.4676 - val_accuracy: 0.9357\n",
      "\n",
      "Epoch 28/30\n",
      "\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 0.1914 - accuracy: 0.9692 - val_loss: 0.3298 - val_accuracy: 0.9438\n",
      "\n",
      "Epoch 29/30\n",
      "\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 0.0901 - accuracy: 0.9789 - val_loss: 0.3811 - val_accuracy: 0.9511\n",
      "\n",
      "Epoch 30/30\n",
      "\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 0.1905 - accuracy: 0.9650 - val_loss: 0.7595 - val_accuracy: 0.9384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79b0e60ae3e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enb4_wo_top = keras.applications.efficientnet.EfficientNetB4(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb4_wo_top.trainable = False\n",
    "\n",
    "enb4_model = keras.models.Sequential()\n",
    "enb4_model.add(enb4_wo_top)\n",
    "enb4_model.add(keras.layers.Flatten())\n",
    "enb4_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb4_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb4_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb4_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb4_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:25:14.414558Z",
     "iopub.status.busy": "2023-08-08T16:25:14.413483Z",
     "iopub.status.idle": "2023-08-08T16:38:53.342629Z",
     "shell.execute_reply": "2023-08-08T16:38:53.341476Z",
     "shell.execute_reply.started": "2023-08-08T16:25:14.414504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n",
      "\n",
      "115263384/115263384 [==============================] - 1s 0us/step\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:25:39.832937: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/efficientnetb5/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 58s 794ms/step - loss: 14.6170 - accuracy: 0.7874 - val_loss: 3.3066 - val_accuracy: 0.8623\n",
      "\n",
      "Epoch 2/30\n",
      "\n",
      "52/52 [==============================] - 25s 492ms/step - loss: 3.8318 - accuracy: 0.8315 - val_loss: 3.0094 - val_accuracy: 0.8877\n",
      "\n",
      "Epoch 3/30\n",
      "\n",
      "52/52 [==============================] - 25s 491ms/step - loss: 3.3900 - accuracy: 0.8521 - val_loss: 5.2666 - val_accuracy: 0.8261\n",
      "\n",
      "Epoch 4/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 1.6034 - accuracy: 0.8792 - val_loss: 1.1937 - val_accuracy: 0.8886\n",
      "\n",
      "Epoch 5/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 1.3244 - accuracy: 0.8913 - val_loss: 0.9691 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 6/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.6872 - accuracy: 0.9215 - val_loss: 0.8188 - val_accuracy: 0.9121\n",
      "\n",
      "Epoch 7/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.6973 - accuracy: 0.9257 - val_loss: 0.7954 - val_accuracy: 0.9149\n",
      "\n",
      "Epoch 8/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 1.1151 - accuracy: 0.9016 - val_loss: 1.6219 - val_accuracy: 0.8822\n",
      "\n",
      "Epoch 9/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 1.0556 - accuracy: 0.9076 - val_loss: 0.8859 - val_accuracy: 0.9139\n",
      "\n",
      "Epoch 10/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.9023 - accuracy: 0.9179 - val_loss: 0.4620 - val_accuracy: 0.9303\n",
      "\n",
      "Epoch 11/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.2743 - accuracy: 0.9475 - val_loss: 0.4171 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 12/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.3568 - accuracy: 0.9469 - val_loss: 0.3610 - val_accuracy: 0.9366\n",
      "\n",
      "Epoch 13/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.4385 - accuracy: 0.9457 - val_loss: 0.6243 - val_accuracy: 0.9384\n",
      "\n",
      "Epoch 14/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.5887 - accuracy: 0.9203 - val_loss: 0.4403 - val_accuracy: 0.9303\n",
      "\n",
      "Epoch 15/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.5186 - accuracy: 0.9245 - val_loss: 0.7303 - val_accuracy: 0.9049\n",
      "\n",
      "Epoch 16/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 0.3024 - accuracy: 0.9511 - val_loss: 0.5021 - val_accuracy: 0.9357\n",
      "\n",
      "Epoch 17/30\n",
      "\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.2561 - accuracy: 0.9529 - val_loss: 0.2227 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 18/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.2373 - accuracy: 0.9481 - val_loss: 0.4880 - val_accuracy: 0.9248\n",
      "\n",
      "Epoch 19/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.5427 - accuracy: 0.9179 - val_loss: 0.6189 - val_accuracy: 0.9158\n",
      "\n",
      "Epoch 20/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 0.3141 - accuracy: 0.9390 - val_loss: 0.8658 - val_accuracy: 0.9031\n",
      "\n",
      "Epoch 21/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 0.2598 - accuracy: 0.9511 - val_loss: 0.3488 - val_accuracy: 0.9384\n",
      "\n",
      "Epoch 22/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 0.1560 - accuracy: 0.9674 - val_loss: 0.4051 - val_accuracy: 0.9357\n",
      "\n",
      "Epoch 23/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.2912 - accuracy: 0.9457 - val_loss: 1.3913 - val_accuracy: 0.9149\n",
      "\n",
      "Epoch 24/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.3184 - accuracy: 0.9541 - val_loss: 0.3680 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 25/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.1682 - accuracy: 0.9632 - val_loss: 0.3580 - val_accuracy: 0.9402\n",
      "\n",
      "Epoch 26/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 0.4220 - accuracy: 0.9426 - val_loss: 0.3209 - val_accuracy: 0.9447\n",
      "\n",
      "Epoch 27/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 0.2451 - accuracy: 0.9547 - val_loss: 0.7629 - val_accuracy: 0.9040\n",
      "\n",
      "Epoch 28/30\n",
      "\n",
      "52/52 [==============================] - 26s 508ms/step - loss: 0.2362 - accuracy: 0.9529 - val_loss: 0.7903 - val_accuracy: 0.9230\n",
      "\n",
      "Epoch 29/30\n",
      "\n",
      "52/52 [==============================] - 25s 491ms/step - loss: 0.2235 - accuracy: 0.9571 - val_loss: 0.2815 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 30/30\n",
      "\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.1570 - accuracy: 0.9680 - val_loss: 0.4111 - val_accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79b0e2f33dc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enb5_wo_top = keras.applications.efficientnet.EfficientNetB5(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb5_wo_top.trainable = False\n",
    "\n",
    "enb5_model = keras.models.Sequential()\n",
    "enb5_model.add(enb5_wo_top)\n",
    "enb5_model.add(keras.layers.Flatten())\n",
    "enb5_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb5_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb5_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb5_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb5_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:38:53.345419Z",
     "iopub.status.busy": "2023-08-08T16:38:53.345048Z",
     "iopub.status.idle": "2023-08-08T16:59:44.213450Z",
     "shell.execute_reply": "2023-08-08T16:59:44.211622Z",
     "shell.execute_reply.started": "2023-08-08T16:38:53.345383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5\n",
      "\n",
      "165234480/165234480 [==============================] - 1s 0us/step\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 16:39:21.370535: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_3/efficientnetb6/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 66s 898ms/step - loss: 20.4183 - accuracy: 0.7627 - val_loss: 4.0538 - val_accuracy: 0.8659\n",
      "\n",
      "Epoch 2/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 3.9529 - accuracy: 0.8339 - val_loss: 5.1934 - val_accuracy: 0.5806\n",
      "\n",
      "Epoch 3/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 2.7704 - accuracy: 0.8460 - val_loss: 1.9560 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 4/30\n",
      "\n",
      "52/52 [==============================] - 33s 638ms/step - loss: 1.8828 - accuracy: 0.8581 - val_loss: 5.3015 - val_accuracy: 0.5761\n",
      "\n",
      "Epoch 5/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 1.5618 - accuracy: 0.8702 - val_loss: 1.5069 - val_accuracy: 0.8560\n",
      "\n",
      "Epoch 6/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 1.7237 - accuracy: 0.8798 - val_loss: 1.6337 - val_accuracy: 0.8786\n",
      "\n",
      "Epoch 7/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 1.1548 - accuracy: 0.8998 - val_loss: 1.5990 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 8/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 1.2232 - accuracy: 0.8937 - val_loss: 1.1802 - val_accuracy: 0.8904\n",
      "\n",
      "Epoch 9/30\n",
      "\n",
      "52/52 [==============================] - 33s 639ms/step - loss: 1.2221 - accuracy: 0.9004 - val_loss: 1.3182 - val_accuracy: 0.8958\n",
      "\n",
      "Epoch 10/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 0.6280 - accuracy: 0.9287 - val_loss: 0.8672 - val_accuracy: 0.8986\n",
      "\n",
      "Epoch 11/30\n",
      "\n",
      "52/52 [==============================] - 33s 638ms/step - loss: 1.0145 - accuracy: 0.9106 - val_loss: 1.2674 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 12/30\n",
      "\n",
      "52/52 [==============================] - 33s 639ms/step - loss: 0.8810 - accuracy: 0.9136 - val_loss: 0.8386 - val_accuracy: 0.9185\n",
      "\n",
      "Epoch 13/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 0.7672 - accuracy: 0.9203 - val_loss: 0.7996 - val_accuracy: 0.9158\n",
      "\n",
      "Epoch 14/30\n",
      "\n",
      "52/52 [==============================] - 33s 639ms/step - loss: 0.6955 - accuracy: 0.9221 - val_loss: 1.8366 - val_accuracy: 0.8714\n",
      "\n",
      "Epoch 15/30\n",
      "\n",
      "52/52 [==============================] - 33s 639ms/step - loss: 0.9098 - accuracy: 0.9167 - val_loss: 1.0636 - val_accuracy: 0.9158\n",
      "\n",
      "Epoch 16/30\n",
      "\n",
      "52/52 [==============================] - 41s 803ms/step - loss: 0.7765 - accuracy: 0.9191 - val_loss: 0.8637 - val_accuracy: 0.9040\n",
      "\n",
      "Epoch 17/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 0.4665 - accuracy: 0.9342 - val_loss: 0.6938 - val_accuracy: 0.9149\n",
      "\n",
      "Epoch 18/30\n",
      "\n",
      "52/52 [==============================] - 41s 803ms/step - loss: 0.6344 - accuracy: 0.9227 - val_loss: 1.7362 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 19/30\n",
      "\n",
      "52/52 [==============================] - 33s 639ms/step - loss: 0.3599 - accuracy: 0.9493 - val_loss: 0.6368 - val_accuracy: 0.9185\n",
      "\n",
      "Epoch 20/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 0.3544 - accuracy: 0.9481 - val_loss: 0.9718 - val_accuracy: 0.9158\n",
      "\n",
      "Epoch 21/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 0.3587 - accuracy: 0.9511 - val_loss: 0.8504 - val_accuracy: 0.9149\n",
      "\n",
      "Epoch 22/30\n",
      "\n",
      "52/52 [==============================] - 41s 803ms/step - loss: 1.2400 - accuracy: 0.9167 - val_loss: 2.3856 - val_accuracy: 0.8714\n",
      "\n",
      "Epoch 23/30\n",
      "\n",
      "52/52 [==============================] - 41s 801ms/step - loss: 0.8124 - accuracy: 0.9143 - val_loss: 1.4851 - val_accuracy: 0.8460\n",
      "\n",
      "Epoch 24/30\n",
      "\n",
      "52/52 [==============================] - 41s 803ms/step - loss: 0.4989 - accuracy: 0.9360 - val_loss: 2.3869 - val_accuracy: 0.8813\n",
      "\n",
      "Epoch 25/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 0.7575 - accuracy: 0.9239 - val_loss: 1.4215 - val_accuracy: 0.9076\n",
      "\n",
      "Epoch 26/30\n",
      "\n",
      "52/52 [==============================] - 41s 803ms/step - loss: 0.6724 - accuracy: 0.9275 - val_loss: 1.4842 - val_accuracy: 0.8913\n",
      "\n",
      "Epoch 27/30\n",
      "\n",
      "52/52 [==============================] - 41s 803ms/step - loss: 0.3315 - accuracy: 0.9469 - val_loss: 0.7131 - val_accuracy: 0.9031\n",
      "\n",
      "Epoch 28/30\n",
      "\n",
      "52/52 [==============================] - 41s 801ms/step - loss: 0.1992 - accuracy: 0.9529 - val_loss: 0.6861 - val_accuracy: 0.9275\n",
      "\n",
      "Epoch 29/30\n",
      "\n",
      "52/52 [==============================] - 41s 802ms/step - loss: 0.3118 - accuracy: 0.9463 - val_loss: 0.8560 - val_accuracy: 0.8659\n",
      "\n",
      "Epoch 30/30\n",
      "\n",
      "52/52 [==============================] - 33s 639ms/step - loss: 0.2137 - accuracy: 0.9565 - val_loss: 0.5036 - val_accuracy: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79b0df381270>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enb6_wo_top = keras.applications.efficientnet.EfficientNetB6(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb6_wo_top.trainable = False\n",
    "\n",
    "enb6_model = keras.models.Sequential()\n",
    "enb6_model.add(enb6_wo_top)\n",
    "enb6_model.add(keras.layers.Flatten())\n",
    "enb6_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb6_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb6_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb6_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb6_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T16:59:44.215591Z",
     "iopub.status.busy": "2023-08-08T16:59:44.215017Z",
     "iopub.status.idle": "2023-08-08T17:24:42.082755Z",
     "shell.execute_reply": "2023-08-08T17:24:42.081673Z",
     "shell.execute_reply.started": "2023-08-08T16:59:44.215552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
      "\n",
      "258076736/258076736 [==============================] - 1s 0us/step\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 17:00:18.938491: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_4/efficientnetb7/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 76s 1s/step - loss: 10.4503 - accuracy: 0.7766 - val_loss: 6.5107 - val_accuracy: 0.8632\n",
      "\n",
      "Epoch 2/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 3.7033 - accuracy: 0.8484 - val_loss: 1.7107 - val_accuracy: 0.8723\n",
      "\n",
      "Epoch 3/30\n",
      "\n",
      "52/52 [==============================] - 44s 853ms/step - loss: 1.8217 - accuracy: 0.8545 - val_loss: 1.4632 - val_accuracy: 0.8587\n",
      "\n",
      "Epoch 4/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 1.4227 - accuracy: 0.8738 - val_loss: 1.0049 - val_accuracy: 0.8859\n",
      "\n",
      "Epoch 5/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.8374 - accuracy: 0.8859 - val_loss: 0.7162 - val_accuracy: 0.9049\n",
      "\n",
      "Epoch 6/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.5495 - accuracy: 0.9064 - val_loss: 0.8654 - val_accuracy: 0.8949\n",
      "\n",
      "Epoch 7/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.5023 - accuracy: 0.9058 - val_loss: 0.7524 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 8/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.4077 - accuracy: 0.9161 - val_loss: 0.9410 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 9/30\n",
      "\n",
      "52/52 [==============================] - 48s 932ms/step - loss: 0.5059 - accuracy: 0.9185 - val_loss: 1.4617 - val_accuracy: 0.7246\n",
      "\n",
      "Epoch 10/30\n",
      "\n",
      "52/52 [==============================] - 44s 854ms/step - loss: 0.4325 - accuracy: 0.9173 - val_loss: 0.8119 - val_accuracy: 0.8877\n",
      "\n",
      "Epoch 11/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.6825 - accuracy: 0.8937 - val_loss: 0.6470 - val_accuracy: 0.9194\n",
      "\n",
      "Epoch 12/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.5215 - accuracy: 0.9124 - val_loss: 0.6693 - val_accuracy: 0.8668\n",
      "\n",
      "Epoch 13/30\n",
      "\n",
      "52/52 [==============================] - 44s 853ms/step - loss: 0.2647 - accuracy: 0.9426 - val_loss: 0.5267 - val_accuracy: 0.9085\n",
      "\n",
      "Epoch 14/30\n",
      "\n",
      "52/52 [==============================] - 44s 853ms/step - loss: 0.2088 - accuracy: 0.9481 - val_loss: 0.3519 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 15/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.1818 - accuracy: 0.9559 - val_loss: 0.3727 - val_accuracy: 0.9293\n",
      "\n",
      "Epoch 16/30\n",
      "\n",
      "52/52 [==============================] - 44s 851ms/step - loss: 0.2094 - accuracy: 0.9505 - val_loss: 0.4651 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 17/30\n",
      "\n",
      "52/52 [==============================] - 44s 852ms/step - loss: 0.1686 - accuracy: 0.9553 - val_loss: 0.3692 - val_accuracy: 0.9447\n",
      "\n",
      "Epoch 18/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.2183 - accuracy: 0.9463 - val_loss: 0.2417 - val_accuracy: 0.9420\n",
      "\n",
      "Epoch 19/30\n",
      "\n",
      "52/52 [==============================] - 44s 851ms/step - loss: 0.2760 - accuracy: 0.9384 - val_loss: 0.4739 - val_accuracy: 0.9004\n",
      "\n",
      "Epoch 20/30\n",
      "\n",
      "52/52 [==============================] - 44s 850ms/step - loss: 0.2889 - accuracy: 0.9354 - val_loss: 0.5266 - val_accuracy: 0.9221\n",
      "\n",
      "Epoch 21/30\n",
      "\n",
      "52/52 [==============================] - 48s 932ms/step - loss: 0.1261 - accuracy: 0.9614 - val_loss: 0.2756 - val_accuracy: 0.9420\n",
      "\n",
      "Epoch 22/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.0899 - accuracy: 0.9698 - val_loss: 0.2228 - val_accuracy: 0.9520\n",
      "\n",
      "Epoch 23/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.1861 - accuracy: 0.9595 - val_loss: 0.3230 - val_accuracy: 0.9393\n",
      "\n",
      "Epoch 24/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.0788 - accuracy: 0.9783 - val_loss: 0.3621 - val_accuracy: 0.9076\n",
      "\n",
      "Epoch 25/30\n",
      "\n",
      "52/52 [==============================] - 48s 930ms/step - loss: 0.1802 - accuracy: 0.9499 - val_loss: 0.6997 - val_accuracy: 0.9094\n",
      "\n",
      "Epoch 26/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.2497 - accuracy: 0.9444 - val_loss: 0.2623 - val_accuracy: 0.9466\n",
      "\n",
      "Epoch 27/30\n",
      "\n",
      "52/52 [==============================] - 48s 932ms/step - loss: 0.1368 - accuracy: 0.9638 - val_loss: 0.3445 - val_accuracy: 0.9303\n",
      "\n",
      "Epoch 28/30\n",
      "\n",
      "52/52 [==============================] - 48s 932ms/step - loss: 0.2623 - accuracy: 0.9475 - val_loss: 0.3760 - val_accuracy: 0.9366\n",
      "\n",
      "Epoch 29/30\n",
      "\n",
      "52/52 [==============================] - 48s 933ms/step - loss: 0.1526 - accuracy: 0.9559 - val_loss: 0.2582 - val_accuracy: 0.9366\n",
      "\n",
      "Epoch 30/30\n",
      "\n",
      "52/52 [==============================] - 44s 854ms/step - loss: 0.1258 - accuracy: 0.9662 - val_loss: 0.3750 - val_accuracy: 0.9348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79b0db3cd810>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enb7_wo_top = keras.applications.efficientnet.EfficientNetB7(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "enb7_wo_top.trainable = False\n",
    "\n",
    "enb7_model = keras.models.Sequential()\n",
    "enb7_model.add(enb7_wo_top)\n",
    "enb7_model.add(keras.layers.Flatten())\n",
    "enb7_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "enb7_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "enb7_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "enb7_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "enb7_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T17:24:42.166382Z",
     "iopub.status.busy": "2023-08-08T17:24:42.165559Z",
     "iopub.status.idle": "2023-08-08T17:30:11.913714Z",
     "shell.execute_reply": "2023-08-08T17:30:11.912702Z",
     "shell.execute_reply.started": "2023-08-08T17:24:42.166345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\n",
      "94765736/94765736 [==============================] - 1s 0us/step\n",
      "\n",
      "Epoch 1/30\n",
      "\n",
      "52/52 [==============================] - 19s 266ms/step - loss: 9.5755 - accuracy: 0.8110 - val_loss: 4.1062 - val_accuracy: 0.8940\n",
      "\n",
      "Epoch 2/30\n",
      "\n",
      "52/52 [==============================] - 10s 196ms/step - loss: 3.2696 - accuracy: 0.9004 - val_loss: 2.3007 - val_accuracy: 0.9221\n",
      "\n",
      "Epoch 3/30\n",
      "\n",
      "52/52 [==============================] - 11s 219ms/step - loss: 0.8234 - accuracy: 0.9457 - val_loss: 1.0823 - val_accuracy: 0.9429\n",
      "\n",
      "Epoch 4/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.9841 - accuracy: 0.9499 - val_loss: 1.3779 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 5/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 1.6150 - accuracy: 0.9372 - val_loss: 1.8411 - val_accuracy: 0.9239\n",
      "\n",
      "Epoch 6/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.3091 - accuracy: 0.9746 - val_loss: 1.3336 - val_accuracy: 0.9447\n",
      "\n",
      "Epoch 7/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.1924 - accuracy: 0.9855 - val_loss: 0.6305 - val_accuracy: 0.9629\n",
      "\n",
      "Epoch 8/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 0.3304 - accuracy: 0.9813 - val_loss: 1.5922 - val_accuracy: 0.9176\n",
      "\n",
      "Epoch 9/30\n",
      "\n",
      "52/52 [==============================] - 10s 198ms/step - loss: 0.1440 - accuracy: 0.9855 - val_loss: 1.3015 - val_accuracy: 0.9493\n",
      "\n",
      "Epoch 10/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.0374 - accuracy: 0.9952 - val_loss: 0.8229 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 11/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 0.0228 - accuracy: 0.9964 - val_loss: 0.9358 - val_accuracy: 0.9638\n",
      "\n",
      "Epoch 12/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.1565 - accuracy: 0.9903 - val_loss: 1.0917 - val_accuracy: 0.9592\n",
      "\n",
      "Epoch 13/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.1788 - accuracy: 0.9795 - val_loss: 0.8329 - val_accuracy: 0.9620\n",
      "\n",
      "Epoch 14/30\n",
      "\n",
      "52/52 [==============================] - 11s 221ms/step - loss: 0.1477 - accuracy: 0.9861 - val_loss: 1.7373 - val_accuracy: 0.9511\n",
      "\n",
      "Epoch 15/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 0.1521 - accuracy: 0.9855 - val_loss: 2.8388 - val_accuracy: 0.9149\n",
      "\n",
      "Epoch 16/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.4384 - accuracy: 0.9783 - val_loss: 1.2302 - val_accuracy: 0.9583\n",
      "\n",
      "Epoch 17/30\n",
      "\n",
      "52/52 [==============================] - 10s 199ms/step - loss: 0.2270 - accuracy: 0.9867 - val_loss: 1.0613 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 18/30\n",
      "\n",
      "52/52 [==============================] - 10s 199ms/step - loss: 0.0851 - accuracy: 0.9946 - val_loss: 1.8737 - val_accuracy: 0.9420\n",
      "\n",
      "Epoch 19/30\n",
      "\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.0338 - accuracy: 0.9976 - val_loss: 1.1597 - val_accuracy: 0.9683\n",
      "\n",
      "Epoch 20/30\n",
      "\n",
      "52/52 [==============================] - 10s 198ms/step - loss: 0.0018 - accuracy: 0.9988 - val_loss: 1.4556 - val_accuracy: 0.9529\n",
      "\n",
      "Epoch 21/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 0.0495 - accuracy: 0.9952 - val_loss: 0.7854 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 22/30\n",
      "\n",
      "52/52 [==============================] - 10s 198ms/step - loss: 6.2951e-05 - accuracy: 1.0000 - val_loss: 0.6948 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 23/30\n",
      "\n",
      "52/52 [==============================] - 10s 198ms/step - loss: 2.7427e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 24/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 2.7355e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 25/30\n",
      "\n",
      "52/52 [==============================] - 10s 198ms/step - loss: 2.7211e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 26/30\n",
      "\n",
      "52/52 [==============================] - 10s 198ms/step - loss: 2.7139e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 27/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 2.6995e-08 - accuracy: 1.0000 - val_loss: 0.6906 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 28/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 2.6923e-08 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 29/30\n",
      "\n",
      "52/52 [==============================] - 11s 220ms/step - loss: 2.6707e-08 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 30/30\n",
      "\n",
      "52/52 [==============================] - 10s 199ms/step - loss: 2.6563e-08 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.9692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79b0e0910e80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_wo_top = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(350, 350, 3))\n",
    "\n",
    "resnet_wo_top.trainable = False\n",
    "\n",
    "resnet_model = keras.models.Sequential()\n",
    "resnet_model.add(resnet_wo_top)\n",
    "resnet_model.add(keras.layers.Flatten())\n",
    "resnet_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "resnet_model.add(keras.layers.Dense(64, activation='relu'))\n",
    "resnet_model.add(keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "resnet_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "resnet_model.fit(x_train, y_train, batch_size=32, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
